#!/usr/bin/env python

# Copyright 2024 The HuggingFace Inc. team.
# All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math
from dataclasses import asdict
from typing import Callable, Literal

import einops
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F  # noqa: N812
from torch import Tensor
from torch.distributions import MultivariateNormal, TanhTransform, Transform, TransformedDistribution

from lerobot.common.policies.normalize import NormalizeBuffer
from lerobot.common.policies.pretrained import PreTrainedPolicy
from lerobot.common.policies.sac.mlp_bc_model.configuration_mlp_bc import MLPBCConfig, is_image_feature
from lerobot.common.policies.utils import get_device_from_parameters
from lerobot.common.policies.sac.modeling_sac import SACObservationEncoder, TanhMultivariateNormalDiag, orthogonal_init, _convert_normalization_params_to_tensor

class MLPBCPolicy(PreTrainedPolicy):
    config_class = MLPBCConfig
    name = "mlp_bc"

    def __init__(
        self,
        config: MLPBCConfig | None = None,
        dataset_stats: dict[str, dict[str, Tensor]] | None = None,
    ):
        # é€šè¿‡é…ç½®æ–‡ä»¶åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(config)
        # éªŒè¯è¾“å…¥è¾“å‡ºç‰¹å¾
        config.validate_features() 
        self.config = config

        # Determine action dimension and initialize all components
        action_feature = config.output_features["action"]
        if hasattr(action_feature, 'shape'):
            continuous_action_dim = action_feature.shape[0]  # ğŸ”¥ è·å–è¿ç»­åŠ¨ä½œç»´åº¦
        else:
            # Handle case where shape is a list/dict from JSON config
            continuous_action_dim = action_feature["shape"][0]  # ğŸ”¥ è·å–è¿ç»­åŠ¨ä½œç»´åº¦
        self._init_normalization(dataset_stats) # ğŸ”¥ åˆå§‹åŒ–å½’ä¸€åŒ–, é€šè¿‡ dataset_stats ä¸­çš„ min å’Œ max å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
        self._init_encoders() # ğŸ”¥ åˆå§‹åŒ–ç¼–ç å™¨
        self._init_actor(continuous_action_dim) # ğŸ”¥ åˆå§‹åŒ–actor

        # Initialize member variable to store td_target for wandb logging
        self.last_td_target: Tensor | None = None

    def forward(self, observations: dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Forward pass for behavior cloning training.
        
        Args:
            observations: Dictionary containing observation tensors
            
        Returns:
            predicted_actions: Predicted actions from the policy
        """
        # Get predicted actions using the actor network
        predicted_actions, _, _ = self.actor(observations)
        return predicted_actions

    def compute_loss(
        self, 
        observations: dict[str, torch.Tensor], 
        target_actions: torch.Tensor
    ) -> tuple[torch.Tensor, dict[str, float]]:
        """
        Compute behavior cloning loss and metrics.
        
        Args:
            observations: Dictionary containing observation tensors
            target_actions: Target actions from expert demonstrations
            
        Returns:
            loss: Computed behavior cloning loss
            metrics: Dictionary containing training metrics
        """
        # Forward pass to get predicted actions
        predicted_actions = self.forward(observations)
        
        # Compute loss using config
        loss = self.config.compute_bc_loss(predicted_actions, target_actions)
        
        # Compute additional metrics
        metrics = self.config.compute_metrics(predicted_actions, target_actions)
        
        return loss, metrics

    def predict(self, observations: dict[str, torch.Tensor], deterministic: bool = True) -> torch.Tensor:
        """
        Predict actions for given observations (inference mode).
        
        Args:
            observations: Dictionary containing observation tensors
            deterministic: If True, return mean actions; if False, sample from distribution
            
        Returns:
            predicted_actions: Predicted actions
        """
        self.eval()
        with torch.no_grad():
            if deterministic:
                # Return mean actions for deterministic behavior
                predicted_actions = self.forward(observations)
            else:
                # Sample from the policy distribution
                actions, _, _ = self.actor(observations)
                predicted_actions = actions
        return predicted_actions

    def select_action(self, observations: dict[str, torch.Tensor], deterministic: bool = True) -> torch.Tensor:
        """
        Select action for given observations (for environment interaction).
        This is the main method used during policy execution.
        
        Args:
            observations: Dictionary containing observation tensors
            deterministic: If True, return mean actions; if False, sample from distribution
            
        Returns:
            selected_actions: Selected actions for environment interaction
        """
        return self.predict(observations, deterministic=deterministic)

    def reset(self) -> None:
        """
        Reset the policy state. For MLP BC, there's no internal state to reset,
        but this method is required by the PreTrainedPolicy interface.
        """
        pass

    def get_optim_params(self) -> dict:
        """
        Get optimization parameters for the policy.
        Returns a dictionary mapping parameter groups to their parameters.
        """
        return {
            "policy": list(self.parameters())
        }

    def _init_normalization(self, dataset_stats):
        """Initialize input/output normalization modules."""
        self.normalize_inputs = nn.Identity()  # ç½‘ç»œå±‚å ä½åˆå§‹åŒ–
        self.normalize_targets = nn.Identity() # ç½‘ç»œå±‚å ä½åˆå§‹åŒ–
        """
            pre-train é˜¶æ®µä½¿ç”¨configå½“ä¸­çš„dataset_statså¯¹è¾“å…¥æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
            eval é˜¶æ®µä½¿ç”¨ ./pretrained_model/config.json å½“ä¸­çš„dataset_statså¯¹è¾“å‡ºæ•°æ®è¿›è¡Œå½’ä¸€åŒ–
        """
        if self.config.dataset_stats is not None: # å¦‚æœconfigå½“ä¸­å®šä¹‰äº†dataset_stats
            params = _convert_normalization_params_to_tensor(self.config.dataset_stats) # å°†å½’ä¸€åŒ–å‚æ•°è½¬æ¢ä¸ºå¼ é‡
            self.normalize_inputs = NormalizeBuffer(
                self.config.input_features, self.config.normalization_mapping, params
            )
            """
                ä¼˜å…ˆä½¿ç”¨dataset_statsä¸­çš„minå’Œmaxå¯¹è¾“å‡ºæ•°æ®è¿›è¡Œå½’ä¸€åŒ–, 
                å¦‚æœdataset_statsä¸ºNone
                åˆ™ä½¿ç”¨paramså¯¹è¾“å‡ºæ•°æ®è¿›è¡Œå½’ä¸€åŒ–
            """
            stats = dataset_stats or params # å¦‚æœdataset_stats ä¸ºNoneï¼Œåˆ™ä½¿ç”¨params
            self.normalize_targets = NormalizeBuffer(
                self.config.output_features, self.config.normalization_mapping, stats
            )

    def _init_encoders(self):
        """Initialize encoder for MLP BC policy."""
        # For MLP BC, we don't create any encoders at this level
        # All encoding is handled inside the Policy (actor)
        self.shared_encoder = self.config.shared_encoder
        # Don't create any encoder attributes to avoid duplication
        # self.encoder_actor and self.encoder_critic will be None

    def _init_actor(self, continuous_action_dim):
        """åˆå§‹åŒ–ç­–ç•¥Actorç½‘ç»œå’Œé»˜è®¤ç›®æ ‡ç†µå€¼ã€‚
        
        Actorç½‘ç»œæ¶æ„è¯´æ˜ï¼š
        1. è§‚æµ‹ç¼–ç å™¨ (SACObservationEncoder): å°†åŸå§‹è§‚æµ‹è½¬æ¢ä¸ºç‰¹å¾å‘é‡
        2. ä¸»å¹²ç½‘ç»œ (MLP): å¤šå±‚æ„ŸçŸ¥æœºï¼Œå¤„ç†ç¼–ç åçš„è§‚æµ‹ç‰¹å¾
        3. å‡å€¼å±‚ (mean_layer): è¾“å‡ºåŠ¨ä½œçš„å‡å€¼
        4. æ ‡å‡†å·®å±‚ (std_layer): è¾“å‡ºåŠ¨ä½œçš„æ ‡å‡†å·®ï¼ˆç”¨äºæ¢ç´¢ï¼‰
        
        ç½‘ç»œæµç¨‹ï¼š
        è§‚æµ‹è¾“å…¥ â†’ SACObservationEncoder â†’ è§‚æµ‹ç¼–ç  (256ç»´)
                                        â†“
                                    MLPä¸»å¹²ç½‘ç»œ (256â†’256â†’256)
                                        â†“
                                    å‡å€¼å±‚ (256â†’action_dim) â†’ åŠ¨ä½œå‡å€¼
                                    æ ‡å‡†å·®å±‚ (256â†’action_dim) â†’ åŠ¨ä½œæ ‡å‡†å·®
                                        â†“
                                    TanhMultivariateNormalDiag â†’ é‡‡æ ·åŠ¨ä½œ
        """
        # æ³¨æ„ï¼šActoråªé€‰æ‹©è¿ç»­åŠ¨ä½œéƒ¨åˆ†ï¼Œç¦»æ•£åŠ¨ä½œç”±ç¦»æ•£Criticå¤„ç†
        # Get network kwargs as dict (handle both dataclass and dict)
        if hasattr(self.config.actor_network_kwargs, '__dict__'):
            actor_network_kwargs = asdict(self.config.actor_network_kwargs)
        else:
            actor_network_kwargs = self.config.actor_network_kwargs if isinstance(self.config.actor_network_kwargs, dict) else {}
            
        # Get policy kwargs as dict (handle both dataclass and dict)
        if hasattr(self.config.policy_kwargs, '__dict__'):
            policy_kwargs = asdict(self.config.policy_kwargs)
        else:
            policy_kwargs = self.config.policy_kwargs if isinstance(self.config.policy_kwargs, dict) else {}
            
        # Let Policy create and manage its own encoder completely
        # Don't store any encoder reference at MLPBCPolicy level
        
        # Calculate input_dim first
        temp_encoder = SACObservationEncoder(self.config, self.normalize_inputs)
        input_dim = temp_encoder.output_dim
        del temp_encoder  # Clean up temporary encoder
        
        self.actor = Policy(
            encoder=SACObservationEncoder(self.config, self.normalize_inputs),  # Policy owns the encoder
            network=MLP(  # ä¸»å¹²ç½‘ç»œï¼šå¤šå±‚æ„ŸçŸ¥æœº
                input_dim=input_dim,  # è¾“å…¥ç»´åº¦ï¼šè§‚æµ‹ç¼–ç çš„ç»´åº¦
                **actor_network_kwargs,  # ç½‘ç»œé…ç½®å‚æ•°ï¼ˆéšè—å±‚ç»´åº¦ã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰
            ),
            action_dim=continuous_action_dim,  # åŠ¨ä½œç»´åº¦ï¼šè¿ç»­åŠ¨ä½œçš„ç»´åº¦
            encoder_is_shared=False,  # For BC, we don't share encoders (no critic)
            **policy_kwargs,  # ç­–ç•¥é…ç½®å‚æ•°ï¼ˆæ ‡å‡†å·®èŒƒå›´ã€æ˜¯å¦ä½¿ç”¨tanhç­‰ï¼‰
        )
        
        # DO NOT store encoder reference to avoid duplication
        # self.encoder_actor = None  # Explicitly set to None

class MLP(nn.Module):
    """Multi-layer perceptron builder.

    Dynamically constructs a sequence of layers based on `hidden_dims`:
      1) Linear (in_dim -> out_dim)
      2) Optional Dropout if `dropout_rate` > 0 and (not final layer or `activate_final`)
      3) LayerNorm on the output features
      4) Activation (standard for intermediate layers, `final_activation` for last layer if `activate_final`)

    Arguments:
        input_dim (int): Size of input feature dimension.
        hidden_dims (list[int]): Sizes for each hidden layer.
        activations (Callable or str): Activation to apply between layers.
        activate_final (bool): Whether to apply activation at the final layer.
        dropout_rate (Optional[float]): Dropout probability applied before normalization and activation.
        final_activation (Optional[Callable or str]): Activation for the final layer when `activate_final` is True.

    For each layer, `in_dim` is updated to the previous `out_dim`. All constructed modules are
    stored in `self.net` as an `nn.Sequential` container.
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dims: list[int],
        activations: Callable[[torch.Tensor], torch.Tensor] | str = nn.SiLU(),
        activate_final: bool = False,
        dropout_rate: float | None = None,
        final_activation: Callable[[torch.Tensor], torch.Tensor] | str | None = None,
    ):
        super().__init__()
        layers: list[nn.Module] = []
        in_dim = input_dim
        total = len(hidden_dims)

        for idx, out_dim in enumerate(hidden_dims):
            # 1) linear transform
            layers.append(nn.Linear(in_dim, out_dim))

            is_last = idx == total - 1
            # 2-4) optionally add dropout, normalization, and activation
            if not is_last or activate_final:
                if dropout_rate and dropout_rate > 0:
                    layers.append(nn.Dropout(p=dropout_rate))
                layers.append(nn.LayerNorm(out_dim))
                act_cls = final_activation if is_last and final_activation else activations
                act = act_cls if isinstance(act_cls, nn.Module) else getattr(nn, act_cls)()
                layers.append(act)

            in_dim = out_dim

        self.net = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)
    
class Policy(nn.Module):
    """
    SACç­–ç•¥ç½‘ç»œï¼ˆActorç½‘ç»œï¼‰
    
    ç½‘ç»œæ¶æ„ï¼š
    1. è§‚æµ‹ç¼–ç å™¨ (encoder): å°†åŸå§‹è§‚æµ‹è½¬æ¢ä¸ºç‰¹å¾å‘é‡
    2. ä¸»å¹²ç½‘ç»œ (network): å¤šå±‚æ„ŸçŸ¥æœºï¼Œå¤„ç†ç¼–ç åçš„è§‚æµ‹ç‰¹å¾
    3. å‡å€¼å±‚ (mean_layer): è¾“å‡ºåŠ¨ä½œçš„å‡å€¼
    4. æ ‡å‡†å·®å±‚ (std_layer): è¾“å‡ºåŠ¨ä½œçš„æ ‡å‡†å·®ï¼ˆç”¨äºæ¢ç´¢ï¼‰
    
    å‰å‘ä¼ æ’­æµç¨‹ï¼š
    è§‚æµ‹è¾“å…¥ â†’ ç¼–ç å™¨ â†’ è§‚æµ‹ç‰¹å¾ â†’ ä¸»å¹²ç½‘ç»œ â†’ ç‰¹å¾å‘é‡
                                        â†“
                                    å‡å€¼å±‚ â†’ åŠ¨ä½œå‡å€¼
                                    æ ‡å‡†å·®å±‚ â†’ åŠ¨ä½œæ ‡å‡†å·®
                                        â†“
                                    TanhMultivariateNormalDiag â†’ é‡‡æ ·åŠ¨ä½œ
    
    è¾“å‡ºï¼š
    - actions: é‡‡æ ·çš„åŠ¨ä½œ
    - log_probs: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
    - means: åŠ¨ä½œçš„å‡å€¼ï¼ˆç”¨äºç¡®å®šæ€§åŠ¨ä½œé€‰æ‹©ï¼‰
    """
    def __init__(
        self,
        encoder: SACObservationEncoder,  # è§‚æµ‹ç¼–ç å™¨
        network: nn.Module,  # ä¸»å¹²ç½‘ç»œï¼ˆé€šå¸¸æ˜¯MLPï¼‰
        action_dim: int,  # åŠ¨ä½œç»´åº¦
        std_min: float = -5,  # æ ‡å‡†å·®çš„æœ€å°å€¼ï¼ˆlogç©ºé—´ï¼‰
        std_max: float = 2,  # æ ‡å‡†å·®çš„æœ€å¤§å€¼ï¼ˆlogç©ºé—´ï¼‰
        fixed_std: torch.Tensor | None = None,  # å›ºå®šçš„æ ‡å‡†å·®ï¼ˆå¦‚æœä¸ºNoneåˆ™å­¦ä¹ ï¼‰
        init_final: float | None = None,  # æœ€ç»ˆå±‚çš„åˆå§‹åŒ–å‚æ•°
        use_tanh_squash: bool = False,  # æ˜¯å¦ä½¿ç”¨tanhå‹ç¼©åŠ¨ä½œ
        encoder_is_shared: bool = False,  # ç¼–ç å™¨æ˜¯å¦ä¸Criticå…±äº«
    ):
        super().__init__()
        self.encoder: SACObservationEncoder = encoder  # è§‚æµ‹ç¼–ç å™¨
        self.network = network  # ä¸»å¹²ç½‘ç»œ
        self.action_dim = action_dim  # åŠ¨ä½œç»´åº¦
        self.std_min = std_min  # æ ‡å‡†å·®æœ€å°å€¼
        self.std_max = std_max  # æ ‡å‡†å·®æœ€å¤§å€¼
        self.fixed_std = fixed_std  # å›ºå®šæ ‡å‡†å·®
        self.use_tanh_squash = use_tanh_squash  # æ˜¯å¦ä½¿ç”¨tanhå‹ç¼©
        self.encoder_is_shared = encoder_is_shared  # ç¼–ç å™¨æ˜¯å¦å…±äº«

        # æ‰¾åˆ°ä¸»å¹²ç½‘ç»œæœ€åä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦
        for layer in reversed(network.net):
            if isinstance(layer, nn.Linear):
                out_features = layer.out_features
                break
        
        # å‡å€¼å±‚ï¼šå°†ä¸»å¹²ç½‘ç»œçš„è¾“å‡ºæ˜ å°„ä¸ºåŠ¨ä½œå‡å€¼
        self.mean_layer = nn.Linear(out_features, action_dim)
        if init_final is not None:
            # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
            nn.init.uniform_(self.mean_layer.weight, -init_final, init_final)
            nn.init.uniform_(self.mean_layer.bias, -init_final, init_final)
        else:
            # ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–
            orthogonal_init()(self.mean_layer.weight)

        # æ ‡å‡†å·®å±‚ï¼šå°†ä¸»å¹²ç½‘ç»œçš„è¾“å‡ºæ˜ å°„ä¸ºåŠ¨ä½œæ ‡å‡†å·®
        if fixed_std is None:
            # å¦‚æœä½¿ç”¨å­¦ä¹ å‹æ ‡å‡†å·®
            self.std_layer = nn.Linear(out_features, action_dim)
            if init_final is not None:
                # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
                nn.init.uniform_(self.std_layer.weight, -init_final, init_final)
                nn.init.uniform_(self.std_layer.bias, -init_final, init_final)
            else:
                # ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–
                orthogonal_init()(self.std_layer.weight)

    def forward(
        self,
        observations: torch.Tensor,  # è§‚æµ‹è¾“å…¥
        observation_features: torch.Tensor | None = None,  # é¢„è®¡ç®—çš„è§‚æµ‹ç‰¹å¾
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            observations: è§‚æµ‹å­—å…¸
            observation_features: é¢„è®¡ç®—çš„è§‚æµ‹ç‰¹å¾ï¼ˆç”¨äºç¼“å­˜ï¼‰
            
        Returns:
            actions: é‡‡æ ·çš„åŠ¨ä½œ
            log_probs: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            means: åŠ¨ä½œçš„å‡å€¼
        """
        # å¦‚æœç¼–ç å™¨æ˜¯å…±äº«çš„ï¼Œåˆ™åˆ†ç¦»æ¢¯åº¦ä»¥é¿å…é€šè¿‡ç¼–ç å™¨è¿›è¡Œåå‘ä¼ æ’­
        # è¿™å¾ˆé‡è¦ï¼Œå¯ä»¥é¿å…ç¼–ç å™¨é€šè¿‡ç­–ç•¥ç½‘ç»œæ›´æ–°
        obs_enc = self.encoder(observations, cache=observation_features, detach=self.encoder_is_shared)

        # è·å–ä¸»å¹²ç½‘ç»œçš„è¾“å‡º
        outputs = self.network(obs_enc)
        
        # è®¡ç®—åŠ¨ä½œå‡å€¼
        means = self.mean_layer(outputs)

        # è®¡ç®—åŠ¨ä½œæ ‡å‡†å·®
        if self.fixed_std is None:
            # ä½¿ç”¨å­¦ä¹ å‹æ ‡å‡†å·®
            log_std = self.std_layer(outputs)  # è¾“å‡ºlogæ ‡å‡†å·®
            std = torch.exp(log_std)  # è½¬æ¢ä¸ºæ ‡å‡†å·®
            std = torch.clamp(std, self.std_min, self.std_max)  # è£å‰ªåˆ°æŒ‡å®šèŒƒå›´
        else:
            # ä½¿ç”¨å›ºå®šæ ‡å‡†å·®
            std = self.fixed_std.expand_as(means)

        # æ„å»ºå˜æ¢åˆ†å¸ƒï¼šä½¿ç”¨tanhå˜æ¢çš„å¤šå…ƒæ­£æ€åˆ†å¸ƒ
        dist = TanhMultivariateNormalDiag(loc=means, scale_diag=std)

        # é‡‡æ ·åŠ¨ä½œï¼ˆä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§ï¼‰
        actions = dist.rsample()

        # è®¡ç®—åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        log_probs = dist.log_prob(actions)

        return actions, log_probs, means

    def get_features(self, observations: torch.Tensor) -> torch.Tensor:
        """è·å–è§‚æµ‹çš„ç¼–ç ç‰¹å¾"""
        device = get_device_from_parameters(self)
        observations = observations.to(device)
        if self.encoder is not None:
            with torch.inference_mode():
                return self.encoder(observations)
        return observations