#!/usr/bin/env python3
"""
ACT-SAC Quick Start å¿«é€Ÿå¼€å§‹

æœ€ç®€å•çš„ACT-SACä½¿ç”¨ç¤ºä¾‹ï¼Œ5åˆ†é’Ÿä¸Šæ‰‹
"""

import torch
from lerobot.common.policies.sac.configuration_sac import SACConfig
from lerobot.common.policies.sac.modeling_sac import SACPolicy
from lerobot.configs.types import PolicyFeature, FeatureType


def quick_start_example():
    """5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹ACT-SAC"""
    
    print("ğŸš€ ACT-SAC Quick Start")
    print("=" * 50)
    
    # æ­¥éª¤1: åˆ›å»ºé…ç½®
    print("\nğŸ“ æ­¥éª¤1: åˆ›å»ºé…ç½®")
    config = SACConfig(
        # å®šä¹‰è¾“å…¥è¾“å‡ºç‰¹å¾
        input_features={
            "observation.state": PolicyFeature(type=FeatureType.STATE, shape=(10,)),
        },
        output_features={
            "action": PolicyFeature(type=FeatureType.ACTION, shape=(4,)),
        },
        
        # å¯ç”¨ACT Actorï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
        use_act_actor=True,
        
        # è½»é‡åŒ–é…ç½®ç”¨äºå¿«é€Ÿæµ‹è¯•
        act_dim_model=128,
        act_n_heads=4,
        act_n_encoder_layers=2,
        
        # BCæ··åˆè®­ç»ƒ
        bc_initial_weight=0.5,
        bc_final_weight=0.1,
        bc_decay_steps=1000,
        
        # å½’ä¸€åŒ–é…ç½®
        dataset_stats={
            "observation.state": {"min": [0.0] * 10, "max": [1.0] * 10},
            "action": {"min": [-1.0] * 4, "max": [1.0] * 4},
        },
        
        # ç¦ç”¨è§†è§‰åŠŸèƒ½ç®€åŒ–æµ‹è¯•
        disable_vision_features=True,
    )
    print("  âœ… é…ç½®åˆ›å»ºå®Œæˆ")
    
    # æ­¥éª¤2: åˆ›å»ºç­–ç•¥
    print("\nğŸ¤– æ­¥éª¤2: åˆ›å»ºç­–ç•¥")
    policy = SACPolicy(config=config)
    print(f"  âœ… ç­–ç•¥åˆ›å»ºå®Œæˆï¼ŒActorç±»å‹: {type(policy.actor).__name__}")
    
    # æ­¥éª¤3: åŠ¨ä½œæ¨ç†
    print("\nğŸ¯ æ­¥éª¤3: åŠ¨ä½œæ¨ç†")
    batch_size = 2
    observations = {
        "observation.state": torch.randn(batch_size, 10)
    }
    
    with torch.no_grad():
        actions = policy.select_action(observations)
    
    print(f"  ğŸ“Š è¾“å…¥è§‚æµ‹å½¢çŠ¶: {observations['observation.state'].shape}")
    print(f"  ğŸ“Š è¾“å‡ºåŠ¨ä½œå½¢çŠ¶: {actions.shape}")
    print(f"  ğŸ“Š åŠ¨ä½œå€¼èŒƒå›´: [{actions.min().item():.3f}, {actions.max().item():.3f}]")
    print("  âœ… åŠ¨ä½œæ¨ç†æˆåŠŸ")
    
    # æ­¥éª¤4: è®­ç»ƒæŸå¤±è®¡ç®—
    print("\nğŸ“ˆ æ­¥éª¤4: è®­ç»ƒæŸå¤±è®¡ç®—")
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    expert_actions = torch.randn(batch_size, 4)
    rewards = torch.randn(batch_size, 1)
    done = torch.zeros(batch_size, 1, dtype=torch.bool)
    next_observations = {
        "observation.state": torch.randn(batch_size, 10)
    }
    
    # æ„å»ºè®­ç»ƒæ‰¹æ¬¡
    batch = {
        "state": observations,
        "action": actions,
        "expert_action": expert_actions,  # ä¸“å®¶åŠ¨ä½œç”¨äºBCæŸå¤±
        "reward": rewards,
        "next_state": next_observations,
        "done": done,
        "training_step": 500,  # å½“å‰è®­ç»ƒæ­¥æ•°
    }
    
    # è®¡ç®—ActoræŸå¤±
    policy.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    loss_dict = policy.forward(batch, model="actor")
    
    print(f"  ğŸ“Š ActoræŸå¤±: {loss_dict['loss_actor'].item():.4f}")
    
    # æ£€æŸ¥BCæƒé‡
    bc_weight = policy._compute_dynamic_bc_weight(500)
    print(f"  ğŸ“Š å½“å‰BCæƒé‡: {bc_weight:.3f}")
    print("  âœ… æŸå¤±è®¡ç®—æˆåŠŸ")
    
    # æ­¥éª¤5: å¯¹æ¯”ä¼ ç»ŸSAC
    print("\nâš–ï¸ æ­¥éª¤5: å¯¹æ¯”ä¼ ç»ŸSAC")
    
    # åˆ›å»ºä¼ ç»ŸSACé…ç½®
    traditional_config = SACConfig(
        input_features=config.input_features,
        output_features=config.output_features,
        use_act_actor=False,  # ä½¿ç”¨ä¼ ç»ŸMLP Actor
        dataset_stats=config.dataset_stats,
        disable_vision_features=True,
    )
    
    traditional_policy = SACPolicy(config=traditional_config)
    
    # å‚æ•°é‡å¯¹æ¯”
    act_params = sum(p.numel() for p in policy.actor.parameters())
    mlp_params = sum(p.numel() for p in traditional_policy.actor.parameters())
    
    print(f"  ğŸ“Š ACT Actorå‚æ•°é‡: {act_params:,}")
    print(f"  ğŸ“Š MLP Actorå‚æ•°é‡: {mlp_params:,}")
    print(f"  ğŸ“Š å‚æ•°å¢é•¿å€æ•°: {act_params / mlp_params:.1f}x")
    print("  âœ… å¯¹æ¯”å®Œæˆ")
    
    print("\nğŸ‰ Quick Start å®Œæˆï¼")
    print("=" * 50)
    print("ğŸ“ ä¸‹ä¸€æ­¥:")
    print("  1. æŸ¥çœ‹å®Œæ•´æ–‡æ¡£: README_ACT_SAC.md")
    print("  2. è¿è¡Œè¯¦ç»†ç¤ºä¾‹: how_to_use_act_sac.py")
    print("  3. è‡ªå®šä¹‰é…ç½®: act_sac_config_example.py")
    print("  4. è¿è¡Œå®Œæ•´æµ‹è¯•: test_act_sac.py")


def compare_act_vs_mlp():
    """å¿«é€Ÿå¯¹æ¯”ACTå’ŒMLP Actor"""
    
    print("\nğŸ”¬ ACT vs MLP å¿«é€Ÿå¯¹æ¯”")
    print("-" * 30)
    
    # é€šç”¨é…ç½®
    base_config = {
        "input_features": {"observation.state": PolicyFeature(type=FeatureType.STATE, shape=(15,))},
        "output_features": {"action": PolicyFeature(type=FeatureType.ACTION, shape=(6,))},
        "dataset_stats": {
            "observation.state": {"min": [0.0] * 15, "max": [1.0] * 15},
            "action": {"min": [-1.0] * 6, "max": [1.0] * 6},
        },
        "disable_vision_features": True,
    }
    
    configs = {
        "MLP Actor": SACConfig(**base_config, use_act_actor=False),
        "ACT Actor": SACConfig(**base_config, use_act_actor=True, act_dim_model=256),
    }
    
    print(f"{'ç±»å‹':<12} {'å‚æ•°é‡':<10} {'æ¨ç†æ—¶é—´':<10} {'å†…å­˜å ç”¨':<10}")
    print("-" * 50)
    
    for name, config in configs.items():
        policy = SACPolicy(config=config)
        params = sum(p.numel() for p in policy.actor.parameters())
        
        # ç®€å•çš„æ¨ç†æ—¶é—´æµ‹è¯•
        obs = {"observation.state": torch.randn(1, 15)}
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                policy.select_action(obs)
        
        # è®¡æ—¶
        import time
        start_time = time.time()
        with torch.no_grad():
            for _ in range(100):
                policy.select_action(obs)
        inference_time = (time.time() - start_time) / 100 * 1000  # ms
        
        # å†…å­˜ä½¿ç”¨ï¼ˆç®€å•ä¼°è®¡ï¼‰
        memory_mb = params * 4 / 1024 / 1024  # å‡è®¾float32
        
        print(f"{name:<12} {params:<10,} {inference_time:<8.2f}ms {memory_mb:<8.1f}MB")


if __name__ == "__main__":
    try:
        quick_start_example()
        compare_act_vs_mlp()
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ è§£å†³å»ºè®®:")
        print("  1. ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–")
        print("  2. æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®")
        print("  3. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£è·å–å¸®åŠ©")
