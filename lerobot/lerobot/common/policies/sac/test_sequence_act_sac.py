#!/usr/bin/env python3
"""
åºåˆ—ACT-SACæµ‹è¯•è„šæœ¬

æµ‹è¯•çœŸæ­£çš„åŠ¨ä½œåºåˆ—é¢„æµ‹å’Œè”åˆæ¦‚ç‡æŸå¤±è®¡ç®—
"""

import torch
import torch.nn as nn
import logging
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# æµ‹è¯•å¯¼å…¥
try:
    from lerobot.common.policies.sac.configuration_sac import SACConfig
    from lerobot.common.policies.sac.modeling_sac import SACPolicy
    from lerobot.common.policies.sac.modeling_sac_sequence_act_actor import SequenceACTSACActorV2
    from lerobot.configs.types import PolicyFeature, FeatureType
    print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    exit(1)


def create_sequence_test_config():
    """åˆ›å»ºåºåˆ—ACTæµ‹è¯•é…ç½®"""
    
    config = SACConfig(
        # åŸºç¡€ç‰¹å¾
        input_features={
            "observation.state": PolicyFeature(type=FeatureType.STATE, shape=(12,)),
        },
        output_features={
            "action": PolicyFeature(type=FeatureType.ACTION, shape=(4,)),
        },
        
        # å¯ç”¨åºåˆ—ACT Actor
        use_act_actor=True,
        use_sequence_act_actor=True,
        
        # åºåˆ—å‚æ•°
        obs_history_length=1, # è§‚æµ‹å†å²é•¿åº¦
        act_chunk_size=5,  # é¢„æµ‹5æ­¥åŠ¨ä½œåºåˆ—
        
        # è½»é‡åŒ–é…ç½®ç”¨äºæµ‹è¯•
        act_dim_model=128,
        act_n_heads=4,
        act_dim_feedforward=256,
        act_n_encoder_layers=2,
        act_n_decoder_layers=2,
        act_dropout=0.1,
        
        # SACé…ç½®
        num_critics=2,
        shared_encoder=True,
        latent_dim=64,
        
        # BCé…ç½®
        bc_initial_weight=0.5,
        bc_final_weight=0.1,
        bc_decay_steps=1000,
        
        # å½’ä¸€åŒ–é…ç½®
        dataset_stats={
            "observation.state": {
                "min": [0.0] * 12,
                "max": [1.0] * 12,
            },
            "action": {
                "min": [-1.0] * 4,
                "max": [1.0] * 4,
            },
        },
        
        disable_vision_features=True,
    )
    
    return config


def test_sequence_action_prediction():
    """æµ‹è¯•åŠ¨ä½œåºåˆ—é¢„æµ‹"""
    print("\nğŸš€ æµ‹è¯•åŠ¨ä½œåºåˆ—é¢„æµ‹")
    
    config = create_sequence_test_config()
    policy = SACPolicy(config=config)
    
    batch_size = 2
    
    # åˆ›å»ºè§‚æµ‹åºåˆ—
    obs_sequence = []
    for t in range(config.obs_history_length):
        obs = {
            "observation.state": torch.randn(batch_size, 12) * 0.5 + 0.5
        }
        obs_sequence.append(obs)
    
    print(f"  ğŸ“Š è§‚æµ‹åºåˆ—é•¿åº¦: {len(obs_sequence)}")
    print(f"  ğŸ“Š æ¯ä¸ªè§‚æµ‹å½¢çŠ¶: {obs_sequence[0]['observation.state'].shape}")
    
    # æµ‹è¯•åºåˆ—é¢„æµ‹
    with torch.no_grad():
        # é¢„æµ‹å®Œæ•´åŠ¨ä½œåºåˆ—
        action_sequence, log_probs_joint, means_sequence = policy.actor(
            obs_sequence, 
            return_sequence=True
        )
        
        print(f"  âœ… åŠ¨ä½œåºåˆ—å½¢çŠ¶: {action_sequence.shape}")  # åº”è¯¥æ˜¯ (batch, chunk_size, action_dim)
        print(f"  âœ… è”åˆæ¦‚ç‡å½¢çŠ¶: {log_probs_joint.shape}")  # åº”è¯¥æ˜¯ (batch,)
        print(f"  âœ… å‡å€¼åºåˆ—å½¢çŠ¶: {means_sequence.shape}")  # åº”è¯¥æ˜¯ (batch, chunk_size, action_dim)
        
        # é¢„æµ‹å•ä¸ªåŠ¨ä½œï¼ˆç”¨äºSACï¼‰
        single_action, single_log_prob, single_mean = policy.actor(
            obs_sequence,
            return_sequence=False
        )
        
        print(f"  âœ… å•ä¸ªåŠ¨ä½œå½¢çŠ¶: {single_action.shape}")  # åº”è¯¥æ˜¯ (batch, action_dim)
        print(f"  âœ… å•ä¸ªæ¦‚ç‡å½¢çŠ¶: {single_log_prob.shape}")  # åº”è¯¥æ˜¯ (batch,)
        
        # éªŒè¯å•ä¸ªåŠ¨ä½œæ˜¯åºåˆ—çš„ç¬¬ä¸€ä¸ª
        first_from_sequence = action_sequence[:, 0, :]
        print(f"  ğŸ“Š å•ä¸ªåŠ¨ä½œä¸åºåˆ—ç¬¬ä¸€ä¸ªçš„å·®å¼‚: {torch.abs(single_action - first_from_sequence).mean().item():.6f}")


def test_sequence_loss_computation():
    """æµ‹è¯•åºåˆ—æŸå¤±è®¡ç®—"""
    print("\nğŸ“ˆ æµ‹è¯•åºåˆ—æŸå¤±è®¡ç®—")
    
    config = create_sequence_test_config()
    policy = SACPolicy(config=config)
    
    batch_size = 2
    chunk_size = config.act_chunk_size
    action_dim = 4
    
    # åˆ›å»ºè§‚æµ‹åºåˆ—
    obs_sequence = []
    for t in range(config.obs_history_length):
        obs = {
            "observation.state": torch.randn(batch_size, 12) * 0.5 + 0.5
        }
        obs_sequence.append(obs)
    
    # åˆ›å»ºä¸“å®¶åŠ¨ä½œåºåˆ—
    expert_action_sequences = torch.randn(batch_size, chunk_size, action_dim) * 0.5
    
    print(f"  ğŸ“Š ä¸“å®¶åŠ¨ä½œåºåˆ—å½¢çŠ¶: {expert_action_sequences.shape}")
    
    # æµ‹è¯•åºåˆ—æŸå¤±è®¡ç®—
    policy.train()
    
    # æ„å»ºè®­ç»ƒæ‰¹æ¬¡
    batch = {
        "state": obs_sequence,  # æ³¨æ„ï¼šè¿™é‡Œä¼ é€’çš„æ˜¯è§‚æµ‹åºåˆ—
        "expert_action_sequences": expert_action_sequences,
        "training_step": 500,
    }
    
    # è®¡ç®—ActoræŸå¤±
    loss_dict = policy.forward(batch, model="actor")
    
    print(f"  âœ… ActoræŸå¤±: {loss_dict['loss_actor'].item():.4f}")
    print(f"  ğŸ“Š BCæƒé‡: {policy._last_bc_weight:.3f}")
    print(f"  ğŸ“Š åºåˆ—é•¿åº¦: {policy._last_sequence_length}")
    print(f"  ğŸ“Š è”åˆå¯¹æ•°æ¦‚ç‡: {policy._last_joint_log_prob.item():.4f}")


def test_sequence_vs_single_comparison():
    """å¯¹æ¯”åºåˆ—é¢„æµ‹å’Œå•æ­¥é¢„æµ‹"""
    print("\nâš–ï¸ å¯¹æ¯”åºåˆ—é¢„æµ‹å’Œå•æ­¥é¢„æµ‹")
    
    # å•æ­¥é…ç½®
    single_config = create_sequence_test_config()
    single_config.use_sequence_act_actor = False  # ä½¿ç”¨åŸºç¡€ACT
    single_policy = SACPolicy(config=single_config)
    
    # åºåˆ—é…ç½®
    sequence_config = create_sequence_test_config()
    sequence_policy = SACPolicy(config=sequence_config)
    
    batch_size = 2
    
    # å•ä¸ªè§‚æµ‹
    obs = {
        "observation.state": torch.randn(batch_size, 12) * 0.5 + 0.5
    }
    
    with torch.no_grad():
        # å•æ­¥é¢„æµ‹
        single_action, single_log_prob, _ = single_policy.actor(obs)
        
        # åºåˆ—é¢„æµ‹ï¼ˆç¬¬ä¸€ä¸ªåŠ¨ä½œï¼‰
        sequence_action, sequence_log_prob, _ = sequence_policy.actor([obs], return_sequence=False)
        
        print(f"  ğŸ“Š å•æ­¥Actorå‚æ•°é‡: {sum(p.numel() for p in single_policy.actor.parameters()):,}")
        print(f"  ğŸ“Š åºåˆ—Actorå‚æ•°é‡: {sum(p.numel() for p in sequence_policy.actor.parameters()):,}")
        print(f"  ğŸ“Š å•æ­¥åŠ¨ä½œå½¢çŠ¶: {single_action.shape}")
        print(f"  ğŸ“Š åºåˆ—åŠ¨ä½œå½¢çŠ¶: {sequence_action.shape}")
        print(f"  ğŸ“Š å•æ­¥æ¦‚ç‡èŒƒå›´: [{single_log_prob.min().item():.3f}, {single_log_prob.max().item():.3f}]")
        print(f"  ğŸ“Š åºåˆ—æ¦‚ç‡èŒƒå›´: [{sequence_log_prob.min().item():.3f}, {sequence_log_prob.max().item():.3f}]")


def test_autoregressive_generation():
    """æµ‹è¯•è‡ªå›å½’ç”Ÿæˆ"""
    print("\nğŸ”„ æµ‹è¯•è‡ªå›å½’ç”Ÿæˆ")
    
    config = create_sequence_test_config()
    policy = SACPolicy(config=config)
    
    batch_size = 1
    
    # åˆ›å»ºè§‚æµ‹åºåˆ—
    obs_sequence = []
    for t in range(config.obs_history_length):
        obs = {
            "observation.state": torch.randn(batch_size, 12) * 0.5 + 0.5
        }
        obs_sequence.append(obs)
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆè§¦å‘è‡ªå›å½’ç”Ÿæˆï¼‰
    policy.eval()
    
    with torch.no_grad():
        # ç”ŸæˆåŠ¨ä½œåºåˆ—
        action_sequence, log_probs_joint, means_sequence = policy.actor(
            obs_sequence,
            return_sequence=True
        )
        
        print(f"  âœ… è‡ªå›å½’ç”Ÿæˆåºåˆ—å½¢çŠ¶: {action_sequence.shape}")
        print(f"  ğŸ“Š åŠ¨ä½œåºåˆ—å˜åŒ–èŒƒå›´:")
        for t in range(config.act_chunk_size):
            action_t = action_sequence[0, t, :]
            print(f"    æ­¥éª¤ {t}: åŠ¨ä½œèŒƒå›´ [{action_t.min().item():.3f}, {action_t.max().item():.3f}]")
        
        # éªŒè¯åŠ¨ä½œåºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§
        action_diffs = []
        for t in range(1, config.act_chunk_size):
            diff = torch.abs(action_sequence[0, t, :] - action_sequence[0, t-1, :]).mean()
            action_diffs.append(diff.item())
        
        avg_diff = np.mean(action_diffs)
        print(f"  ğŸ“Š å¹³å‡æ­¥é—´åŠ¨ä½œå·®å¼‚: {avg_diff:.4f}")


def test_bc_loss_with_sequences():
    """æµ‹è¯•BCæŸå¤±ä¸åŠ¨ä½œåºåˆ—"""
    print("\nğŸ“š æµ‹è¯•BCæŸå¤±ä¸åŠ¨ä½œåºåˆ—")
    
    config = create_sequence_test_config()
    policy = SACPolicy(config=config)
    
    batch_size = 3
    chunk_size = config.act_chunk_size
    action_dim = 4
    
    # åˆ›å»ºè§‚æµ‹åºåˆ—
    obs_sequence = []
    for t in range(config.obs_history_length):
        obs = {
            "observation.state": torch.randn(batch_size, 12) * 0.5 + 0.5
        }
        obs_sequence.append(obs)
    
    # åˆ›å»ºä¸åŒé•¿åº¦çš„ä¸“å®¶åŠ¨ä½œåºåˆ—è¿›è¡Œæµ‹è¯•
    print("  ğŸ” æµ‹è¯•ä¸åŒé•¿åº¦çš„ä¸“å®¶åºåˆ—:")
    
    # æµ‹è¯•1: å®Œæ•´é•¿åº¦åºåˆ—
    expert_full = torch.randn(batch_size, chunk_size, action_dim) * 0.5
    batch_full = {
        "state": obs_sequence,
        "expert_action_sequences": expert_full,
        "training_step": 100,
    }
    
    policy.train()
    loss_full = policy.forward(batch_full, model="actor")
    print(f"    å®Œæ•´åºåˆ— ({chunk_size}æ­¥): BCæŸå¤± {policy._last_bc_loss.item():.4f}")
    
    # æµ‹è¯•2: è¾ƒçŸ­åºåˆ—
    expert_short = torch.randn(batch_size, chunk_size-2, action_dim) * 0.5
    batch_short = {
        "state": obs_sequence,
        "expert_action_sequences": expert_short,
        "training_step": 100,
    }
    
    loss_short = policy.forward(batch_short, model="actor")
    print(f"    è¾ƒçŸ­åºåˆ— ({chunk_size-2}æ­¥): BCæŸå¤± {policy._last_bc_loss.item():.4f}")
    
    # æµ‹è¯•3: è¾ƒé•¿åºåˆ—
    expert_long = torch.randn(batch_size, chunk_size+3, action_dim) * 0.5
    batch_long = {
        "state": obs_sequence,
        "expert_action_sequences": expert_long,
        "training_step": 100,
    }
    
    loss_long = policy.forward(batch_long, model="actor")
    print(f"    è¾ƒé•¿åºåˆ— ({chunk_size+3}æ­¥): BCæŸå¤± {policy._last_bc_loss.item():.4f}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åºåˆ—ACT-SACæµ‹è¯•...")
    
    try:
        test_sequence_action_prediction()
        test_sequence_loss_computation()
        test_sequence_vs_single_comparison()
        test_autoregressive_generation()
        test_bc_loss_with_sequences()
        
        print("\nğŸ‰ æ‰€æœ‰åºåˆ—ACTæµ‹è¯•é€šè¿‡ï¼")
        
        print("\nğŸ“Š å…³é”®ç‰¹æ€§éªŒè¯:")
        print("  âœ… åŠ¨ä½œåºåˆ—é¢„æµ‹ (chunk-based)")
        print("  âœ… è”åˆæ¦‚ç‡æŸå¤±è®¡ç®—")
        print("  âœ… è‡ªå›å½’åºåˆ—ç”Ÿæˆ")
        print("  âœ… åºåˆ—BCæŸå¤±")
        print("  âœ… SACä¸åºåˆ—æŸå¤±é›†æˆ")
        
        print("\nğŸ’¡ è¿™ä¸ªå®ç°çš„æ ¸å¿ƒä¼˜åŠ¿:")
        print("  ğŸ”„ çœŸæ­£çš„åŠ¨ä½œåºåˆ—å»ºæ¨¡")
        print("  ğŸ¯ è”åˆæ¦‚ç‡ä¼˜åŒ–è€Œéç‹¬ç«‹åŠ¨ä½œ")
        print("  ğŸ§  å……åˆ†åˆ©ç”¨ACTçš„Transformeræ¶æ„")
        print("  âš¡ ä¸SACå¼ºåŒ–å­¦ä¹ æ¡†æ¶æ·±åº¦é›†æˆ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
