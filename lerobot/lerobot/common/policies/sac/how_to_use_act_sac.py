#!/usr/bin/env python3
"""
ACT-SAC ä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨ACT-SACæ··åˆæ¶æ„
"""

import torch
import numpy as np
from lerobot.common.policies.sac.configuration_sac import SACConfig
from lerobot.common.policies.sac.modeling_sac import SACPolicy
from lerobot.configs.types import PolicyFeature, FeatureType


def example_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸ“– åŸºç¡€ACT-SACä½¿ç”¨ç¤ºä¾‹")
    
    # 1. åˆ›å»ºé…ç½®
    config = SACConfig(
        # è¾“å…¥è¾“å‡ºç‰¹å¾å®šä¹‰
        input_features={
            "observation.state": PolicyFeature(type=FeatureType.STATE, shape=(20,)),  # 20ç»´æœºå™¨äººçŠ¶æ€
            "observation.image.front": PolicyFeature(type=FeatureType.VISUAL, shape=(3, 224, 224)),  # å‰ç½®æ‘„åƒå¤´
        },
        output_features={
            "action": PolicyFeature(type=FeatureType.ACTION, shape=(7,)),  # 7ç»´åŠ¨ä½œï¼ˆ6D pose + 1D gripperï¼‰
        },
        
        # å¯ç”¨ACT Actor
        use_act_actor=True,
        
        # ACTæ¶æ„é…ç½®
        act_dim_model=512,
        act_n_heads=8,
        act_n_encoder_layers=4,
        act_n_decoder_layers=1,
        
        # è§†è§‰ç¼–ç å™¨
        vision_encoder_name="helper2424/resnet10",
        freeze_vision_encoder=True,
        
        # BCæ··åˆè®­ç»ƒ
        bc_initial_weight=0.6,
        bc_final_weight=0.05,
        bc_decay_steps=100000,
        
        # SACå‚æ•°
        num_critics=2,
        critic_lr=3e-4,
        actor_lr=1e-4,  # ACT Actorå¯èƒ½éœ€è¦è¾ƒå°çš„å­¦ä¹ ç‡
        temperature_lr=3e-4,
        
        # å½’ä¸€åŒ–é…ç½®ï¼ˆåŒ¹é…æˆ‘ä»¬çš„ç‰¹å¾ç»´åº¦ï¼‰
        dataset_stats={
            "observation.state": {
                "min": [0.0] * 20,  # 20ç»´çŠ¶æ€çš„æœ€å°å€¼
                "max": [1.0] * 20,  # 20ç»´çŠ¶æ€çš„æœ€å¤§å€¼
            },
            "observation.image.front": {
                "mean": [0.485, 0.456, 0.406],  # RGBé€šé“å‡å€¼
                "std": [0.229, 0.224, 0.225],   # RGBé€šé“æ ‡å‡†å·®
            },
            "action": {
                "min": [-1.0] * 7,  # 7ç»´åŠ¨ä½œçš„æœ€å°å€¼
                "max": [1.0] * 7,   # 7ç»´åŠ¨ä½œçš„æœ€å¤§å€¼
            },
        },
    )
    
    # 2. åˆ›å»ºç­–ç•¥
    policy = SACPolicy(config=config)
    print(f"  âœ… åˆ›å»ºç­–ç•¥æˆåŠŸï¼ŒActorç±»å‹: {type(policy.actor).__name__}")
    
    # 3. æ¨¡æ‹Ÿæ¨ç†
    batch_size = 4
    observations = {
        "observation.state": torch.randn(batch_size, 20),
        "observation.image.front": torch.randn(batch_size, 3, 224, 224),
    }
    
    # é€‰æ‹©åŠ¨ä½œ
    with torch.no_grad():
        actions = policy.select_action(observations)
    print(f"  âœ… åŠ¨ä½œæ¨ç†æˆåŠŸï¼Œå½¢çŠ¶: {actions.shape}")
    
    # 4. æ¨¡æ‹Ÿè®­ç»ƒ
    expert_actions = torch.randn(batch_size, 7)
    
    batch = {
        "state": observations,
        "action": actions,
        "expert_action": expert_actions,
        "training_step": 1000,
    }
    
    loss_dict = policy.forward(batch, model="actor")
    print(f"  âœ… æŸå¤±è®¡ç®—æˆåŠŸï¼ŒActor loss: {loss_dict['loss_actor'].item():.4f}")


def example_sequence_usage():
    """åºåˆ—å¤„ç†ç¤ºä¾‹"""
    print("\nğŸ“š åºåˆ—ACT-SACä½¿ç”¨ç¤ºä¾‹")
    
    config = SACConfig(
        input_features={
            "observation.state": PolicyFeature(type=FeatureType.STATE, shape=(15,)),
        },
        output_features={
            "action": PolicyFeature(type=FeatureType.ACTION, shape=(5,)),
        },
        
        # å¯ç”¨åºåˆ—ACT Actor
        use_act_actor=True,
        use_sequence_act_actor=True,
        obs_history_length=8,  # ä½¿ç”¨8æ­¥å†å²
        
        # é€‚åº”åºåˆ—å¤„ç†çš„æ¶æ„
        act_dim_model=256,
        act_n_heads=8,
        act_n_encoder_layers=6,  # æ›´å¤šç¼–ç å™¨å±‚å¤„ç†åºåˆ—
        act_max_seq_length=10,
        
        # å…¶ä»–é…ç½®
        disable_vision_features=True,  # ä»…ä½¿ç”¨çŠ¶æ€è§‚æµ‹
        
        # å½’ä¸€åŒ–é…ç½®
        dataset_stats={
            "observation.state": {
                "min": [0.0] * 15,  # 15ç»´çŠ¶æ€çš„æœ€å°å€¼
                "max": [1.0] * 15,  # 15ç»´çŠ¶æ€çš„æœ€å¤§å€¼
            },
            "action": {
                "min": [-1.0] * 5,  # 5ç»´åŠ¨ä½œçš„æœ€å°å€¼
                "max": [1.0] * 5,   # 5ç»´åŠ¨ä½œçš„æœ€å¤§å€¼
            },
        },
    )
    
    policy = SACPolicy(config=config)
    print(f"  âœ… åˆ›å»ºåºåˆ—ç­–ç•¥æˆåŠŸï¼ŒActorç±»å‹: {type(policy.actor).__name__}")
    
    # æµ‹è¯•å•ä¸ªè§‚æµ‹
    observations = {"observation.state": torch.randn(2, 15)}
    actions = policy.select_action(observations)
    print(f"  âœ… å•è§‚æµ‹æ¨ç†æˆåŠŸï¼Œå½¢çŠ¶: {actions.shape}")


def example_training_loop():
    """è®­ç»ƒå¾ªç¯ç¤ºä¾‹"""
    print("\nğŸ”„ è®­ç»ƒå¾ªç¯ç¤ºä¾‹")
    
    config = SACConfig(
        input_features={"observation.state": PolicyFeature(type=FeatureType.STATE, shape=(10,))},
        output_features={"action": PolicyFeature(type=FeatureType.ACTION, shape=(4,))},
        use_act_actor=True,
        act_dim_model=128,  # è½»é‡åŒ–ç”¨äºæ¼”ç¤º
        bc_decay_steps=5000,
        
        # å½’ä¸€åŒ–é…ç½®
        dataset_stats={
            "observation.state": {
                "min": [0.0] * 10,
                "max": [1.0] * 10,
            },
            "action": {
                "min": [-1.0] * 4,
                "max": [1.0] * 4,
            },
        },
    )
    
    policy = SACPolicy(config=config)
    
    print("  å¼€å§‹æ¨¡æ‹Ÿè®­ç»ƒ...")
    
    for step in range(0, 6000, 1000):
        # æ¨¡æ‹Ÿæ•°æ®
        observations = {"observation.state": torch.randn(8, 10)}
        actions = torch.randn(8, 4)
        expert_actions = torch.randn(8, 4)
        
        # å‰å‘ä¼ æ’­
        batch = {
            "state": observations,
            "action": actions,
            "expert_action": expert_actions,
            "training_step": step,
        }
        
        loss_dict = policy.forward(batch, model="actor")
        
        # è·å–BCæƒé‡
        bc_weight = policy._compute_dynamic_bc_weight(step)
        
        print(f"    æ­¥æ•° {step:4d}: Actor Loss = {loss_dict['loss_actor'].item():.4f}, "
              f"BCæƒé‡ = {bc_weight:.3f}")
    
    print("  âœ… è®­ç»ƒå¾ªç¯æ¼”ç¤ºå®Œæˆ")


def example_config_comparison():
    """é…ç½®å¯¹æ¯”ç¤ºä¾‹"""
    print("\nâš–ï¸ é…ç½®å¯¹æ¯”ç¤ºä¾‹")
    
    base_features = {
        "input_features": {"observation.state": PolicyFeature(type=FeatureType.STATE, shape=(12,))},
        "output_features": {"action": PolicyFeature(type=FeatureType.ACTION, shape=(6,))},
        "dataset_stats": {
            "observation.state": {
                "min": [0.0] * 12,
                "max": [1.0] * 12,
            },
            "action": {
                "min": [-1.0] * 6,
                "max": [1.0] * 6,
            },
        },
    }
    
    configs = {
        "ä¼ ç»ŸMLP": SACConfig(**base_features, use_act_actor=False),
        "ACTåŸºç¡€": SACConfig(**base_features, use_act_actor=True, act_dim_model=256),
        "ACTåºåˆ—": SACConfig(**base_features, use_act_actor=True, use_sequence_act_actor=True, act_dim_model=256),
        "ACTè½»é‡": SACConfig(**base_features, use_act_actor=True, act_dim_model=128, act_n_encoder_layers=2),
    }
    
    for name, config in configs.items():
        try:
            policy = SACPolicy(config=config)
            
            # è®¡ç®—å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in policy.actor.parameters())
            
            print(f"  {name:8s}: âœ… å‚æ•°é‡ = {total_params:,}")
            
        except Exception as e:
            print(f"  {name:8s}: âŒ é”™è¯¯ = {e}")


def example_best_practices():
    """æœ€ä½³å®è·µç¤ºä¾‹"""
    print("\nğŸ’¡ æœ€ä½³å®è·µç¤ºä¾‹")
    
    # æ¨èé…ç½®
    config = SACConfig(
        input_features={
            "observation.state": PolicyFeature(type=FeatureType.STATE, shape=(25,)),
            "observation.image.front": PolicyFeature(type=FeatureType.VISUAL, shape=(3, 224, 224)),
        },
        output_features={"action": PolicyFeature(type=FeatureType.ACTION, shape=(8,))},
        
        # ============ ACTé…ç½® ============
        use_act_actor=True,
        
        # æ¶æ„å‚æ•°ï¼ˆå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡ï¼‰
        act_dim_model=512,
        act_n_heads=8,
        act_n_encoder_layers=4,
        act_n_decoder_layers=1,
        act_dropout=0.1,
        
        # ============ è§†è§‰é…ç½® ============
        vision_encoder_name="helper2424/resnet10",
        freeze_vision_encoder=True,
        shared_encoder=True,
        
        # ============ BCé…ç½® ============
        # æ¸è¿›å¼è¡°å‡ç­–ç•¥
        bc_initial_weight=0.7,      # åˆæœŸé‡è§†æ¨¡ä»¿å­¦ä¹ 
        bc_final_weight=0.02,       # åæœŸä¸»è¦ä¾é RL
        bc_decay_steps=80000,       # åœ¨æ€»è®­ç»ƒçš„å‰80%æ­¥æ•°å†…å®Œæˆè¡°å‡
        
        # ============ SACé…ç½® ============
        num_critics=2,
        critic_lr=3e-4,
        actor_lr=1e-4,              # ACT Actorä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
        temperature_lr=3e-4,
        discount=0.99,
        critic_target_update_weight=0.005,
        
        # ============ ä¼˜åŒ–é…ç½® ============
        use_torch_compile=True,     # å¯ç”¨ç¼–è¯‘åŠ é€Ÿ
        grad_clip_norm=10.0,        # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        
        # ============ å½’ä¸€åŒ–é…ç½® ============
        dataset_stats={
            "observation.state": {
                "min": [0.0] * 25,
                "max": [1.0] * 25,
            },
            "observation.image.front": {
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
            },
            "action": {
                "min": [-1.0] * 8,
                "max": [1.0] * 8,
            },
        },
    )
    
    print("  ğŸ“‹ æ¨èé…ç½®è¦ç‚¹:")
    print(f"    - ACTç»´åº¦: {config.act_dim_model}")
    print(f"    - BCåˆå§‹æƒé‡: {config.bc_initial_weight}")
    print(f"    - Actorå­¦ä¹ ç‡: {config.actor_lr}")
    print(f"    - æ¢¯åº¦è£å‰ª: {config.grad_clip_norm}")
    print("  âœ… è¿™æ˜¯ä¸€ä¸ªå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§çš„æ¨èé…ç½®")


if __name__ == "__main__":
    print("ğŸš€ ACT-SAC ä½¿ç”¨ç¤ºä¾‹\n")
    
    try:
        example_basic_usage()
        example_sequence_usage()
        example_training_loop()
        example_config_comparison()
        example_best_practices()
        
        print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")
        print("\nğŸ“ æ€»ç»“:")
        print("  1. ACT-SACæˆåŠŸèåˆäº†Transformerå’Œå¼ºåŒ–å­¦ä¹ ")
        print("  2. æ”¯æŒå•æ­¥å’Œåºåˆ—ä¸¤ç§æ¨¡å¼")
        print("  3. BCæ··åˆè®­ç»ƒæä¾›äº†å¼ºå¤§çš„åˆå§‹åŒ–èƒ½åŠ›")
        print("  4. é…ç½®çµæ´»ï¼Œå¯æ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´")
        
    except Exception as e:
        print(f"\nâŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
