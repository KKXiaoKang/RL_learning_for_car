#!/usr/bin/env python

# Copyright 2024 The HuggingFace Inc. team.
# All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import math
from dataclasses import asdict
from typing import Callable, Literal

import einops
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F  # noqa: N812
from torch import Tensor
from torch.distributions import MultivariateNormal, TanhTransform, Transform, TransformedDistribution

from lerobot.common.policies.normalize import NormalizeBuffer
from lerobot.common.policies.pretrained import PreTrainedPolicy
from lerobot.common.policies.sac.configuration_sac import SACConfig, is_image_feature
from lerobot.common.policies.utils import get_device_from_parameters

# Add ROS imports for visualization
try:
    import rospy
    from sensor_msgs.msg import Image, CompressedImage
    import cv2
    ROS_AVAILABLE = True
except ImportError:
    ROS_AVAILABLE = False
    rospy = None
    Image = None
    CompressedImage = None
    cv2 = None


DISCRETE_DIMENSION_INDEX = -1  # Gripper is always the last dimension


class SACPolicy(
    PreTrainedPolicy,
):
    config_class = SACConfig
    name = "sac"

    def __init__(
        self,
        config: SACConfig | None = None,
        dataset_stats: dict[str, dict[str, Tensor]] | None = None,
    ):
        # é€šè¿‡é…ç½®æ–‡ä»¶åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(config)
        # éªŒè¯è¾“å…¥è¾“å‡ºç‰¹å¾
        config.validate_features() 
        self.config = config

        # Determine action dimension and initialize all components
        continuous_action_dim = config.output_features["action"].shape[0] # ğŸ”¥ è·å–è¿ç»­åŠ¨ä½œç»´åº¦
        self._init_normalization(dataset_stats) # ğŸ”¥ åˆå§‹åŒ–å½’ä¸€åŒ–, é€šè¿‡ dataset_stats ä¸­çš„ min å’Œ max å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
        self._init_encoders() # ğŸ”¥ åˆå§‹åŒ–ç¼–ç å™¨
        self._init_critics(continuous_action_dim) # ğŸ”¥ åˆå§‹åŒ–critic
        self._init_actor(continuous_action_dim) # ğŸ”¥ åˆå§‹åŒ–actor
        self._init_temperature() # ğŸ”¥ åˆå§‹åŒ–æ¸©åº¦
        self._init_warmup_parameters() # ğŸ”¥ åˆå§‹åŒ–warm-upå‚æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰

        # Initialize member variable to store td_target for wandb logging
        self.last_td_target: Tensor | None = None
    
    def get_optim_params(self) -> dict:
        optim_params = {
            "actor": [
                p
                for n, p in self.actor.named_parameters()
                if not n.startswith("encoder") or not self.shared_encoder
            ],
            "critic": self.critic_ensemble.parameters(),
            "temperature": self.log_alpha,
        }
        if self.config.num_discrete_actions is not None:
            optim_params["discrete_critic"] = self.discrete_critic.parameters()
        return optim_params

    def reset(self):
        """Reset the policy"""
        pass

    @torch.no_grad()
    def select_action(self, batch: dict[str, Tensor]) -> Tensor:
        """Select action for inference/evaluation"""

        observations_features = None
        if self.shared_encoder and self.actor.encoder.has_images:
            # Cache and normalize image features
            observations_features = self.actor.encoder.get_cached_image_features(batch, normalize=True)

        # ğŸ”¥ Q-chunking: å¤„ç†åºåˆ—ACT Actorçš„åŠ¨ä½œé€‰æ‹©
        if hasattr(self.actor, 'chunk_size') and getattr(self.config, 'use_sequence_act_actor', False):
            # åºåˆ—ACT Actorï¼šè·å–åŠ¨ä½œåºåˆ—ï¼Œä½†åªè¿”å›ç¬¬ä¸€ä¸ªåŠ¨ä½œç”¨äºæ‰§è¡Œ
            action_sequence, _, _ = self.actor(
                batch, 
                observations_features, 
                return_sequence=True
            )
            # Q-chunkingæ ¸å¿ƒï¼šåªä½¿ç”¨åºåˆ—çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œè¿›è¡Œå®é™…æ‰§è¡Œ
            actions = action_sequence[:, 0, :]  # (batch, action_dim)
        else:
            # ä¼ ç»ŸActorï¼šç›´æ¥è·å–å•æ­¥åŠ¨ä½œ
            actions, _, _ = self.actor(batch, observations_features)

        if self.config.num_discrete_actions is not None:
            discrete_action_value = self.discrete_critic(batch, observations_features)
            discrete_action = torch.argmax(discrete_action_value, dim=-1, keepdim=True)
            actions = torch.cat([actions, discrete_action], dim=-1)

        return actions

    def critic_forward(
        self,
        observations: dict[str, Tensor],
        actions: Tensor,
        use_target: bool = False,
        observation_features: Tensor | None = None,
    ) -> Tensor:
        """Forward pass through a critic network ensemble

        Args:
            observations: Dictionary of observations
            actions: Action tensor
            use_target: If True, use target critics, otherwise use ensemble critics

        Returns:
            Tensor of Q-values from all critics
        """

        critics = self.critic_target if use_target else self.critic_ensemble
        q_values = critics(observations, actions, observation_features)
        return q_values

    def discrete_critic_forward(
        self, observations, use_target=False, observation_features=None
    ) -> torch.Tensor:
        """Forward pass through a discrete critic network

        Args:
            observations: Dictionary of observations
            use_target: If True, use target critics, otherwise use ensemble critics
            observation_features: Optional pre-computed observation features to avoid recomputing encoder output

        Returns:
            Tensor of Q-values from the discrete critic network
        """
        discrete_critic = self.discrete_critic_target if use_target else self.discrete_critic
        q_values = discrete_critic(observations, observation_features)
        return q_values

    def forward(
        self,
        batch: dict[str, Tensor | dict[str, Tensor]],
        model: Literal["actor", "critic", "temperature", "discrete_critic"] = "critic",
    ) -> dict[str, Tensor]:
        """Compute the loss for the given model

        Args:
            batch: Dictionary containing:
                - action: Action tensor
                - reward: Reward tensor
                - state: Observations tensor dict
                - next_state: Next observations tensor dict
                - done: Done mask tensor
                - observation_feature: Optional pre-computed observation features
                - next_observation_feature: Optional pre-computed next observation features
            model: Which model to compute the loss for ("actor", "critic", "discrete_critic", or "temperature")

        Returns:
            The computed loss tensor
        """
        # Extract common components from batch
        # æ³¨æ„ï¼šå¯¹äºåºåˆ—ACT Actorçš„æƒ…å†µï¼Œactionå¯èƒ½ä¸å­˜åœ¨
        actions: Tensor = batch.get("action")  # å¯¹äºåºåˆ—ACTå¯èƒ½ä¸ºNone
        observations: dict[str, Tensor] | list[dict[str, Tensor]] = batch["state"]  # å¯èƒ½æ˜¯åºåˆ—
        observation_features: Tensor = batch.get("observation_feature")

        if model == "critic":
            # Criticéœ€è¦actionsï¼Œç¡®ä¿å®ƒå­˜åœ¨
            if actions is None:
                raise ValueError("Critic loss computation requires 'action' in batch")
            
            # Extract critic-specific components
            rewards: Tensor = batch["reward"]
            next_observations: dict[str, Tensor] = batch["next_state"]
            done: Tensor = batch["done"]
            next_observation_features: Tensor = batch.get("next_observation_feature")

            loss_critic = self.compute_loss_critic(
                observations=observations,
                actions=actions,
                rewards=rewards,
                next_observations=next_observations,
                done=done,
                observation_features=observation_features,
                next_observation_features=next_observation_features,
            )

            return {"loss_critic": loss_critic}

        if model == "discrete_critic" and self.config.num_discrete_actions is not None:
            # Discrete criticä¹Ÿéœ€è¦actions
            if actions is None:
                raise ValueError("Discrete critic loss computation requires 'action' in batch")
            
            # Extract critic-specific components
            rewards: Tensor = batch["reward"]
            next_observations: dict[str, Tensor] = batch["next_state"]
            done: Tensor = batch["done"]
            next_observation_features: Tensor = batch.get("next_observation_feature")
            complementary_info = batch.get("complementary_info")
            loss_discrete_critic = self.compute_loss_discrete_critic(
                observations=observations,
                actions=actions,
                rewards=rewards,
                next_observations=next_observations,
                done=done,
                observation_features=observation_features,
                next_observation_features=next_observation_features,
                complementary_info=complementary_info,
            )
            return {"loss_discrete_critic": loss_discrete_critic}
        if model == "actor":
            # æå–ä¸“å®¶åŠ¨ä½œï¼ˆå¦‚æœå¯ç”¨ï¼‰å’Œè®­ç»ƒæ­¥æ•°
            expert_actions = batch.get("expert_action")
            expert_action_sequences = batch.get("expert_action_sequences")
            training_step = batch.get("training_step", 0)
            
            return {
                "loss_actor": self.compute_loss_actor(
                    observations=observations,
                    observation_features=observation_features,
                    expert_actions=expert_actions,
                    expert_action_sequences=expert_action_sequences,
                    training_step=training_step,
                )
            }

        if model == "temperature":
            return {
                "loss_temperature": self.compute_loss_temperature(
                    observations=observations,
                    observation_features=observation_features,
                )
            }

        raise ValueError(f"Unknown model type: {model}")

    def update_target_networks(self):
        """Update target networks with exponential moving average"""
        for target_param, param in zip(
            self.critic_target.parameters(),
            self.critic_ensemble.parameters(),
            strict=True,
        ):
            target_param.data.copy_(
                param.data * self.config.critic_target_update_weight
                + target_param.data * (1.0 - self.config.critic_target_update_weight)
            )
        if self.config.num_discrete_actions is not None:
            for target_param, param in zip(
                self.discrete_critic_target.parameters(),
                self.discrete_critic.parameters(),
                strict=True,
            ):
                target_param.data.copy_(
                    param.data * self.config.critic_target_update_weight
                    + target_param.data * (1.0 - self.config.critic_target_update_weight)
                )

    def update_temperature(self):
        self.temperature = self.log_alpha.exp().item()

    def compute_loss_critic(
        self,
        observations,
        actions,
        rewards,
        next_observations,
        done,
        observation_features: Tensor | None = None,
        next_observation_features: Tensor | None = None,
        use_q_chunking: bool = None,  # Q-chunkingå¼€å…³
    ) -> Tensor:
        """
        è®¡ç®—CriticæŸå¤±ï¼Œæ”¯æŒQ-chunkingçš„n-step TD learning
        
        Q-chunkingæ ¸å¿ƒæ”¹è¿›ï¼š
        1. ä½¿ç”¨åŠ¨ä½œåºåˆ—çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œè¿›è¡ŒQå€¼è®¡ç®—
        2. ä½¿ç”¨n-step TD targetsè¿›è¡Œæ›´ç¨³å®šçš„å­¦ä¹ 
        3. è€ƒè™‘åŠ¨ä½œåºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨Q-chunking
        if use_q_chunking is None:
            use_q_chunking = (hasattr(self.actor, 'chunk_size') and 
                             getattr(self.config, 'use_sequence_act_actor', False) and
                             getattr(self.config, 'enable_q_chunking', True))
        
        with torch.no_grad():
            if use_q_chunking:
                # Q-chunkingæ¨¡å¼ï¼šä½¿ç”¨åŠ¨ä½œåºåˆ—è¿›è¡Œé¢„æµ‹
                td_target = self._compute_q_chunking_td_target(
                    next_observations=next_observations,
                    rewards=rewards,
                    done=done,
                    next_observation_features=next_observation_features
                )
            else:
                # ä¼ ç»ŸSACæ¨¡å¼ï¼šå•æ­¥TD target
                td_target = self._compute_traditional_td_target(
                    next_observations=next_observations,
                    rewards=rewards,
                    done=done,
                    next_observation_features=next_observation_features
                )

            # Store td_target for wandb logging (detach to avoid affecting gradients)
            self.last_td_target = td_target.detach().clone()

        # è®¡ç®—å½“å‰Qå€¼é¢„æµ‹
        q_preds = self._compute_current_q_values(
            observations=observations,
            actions=actions,
            observation_features=observation_features,
            use_q_chunking=use_q_chunking
        )

        # è®¡ç®—TDæŸå¤±
        td_target_duplicate = einops.repeat(td_target, "b -> e b", e=q_preds.shape[0])
        critics_loss = (
            F.mse_loss(
                input=q_preds,
                target=td_target_duplicate,
                reduction="none",
            ).mean(dim=1)
        ).sum()
        
        return critics_loss
    
    def _compute_q_chunking_td_target(
        self,
        next_observations,
        rewards,
        done,
        next_observation_features: Tensor | None = None,
    ) -> Tensor:
        """
        è®¡ç®—Q-chunkingçš„TD target
        
        Q-chunkingçš„æ ¸å¿ƒåˆ›æ–°ï¼š
        1. ä½¿ç”¨åŠ¨ä½œåºåˆ—çš„è”åˆæ¦‚ç‡è¿›è¡Œç­–ç•¥è¯„ä¼°
        2. ä½¿ç”¨n-step returnsè¿›è¡Œæ›´ç¨³å®šçš„å€¼å‡½æ•°ä¼°è®¡
        3. åœ¨"chunked"åŠ¨ä½œç©ºé—´ä¸­è¿è¡ŒSAC
        """
        # 1. è·å–ä¸‹ä¸€çŠ¶æ€çš„åŠ¨ä½œåºåˆ—é¢„æµ‹
        next_action_sequence, next_log_probs_joint, _ = self.actor(
            next_observations, 
            next_observation_features, 
            return_sequence=True
        )
        
        # 2. ä½¿ç”¨åºåˆ—çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œè®¡ç®—Qå€¼ï¼ˆQ-chunkingçš„å…³é”®ï¼‰
        next_first_action = next_action_sequence[:, 0, :]  # (batch, action_dim)
        
        # 3. è®¡ç®—ç›®æ ‡Qå€¼
        q_targets = self.critic_forward(
            observations=next_observations,
            actions=next_first_action,
            use_target=True,
            observation_features=next_observation_features,
        )

        # 4. Critic ensembleå­é‡‡æ ·ï¼ˆé˜²æ­¢é«˜UTDæ—¶è¿‡æ‹Ÿåˆï¼‰
        if self.config.num_subsample_critics is not None:
            indices = torch.randperm(self.config.num_critics)
            indices = indices[: self.config.num_subsample_critics]
            q_targets = q_targets[indices]

        # 5. è®¡ç®—æœ€å°Qå€¼ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
        min_q, _ = q_targets.min(dim=0)
        
        # 6. Q-chunkingæ ¸å¿ƒï¼šä½¿ç”¨è”åˆå¯¹æ•°æ¦‚ç‡è¿›è¡Œç†µæ­£åˆ™åŒ–
        if self.config.use_backup_entropy:
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯æ•´ä¸ªåŠ¨ä½œåºåˆ—çš„è”åˆæ¦‚ç‡ï¼Œè€Œä¸æ˜¯å•ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
            min_q = min_q - (self.temperature * next_log_probs_joint)

        # 7. è®¡ç®—n-step TD targetï¼ˆQ-chunkingçš„å¦ä¸€ä¸ªå…³é”®æ”¹è¿›ï¼‰
        chunk_size = getattr(self.actor, 'chunk_size', 1)
        if chunk_size > 1 and hasattr(self.actor, 'compute_n_step_returns'):
            try:
                # ä½¿ç”¨åºåˆ—Actorçš„n-step returnè®¡ç®—èƒ½åŠ›
                n_step_returns = self.actor.compute_n_step_returns(
                    rewards=rewards,
                    next_observations=next_observations,
                    done=done,
                    gamma=self.config.discount,
                    observation_features=next_observation_features
                )
                # n-step TD target: R_t + Î³^n * Q_target(s_{t+n}, a_{t+n})
                td_target = n_step_returns + (1 - done) * (self.config.discount ** chunk_size) * min_q
                
                logging.debug(f"Q-chunking using {chunk_size}-step TD target")
            except Exception as e:
                logging.warning(f"Failed to compute n-step returns, falling back to 1-step: {e}")
                td_target = rewards + (1 - done) * self.config.discount * min_q
        else:
            # é™çº§åˆ°æ ‡å‡†1-step TD target
            td_target = rewards + (1 - done) * self.config.discount * min_q

        return td_target
    
    def _compute_traditional_td_target(
        self,
        next_observations,
        rewards,
        done,
        next_observation_features: Tensor | None = None,
    ) -> Tensor:
        """è®¡ç®—ä¼ ç»ŸSACçš„TD target"""
        # ä¼ ç»ŸActorï¼šç›´æ¥è·å–å•æ­¥åŠ¨ä½œ
        next_action_preds, next_log_probs, _ = self.actor(next_observations, next_observation_features)

        # è®¡ç®—ç›®æ ‡Qå€¼
        q_targets = self.critic_forward(
            observations=next_observations,
            actions=next_action_preds,
            use_target=True,
            observation_features=next_observation_features,
        )

        # Critic ensembleå­é‡‡æ ·
        if self.config.num_subsample_critics is not None:
            indices = torch.randperm(self.config.num_critics)
            indices = indices[: self.config.num_subsample_critics]
            q_targets = q_targets[indices]

        # è®¡ç®—æœ€å°Qå€¼
        min_q, _ = q_targets.min(dim=0)
        if self.config.use_backup_entropy:
            min_q = min_q - (self.temperature * next_log_probs)

        # æ ‡å‡†1-step TD target
        td_target = rewards + (1 - done) * self.config.discount * min_q
        return td_target
    
    def _compute_current_q_values(
        self,
        observations,
        actions,
        observation_features: Tensor | None = None,
        use_q_chunking: bool = False,
    ) -> Tensor:
        """è®¡ç®—å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå€¼"""
        if use_q_chunking:
            # Q-chunking: å¤„ç†å¯èƒ½çš„åŠ¨ä½œåºåˆ—è¾“å…¥
            if len(actions.shape) == 3:
                # åŠ¨ä½œåºåˆ—è¾“å…¥ï¼šåªä½¿ç”¨ç¬¬ä¸€ä¸ªåŠ¨ä½œè®¡ç®—Qå€¼
                current_actions = actions[:, 0, :]  # (batch, action_dim)
            else:
                # å•ä¸ªåŠ¨ä½œè¾“å…¥
                current_actions = actions
        else:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨åŠ¨ä½œ
            current_actions = actions
            
        # å¤„ç†ç¦»æ•£åŠ¨ä½œï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.config.num_discrete_actions is not None:
            current_actions = current_actions[:, :DISCRETE_DIMENSION_INDEX]
            
        # è®¡ç®—Qå€¼
        q_preds = self.critic_forward(
            observations=observations,
            actions=current_actions,
            use_target=False,
            observation_features=observation_features,
        )
        
        return q_preds

    def compute_loss_discrete_critic(
        self,
        observations,
        actions,
        rewards,
        next_observations,
        done,
        observation_features=None,
        next_observation_features=None,
        complementary_info=None,
    ):
        # NOTE: We only want to keep the discrete action part
        # In the buffer we have the full action space (continuous + discrete)
        # We need to split them before concatenating them in the critic forward
        actions_discrete: Tensor = actions[:, DISCRETE_DIMENSION_INDEX:].clone()
        actions_discrete = torch.round(actions_discrete)
        actions_discrete = actions_discrete.long()

        discrete_penalties: Tensor | None = None
        if complementary_info is not None:
            discrete_penalties: Tensor | None = complementary_info.get("discrete_penalty")

        with torch.no_grad():
            # For DQN, select actions using online network, evaluate with target network
            next_discrete_qs = self.discrete_critic_forward(
                next_observations, use_target=False, observation_features=next_observation_features
            )
            best_next_discrete_action = torch.argmax(next_discrete_qs, dim=-1, keepdim=True)

            # Get target Q-values from target network
            target_next_discrete_qs = self.discrete_critic_forward(
                observations=next_observations,
                use_target=True,
                observation_features=next_observation_features,
            )

            # Use gather to select Q-values for best actions
            target_next_discrete_q = torch.gather(
                target_next_discrete_qs, dim=1, index=best_next_discrete_action
            ).squeeze(-1)

            # Compute target Q-value with Bellman equation
            rewards_discrete = rewards
            if discrete_penalties is not None:
                rewards_discrete = rewards + discrete_penalties
            target_discrete_q = rewards_discrete + (1 - done) * self.config.discount * target_next_discrete_q

        # Get predicted Q-values for current observations
        predicted_discrete_qs = self.discrete_critic_forward(
            observations=observations, use_target=False, observation_features=observation_features
        )

        # Use gather to select Q-values for taken actions
        predicted_discrete_q = torch.gather(predicted_discrete_qs, dim=1, index=actions_discrete).squeeze(-1)

        # Compute MSE loss between predicted and target Q-values
        discrete_critic_loss = F.mse_loss(input=predicted_discrete_q, target=target_discrete_q)
        return discrete_critic_loss

    def compute_loss_temperature(self, observations, observation_features: Tensor | None = None) -> Tensor:
        """
        è®¡ç®—æ¸©åº¦å‚æ•°æŸå¤±ï¼Œæ”¯æŒQ-chunking
        
        Q-chunkingæ”¹è¿›ï¼šä½¿ç”¨åŠ¨ä½œåºåˆ—çš„è”åˆæ¦‚ç‡è®¡ç®—æ¸©åº¦æŸå¤±
        è¿™ç¡®ä¿æ¸©åº¦å‚æ•°é€‚åº”åŠ¨ä½œåºåˆ—çš„å¤æ‚æ€§
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨Q-chunking
        use_q_chunking = (hasattr(self.actor, 'chunk_size') and 
                         getattr(self.config, 'use_sequence_act_actor', False) and
                         getattr(self.config, 'enable_q_chunking', True))
        
        # è®¡ç®—æ¸©åº¦æŸå¤±
        with torch.no_grad():
            if use_q_chunking:
                # Q-chunkingæ¨¡å¼ï¼šä½¿ç”¨åŠ¨ä½œåºåˆ—çš„è”åˆå¯¹æ•°æ¦‚ç‡
                _, log_probs_joint, _ = self.actor(
                    observations, 
                    observation_features, 
                    return_sequence=True
                )
                
                # Q-chunkingçš„ç›®æ ‡ç†µè°ƒæ•´
                # ç”±äºä½¿ç”¨è”åˆæ¦‚ç‡ï¼Œç›®æ ‡ç†µéœ€è¦æ ¹æ®chunk_sizeè°ƒæ•´
                chunk_size = getattr(self.actor, 'chunk_size', 1)
                adjusted_target_entropy = self._get_adjusted_target_entropy(chunk_size)
                
                temperature_loss = (-self.log_alpha.exp() * (log_probs_joint + adjusted_target_entropy)).mean()
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼šä½¿ç”¨å•æ­¥å¯¹æ•°æ¦‚ç‡
                _, log_probs, _ = self.actor(observations, observation_features)
                temperature_loss = (-self.log_alpha.exp() * (log_probs + self.target_entropy)).mean()
                
        return temperature_loss
    
    def _get_adjusted_target_entropy(self, chunk_size: int) -> float:
        """
        è®¡ç®—Q-chunkingçš„è°ƒæ•´ç›®æ ‡ç†µ
        
        ç”±äºQ-chunkingä½¿ç”¨è”åˆæ¦‚ç‡ï¼Œç›®æ ‡ç†µéœ€è¦ç›¸åº”è°ƒæ•´ï¼š
        - è”åˆæ¦‚ç‡é€šå¸¸æ¯”å•ä¸ªåŠ¨ä½œæ¦‚ç‡æ›´å°ï¼ˆæ›´è´Ÿï¼‰
        - ç›®æ ‡ç†µåº”è¯¥æ ¹æ®åºåˆ—é•¿åº¦è¿›è¡Œç¼©æ”¾
        
        Args:
            chunk_size: åŠ¨ä½œåºåˆ—é•¿åº¦
            
        Returns:
            è°ƒæ•´åçš„ç›®æ ‡ç†µ
        """
        base_target_entropy = self.target_entropy
        
        # Q-chunkingç›®æ ‡ç†µè°ƒæ•´ç­–ç•¥
        entropy_scaling_strategy = getattr(self.config, 'q_chunking_entropy_scaling', 'linear')
        
        if entropy_scaling_strategy == 'linear':
            # çº¿æ€§ç¼©æ”¾ï¼šç›®æ ‡ç†µä¸åºåˆ—é•¿åº¦æˆæ­£æ¯”
            adjusted_entropy = base_target_entropy * chunk_size
        elif entropy_scaling_strategy == 'sqrt':
            # å¹³æ–¹æ ¹ç¼©æ”¾ï¼šæ›´æ¸©å’Œçš„è°ƒæ•´
            adjusted_entropy = base_target_entropy * math.sqrt(chunk_size)
        elif entropy_scaling_strategy == 'log':
            # å¯¹æ•°ç¼©æ”¾ï¼šæœ€æ¸©å’Œçš„è°ƒæ•´
            adjusted_entropy = base_target_entropy * math.log(chunk_size + 1)
        else:
            # æ— ç¼©æ”¾ï¼šä¿æŒåŸå§‹ç›®æ ‡ç†µ
            adjusted_entropy = base_target_entropy
        
        logging.debug(f"Q-chunking entropy: base={base_target_entropy:.3f}, "
                     f"adjusted={adjusted_entropy:.3f}, chunk_size={chunk_size}")
        
        return adjusted_entropy

    def compute_loss_actor(
        self,
        observations,
        observation_features: Tensor | None = None,
        expert_actions: Tensor | None = None,
        expert_action_sequences: Tensor | None = None,  # ä¸“å®¶åŠ¨ä½œåºåˆ—ç”¨äºåºåˆ—ACT
        training_step: int = 0,
    ) -> Tensor:
        """
        è®¡ç®—ActoræŸå¤±ï¼ŒåŒ…å«SACæŸå¤±å’Œå¯é€‰çš„BCæŸå¤±
        
        æ”¯æŒä¼ ç»ŸMLP Actorã€åŸºç¡€ACT Actorå’Œåºåˆ—ACT Actorä¸‰ç§æ¶æ„
        
        Args:
            observations: è§‚æµ‹æ•°æ®
            observation_features: é¢„è®¡ç®—çš„è§‚æµ‹ç‰¹å¾
            expert_actions: ä¸“å®¶åŠ¨ä½œï¼ˆæ¥è‡ªç¦»çº¿æ•°æ®é›†ï¼‰ï¼Œç”¨äºè®¡ç®—å•æ­¥BCæŸå¤±
            expert_action_sequences: ä¸“å®¶åŠ¨ä½œåºåˆ—ï¼Œç”¨äºåºåˆ—ACTçš„BCæŸå¤±
            training_step: å½“å‰è®­ç»ƒæ­¥æ•°ï¼Œç”¨äºåŠ¨æ€æƒé‡è®¡ç®—
            
        Returns:
            Actoræ€»æŸå¤±
        """
        # æ£€æŸ¥Actorç±»å‹
        use_act_actor = getattr(self.config, 'use_act_actor', False)
        use_sequence_act_actor = getattr(self.config, 'use_sequence_act_actor', False)
        
        if use_act_actor and use_sequence_act_actor:
            # åºåˆ—ACT Actor - æ”¯æŒåŠ¨ä½œåºåˆ—é¢„æµ‹
            return self._compute_sequence_actor_loss(
                observations=observations,
                observation_features=observation_features,
                expert_actions=expert_actions,
                expert_action_sequences=expert_action_sequences,
                training_step=training_step
            )
        elif use_act_actor:
            # åŸºç¡€ACT Actor - å•æ­¥é¢„æµ‹
            actions_pi, log_probs, means = self.actor(observations, observation_features)
        else:
            # ä¼ ç»ŸMLP Actor
            actions_pi, log_probs, _ = self.actor(observations, observation_features)

        # ä¼ ç»Ÿå•æ­¥æŸå¤±è®¡ç®—
        q_preds = self.critic_forward(
            observations=observations,
            actions=actions_pi,
            use_target=False,
            observation_features=observation_features,
        )
        min_q_preds = q_preds.min(dim=0)[0]

        # è®¡ç®—æ ‡å‡†SAC ActoræŸå¤±
        sac_actor_loss = ((self.temperature * log_probs) - min_q_preds).mean()
        
        # å¦‚æœæœ‰ä¸“å®¶åŠ¨ä½œï¼Œè®¡ç®—BCæŸå¤±
        if expert_actions is not None:
            if use_act_actor:
                # å¯¹äºACT Actorï¼Œä½¿ç”¨ç¡®å®šæ€§åŠ¨ä½œï¼ˆmeansï¼‰è®¡ç®—BCæŸå¤±
                bc_mse_loss = self._compute_bc_loss_act(
                    predicted_actions=means,  # ä½¿ç”¨ç¡®å®šæ€§è¾“å‡º
                    expert_actions=expert_actions
                )
            else:
                # å¯¹äºä¼ ç»ŸActorï¼Œä½¿ç”¨é‡‡æ ·åŠ¨ä½œè®¡ç®—BCæŸå¤±
                bc_mse_loss = self._compute_bc_loss(
                    predicted_actions=actions_pi,
                    expert_actions=expert_actions
                )
            
            # è®¡ç®—åŠ¨æ€æƒé‡ï¼ˆéšè®­ç»ƒæ­¥æ•°è¡°å‡ï¼‰
            bc_weight = self._compute_dynamic_bc_weight(training_step)
            sac_weight = 1.0 - bc_weight
            
            # æ··åˆæŸå¤±
            actor_loss = sac_weight * sac_actor_loss + bc_weight * bc_mse_loss
            
            # å­˜å‚¨æŸå¤±ç»„ä»¶ç”¨äºæ—¥å¿—è®°å½•
            self._last_bc_loss = bc_mse_loss.detach().clone()
            self._last_bc_weight = bc_weight
            self._last_sac_actor_loss = sac_actor_loss.detach().clone()
        else:
            # çº¯SACæŸå¤±
            actor_loss = sac_actor_loss
            self._last_bc_loss = None
            self._last_bc_weight = 0.0
            self._last_sac_actor_loss = sac_actor_loss.detach().clone()
            
        return actor_loss
    
    def _compute_bc_loss(self, predicted_actions: Tensor, expert_actions: Tensor) -> Tensor:
        """
        è®¡ç®—è¡Œä¸ºå…‹éš†çš„MSEæŸå¤±
        
        Args:
            predicted_actions: ç­–ç•¥ç½‘ç»œé¢„æµ‹çš„åŠ¨ä½œ
            expert_actions: ä¸“å®¶æ¼”ç¤ºçš„åŠ¨ä½œ
            
        Returns:
            BC MSEæŸå¤±
        """
        # è®°å½•åŸå§‹å½¢çŠ¶ç”¨äºè°ƒè¯•
        orig_pred_shape = predicted_actions.shape
        orig_expert_shape = expert_actions.shape
        
        # é¦–å…ˆå¤„ç†åŠ¨ä½œç»´åº¦ä¸åŒ¹é…çš„é—®é¢˜
        if predicted_actions.shape[1] != expert_actions.shape[1]:
            # å¦‚æœæœ‰ç¦»æ•£åŠ¨ä½œç»´åº¦ï¼Œåªä½¿ç”¨è¿ç»­éƒ¨åˆ†
            if self.config.num_discrete_actions is not None:
                expert_actions = expert_actions[:, :predicted_actions.shape[1]]
                logging.debug(f"BC: Adjusted expert action dims for discrete actions: {orig_expert_shape} -> {expert_actions.shape}")
            else:
                raise ValueError(f"Action dimension mismatch: predicted {predicted_actions.shape[1]} dims, expert {expert_actions.shape[1]} dims")
        
        # å¤„ç†æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…çš„é—®é¢˜
        if predicted_actions.shape[0] != expert_actions.shape[0]:
            pred_batch_size = predicted_actions.shape[0]
            expert_batch_size = expert_actions.shape[0]
            
            if expert_batch_size < pred_batch_size:
                # é‡å¤ä¸“å®¶åŠ¨ä½œä»¥åŒ¹é…é¢„æµ‹æ‰¹æ¬¡å¤§å°
                repeat_times = (pred_batch_size + expert_batch_size - 1) // expert_batch_size
                expert_actions = expert_actions.repeat(repeat_times, 1)[:pred_batch_size]
                logging.debug(f"BC: Repeated expert actions: {orig_expert_shape} -> {expert_actions.shape}")
            elif expert_batch_size > pred_batch_size:
                # æˆªæ–­ä¸“å®¶åŠ¨ä½œä»¥åŒ¹é…é¢„æµ‹æ‰¹æ¬¡å¤§å°
                expert_actions = expert_actions[:pred_batch_size]
                logging.debug(f"BC: Truncated expert actions: {orig_expert_shape} -> {expert_actions.shape}")
        
        # æœ€ç»ˆæ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
        if predicted_actions.shape != expert_actions.shape:
            raise ValueError(f"Final shape mismatch after adjustments: predicted {predicted_actions.shape}, expert {expert_actions.shape}")
        
        # è®¡ç®—MSEæŸå¤±
        bc_loss = F.mse_loss(predicted_actions, expert_actions)
        return bc_loss
    
    def _compute_dynamic_bc_weight(self, training_step: int) -> float:
        """
        è®¡ç®—åŠ¨æ€BCæƒé‡ï¼Œéšè®­ç»ƒæ­¥æ•°è¡°å‡
        
        ç­–ç•¥ï¼š
        - åˆå§‹æƒé‡ï¼š0.5ï¼ˆBCå’ŒSACå„å ä¸€åŠï¼‰
        - è¡°å‡æ–¹å¼ï¼šæŒ‡æ•°è¡°å‡
        - æœ€ç»ˆæƒé‡ï¼š0.01ï¼ˆä¿æŒå°‘é‡BCæ­£åˆ™åŒ–ï¼‰
        - è¡°å‡æ­¥æ•°ï¼šæ ¹æ®é…ç½®è®¾å®š
        
        Args:
            training_step: å½“å‰è®­ç»ƒæ­¥æ•°
            
        Returns:
            BCæƒé‡ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        # ä»é…ç½®ä¸­è·å–è¡°å‡å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        initial_bc_weight = getattr(self.config, 'bc_initial_weight', 0.5)
        final_bc_weight = getattr(self.config, 'bc_final_weight', 0.01)
        bc_decay_steps = getattr(self.config, 'bc_decay_steps', 50000)
        
        if training_step >= bc_decay_steps:
            return final_bc_weight
        
        # æŒ‡æ•°è¡°å‡
        decay_ratio = training_step / bc_decay_steps
        weight = initial_bc_weight * (final_bc_weight / initial_bc_weight) ** decay_ratio
        
        return max(weight, final_bc_weight)
    
    def _compute_bc_loss_act(self, predicted_actions: Tensor, expert_actions: Tensor) -> Tensor:
        """
        è®¡ç®—ACT Actorçš„è¡Œä¸ºå…‹éš†MSEæŸå¤±
        
        Args:
            predicted_actions: ACT Actoré¢„æµ‹çš„ç¡®å®šæ€§åŠ¨ä½œï¼ˆmeansï¼‰
            expert_actions: ä¸“å®¶æ¼”ç¤ºçš„åŠ¨ä½œ
            
        Returns:
            BC MSEæŸå¤±
        """
        # è®°å½•åŸå§‹å½¢çŠ¶ç”¨äºè°ƒè¯•
        orig_pred_shape = predicted_actions.shape
        orig_expert_shape = expert_actions.shape
        
        # é¦–å…ˆå¤„ç†åŠ¨ä½œç»´åº¦ä¸åŒ¹é…çš„é—®é¢˜
        if predicted_actions.shape[1] != expert_actions.shape[1]:
            # å¦‚æœæœ‰ç¦»æ•£åŠ¨ä½œç»´åº¦ï¼Œåªä½¿ç”¨è¿ç»­éƒ¨åˆ†
            if self.config.num_discrete_actions is not None:
                expert_actions = expert_actions[:, :predicted_actions.shape[1]]
                logging.debug(f"ACT BC: Adjusted expert action dims for discrete actions: {orig_expert_shape} -> {expert_actions.shape}")
            else:
                raise ValueError(f"Action dimension mismatch: predicted {predicted_actions.shape[1]} dims, expert {expert_actions.shape[1]} dims")
        
        # å¤„ç†æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…çš„é—®é¢˜
        if predicted_actions.shape[0] != expert_actions.shape[0]:
            pred_batch_size = predicted_actions.shape[0]
            expert_batch_size = expert_actions.shape[0]
            
            if expert_batch_size < pred_batch_size:
                # é‡å¤ä¸“å®¶åŠ¨ä½œä»¥åŒ¹é…é¢„æµ‹æ‰¹æ¬¡å¤§å°
                repeat_times = (pred_batch_size + expert_batch_size - 1) // expert_batch_size
                expert_actions = expert_actions.repeat(repeat_times, 1)[:pred_batch_size]
                logging.debug(f"ACT BC: Repeated expert actions: {orig_expert_shape} -> {expert_actions.shape}")
            elif expert_batch_size > pred_batch_size:
                # æˆªæ–­ä¸“å®¶åŠ¨ä½œä»¥åŒ¹é…é¢„æµ‹æ‰¹æ¬¡å¤§å°
                expert_actions = expert_actions[:pred_batch_size]
                logging.debug(f"ACT BC: Truncated expert actions: {orig_expert_shape} -> {expert_actions.shape}")
        
        # æœ€ç»ˆæ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
        if predicted_actions.shape != expert_actions.shape:
            raise ValueError(f"Final shape mismatch after adjustments: predicted {predicted_actions.shape}, expert {expert_actions.shape}")
        
        # è®¡ç®—MSEæŸå¤±
        bc_loss = F.mse_loss(predicted_actions, expert_actions)
        return bc_loss
    
    def _compute_sequence_actor_loss(
        self,
        observations,
        observation_features: Tensor | None = None,
        expert_actions: Tensor | None = None,
        expert_action_sequences: Tensor | None = None,
        training_step: int = 0,
    ) -> Tensor:
        """
        è®¡ç®—Q-chunkingåºåˆ—ACT Actorçš„æŸå¤±
        
        Q-chunkingçš„æ ¸å¿ƒæ”¹è¿›ï¼š
        1. ä½¿ç”¨åŠ¨ä½œåºåˆ—çš„è”åˆæ¦‚ç‡è¿›è¡ŒSACä¼˜åŒ–
        2. æ”¯æŒå¤šç§Q-chunkingç­–ç•¥ï¼ˆæ ‡å‡†/ä¿å®ˆ/æ—¶é—´åŠ æƒï¼‰
        3. è€ƒè™‘åŠ¨ä½œåºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§
        
        Args:
            observations: è§‚æµ‹æ•°æ®ï¼ˆå¯èƒ½æ˜¯åºåˆ—ï¼‰
            observation_features: é¢„è®¡ç®—çš„è§‚æµ‹ç‰¹å¾
            expert_actions: å•æ­¥ä¸“å®¶åŠ¨ä½œ
            expert_action_sequences: ä¸“å®¶åŠ¨ä½œåºåˆ— (batch, chunk_size, action_dim)
            training_step: å½“å‰è®­ç»ƒæ­¥æ•°
            
        Returns:
            Q-chunking Actoræ€»æŸå¤±
        """
        # 1. è·å–åŠ¨ä½œåºåˆ—é¢„æµ‹ï¼ˆè”åˆæ¦‚ç‡ï¼‰
        action_sequence, log_probs_joint, means_sequence = self.actor(
            observations, 
            observation_features, 
            return_sequence=True
        )
        
        # 2. Q-chunking SACæŸå¤±è®¡ç®—
        sac_actor_loss = self._compute_q_chunking_sac_loss(
            action_sequence=action_sequence,
            log_probs_joint=log_probs_joint,
            observations=observations,
            observation_features=observation_features
        )
        
        # 3. è®¡ç®—åºåˆ—BCæŸå¤±
        bc_sequence_loss = None
        if expert_action_sequences is not None:
            # ä½¿ç”¨å®Œæ•´çš„åŠ¨ä½œåºåˆ—è®¡ç®—BCæŸå¤±
            bc_sequence_loss = self._compute_sequence_bc_loss(
                predicted_sequence=means_sequence,  # ä½¿ç”¨ç¡®å®šæ€§é¢„æµ‹
                expert_sequence=expert_action_sequences
            )
        elif expert_actions is not None:
            # å¦‚æœåªæœ‰å•æ­¥ä¸“å®¶åŠ¨ä½œï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªé¢„æµ‹åŠ¨ä½œè®¡ç®—BCæŸå¤±
            bc_sequence_loss = self._compute_bc_loss_act(
                predicted_actions=means_sequence[:, 0, :],  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªåŠ¨ä½œ
                expert_actions=expert_actions
            )
        
        # 5. æ··åˆæŸå¤±è®¡ç®—
        if bc_sequence_loss is not None:
            bc_weight = self._compute_dynamic_bc_weight(training_step)
            sac_weight = 1.0 - bc_weight
            
            # æ··åˆæŸå¤±
            actor_loss = sac_weight * sac_actor_loss + bc_weight * bc_sequence_loss
            
            # å­˜å‚¨æŸå¤±ç»„ä»¶
            self._last_bc_loss = bc_sequence_loss.detach().clone()
            self._last_bc_weight = bc_weight
            self._last_sac_actor_loss = sac_actor_loss.detach().clone()
            
            # åºåˆ—ç‰¹æœ‰çš„æŸå¤±ä¿¡æ¯
            self._last_sequence_length = action_sequence.shape[1]
            self._last_joint_log_prob = log_probs_joint.mean().detach().clone()
            
            logging.debug(f"Sequence Actor Loss - SAC: {sac_actor_loss.item():.4f}, "
                         f"BC: {bc_sequence_loss.item():.4f}, "
                         f"Weight: {bc_weight:.3f}, "
                         f"Sequence Length: {self._last_sequence_length}")
        else:
            # çº¯SACæŸå¤±
            actor_loss = sac_actor_loss
            self._last_bc_loss = None
            self._last_bc_weight = 0.0
            self._last_sac_actor_loss = sac_actor_loss.detach().clone()
            self._last_sequence_length = action_sequence.shape[1]
            self._last_joint_log_prob = log_probs_joint.mean().detach().clone()
        
        return actor_loss
    
    def _compute_sequence_bc_loss(self, predicted_sequence: Tensor, expert_sequence: Tensor) -> Tensor:
        """
        è®¡ç®—åŠ¨ä½œåºåˆ—çš„BCæŸå¤±
        
        Args:
            predicted_sequence: é¢„æµ‹çš„åŠ¨ä½œåºåˆ— (batch, chunk_size, action_dim)
            expert_sequence: ä¸“å®¶åŠ¨ä½œåºåˆ— (batch, chunk_size, action_dim)
            
        Returns:
            åºåˆ—BCæŸå¤±
        """
        # ç¡®ä¿å½¢çŠ¶åŒ¹é…
        if predicted_sequence.shape != expert_sequence.shape:
            # å¦‚æœåºåˆ—é•¿åº¦ä¸åŒ¹é…ï¼Œæˆªå–æˆ–å¡«å……
            pred_len = predicted_sequence.shape[1]
            expert_len = expert_sequence.shape[1]
            
            if expert_len < pred_len:
                # é‡å¤æœ€åä¸€ä¸ªä¸“å®¶åŠ¨ä½œ
                last_action = expert_sequence[:, -1:, :].expand(-1, pred_len - expert_len, -1)
                expert_sequence = torch.cat([expert_sequence, last_action], dim=1)
                logging.debug(f"Sequence BC: Padded expert sequence from {expert_len} to {pred_len}")
            elif expert_len > pred_len:
                # æˆªå–ä¸“å®¶åºåˆ—
                expert_sequence = expert_sequence[:, :pred_len, :]
                logging.debug(f"Sequence BC: Truncated expert sequence from {expert_len} to {pred_len}")
        
        # è®¡ç®—åºåˆ—MSEæŸå¤±
        # é€‰é¡¹1ï¼šå¹³å‡æ‰€æœ‰æ—¶é—´æ­¥çš„æŸå¤±
        sequence_mse = F.mse_loss(predicted_sequence, expert_sequence)
        
        # é€‰é¡¹2ï¼šåŠ æƒæŸå¤±ï¼ˆå¯é€‰ï¼‰- ç»™è¿‘æœŸåŠ¨ä½œæ›´é«˜æƒé‡
        # weights = torch.exp(-0.1 * torch.arange(predicted_sequence.shape[1], device=predicted_sequence.device))
        # weights = weights / weights.sum()
        # weighted_mse = (weights.view(1, -1, 1) * (predicted_sequence - expert_sequence) ** 2).mean()
        
        return sequence_mse
    
    def _compute_q_chunking_sac_loss(
        self,
        action_sequence: Tensor,
        log_probs_joint: Tensor,
        observations,
        observation_features: Tensor | None = None,
    ) -> Tensor:
        """
        è®¡ç®—Q-chunkingçš„SACæŸå¤±
        
        Q-chunkingçš„æ ¸å¿ƒåˆ›æ–°ï¼šSACæŸå¤±ä½¿ç”¨æ•´ä¸ªåŠ¨ä½œåºåˆ—çš„è”åˆæ¦‚ç‡
        è¿™ç¡®ä¿ç­–ç•¥ä¼˜åŒ–æ—¶è€ƒè™‘åŠ¨ä½œåºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§å’Œé•¿æœŸè§„åˆ’
        
        ç›¸æ¯”ä¼ ç»ŸSACåªè€ƒè™‘å•æ­¥åŠ¨ä½œï¼ŒQ-chunkingè€ƒè™‘çš„æ˜¯ï¼š
        loss = E[Î± * log Ï€(aâ‚:â‚œ|sâ‚:â‚œ) - Q(sâ‚, aâ‚)]
        å…¶ä¸­ aâ‚:â‚œ æ˜¯åŠ¨ä½œåºåˆ—ï¼ŒÏ€(aâ‚:â‚œ|sâ‚:â‚œ) æ˜¯è”åˆåŠ¨ä½œæ¦‚ç‡
        
        Args:
            action_sequence: é¢„æµ‹çš„åŠ¨ä½œåºåˆ— (batch, chunk_size, action_dim)
            log_probs_joint: åŠ¨ä½œåºåˆ—çš„è”åˆå¯¹æ•°æ¦‚ç‡ (batch,)
            observations: è§‚æµ‹æ•°æ®
            observation_features: é¢„è®¡ç®—çš„è§‚æµ‹ç‰¹å¾
            
        Returns:
            Q-chunking SAC ActoræŸå¤±
        """
        # 1. è·å–å½“å‰è§‚æµ‹ï¼ˆç”¨äºQå€¼è®¡ç®—ï¼‰
        if isinstance(observations, list):
            current_obs = observations[-1]  # å–åºåˆ—çš„æœ€åä¸€ä¸ªè§‚æµ‹ä½œä¸ºå½“å‰è§‚æµ‹
        else:
            current_obs = observations
        
        # 2. Q-chunkingç­–ç•¥é€‰æ‹©
        q_chunking_strategy = getattr(self.config, 'q_chunking_strategy', 'standard')
        
        if q_chunking_strategy == 'conservative':
            # ä¿å®ˆç­–ç•¥ï¼šå¯¹åºåˆ—ä¸­çš„å¤šä¸ªåŠ¨ä½œè®¡ç®—Qå€¼å¹¶å–æœ€å°å€¼
            sac_loss = self._compute_conservative_q_chunking_loss(
                action_sequence, log_probs_joint, current_obs, observation_features
            )
        elif q_chunking_strategy == 'temporal_weighted':
            # æ—¶é—´åŠ æƒç­–ç•¥ï¼šå¯¹ä¸åŒæ—¶é—´æ­¥çš„åŠ¨ä½œç»™äºˆä¸åŒæƒé‡
            sac_loss = self._compute_temporal_weighted_q_chunking_loss(
                action_sequence, log_probs_joint, current_obs, observation_features
            )
        else:
            # æ ‡å‡†ç­–ç•¥ï¼šåªä½¿ç”¨ç¬¬ä¸€ä¸ªåŠ¨ä½œè®¡ç®—Qå€¼ï¼Œä½†ä½¿ç”¨è”åˆæ¦‚ç‡
            sac_loss = self._compute_standard_q_chunking_loss(
                action_sequence, log_probs_joint, current_obs, observation_features
            )
        
        return sac_loss
    
    def _compute_standard_q_chunking_loss(
        self,
        action_sequence: Tensor,
        log_probs_joint: Tensor,
        current_obs,
        observation_features: Tensor | None = None,
    ) -> Tensor:
        """
        æ ‡å‡†Q-chunkingç­–ç•¥ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªåŠ¨ä½œè®¡ç®—Qå€¼ï¼Œä½†ä½¿ç”¨åºåˆ—è”åˆæ¦‚ç‡
        
        è¿™æ˜¯Q-chunkingè®ºæ–‡ä¸­çš„ä¸»è¦æ–¹æ³•
        """
        # ä½¿ç”¨åºåˆ—çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œè®¡ç®—Qå€¼
        first_action = action_sequence[:, 0, :]  # (batch, action_dim)
        
        q_preds = self.critic_forward(
            observations=current_obs,
            actions=first_action,
            use_target=False,
            observation_features=observation_features,
        )
        min_q_preds = q_preds.min(dim=0)[0]
        
        # ğŸ”¥ Q-chunkingæ ¸å¿ƒï¼šä½¿ç”¨æ•´ä¸ªåŠ¨ä½œåºåˆ—çš„è”åˆæ¦‚ç‡
        # è¿™ç¡®ä¿ç­–ç•¥å­¦ä¹ åˆ°æ—¶é—´ä¸€è‡´çš„åŠ¨ä½œåºåˆ—
        sac_actor_loss = ((self.temperature * log_probs_joint) - min_q_preds).mean()
        
        return sac_actor_loss
    
    def _compute_conservative_q_chunking_loss(
        self,
        action_sequence: Tensor,
        log_probs_joint: Tensor,
        current_obs,
        observation_features: Tensor | None = None,
    ) -> Tensor:
        """
        ä¿å®ˆQ-chunkingç­–ç•¥ï¼šå¯¹åºåˆ—ä¸­çš„å¤šä¸ªåŠ¨ä½œè®¡ç®—Qå€¼å¹¶å–æœ€å°å€¼
        
        è¿™æä¾›äº†æ›´ä¿å®ˆçš„ä»·å€¼ä¼°è®¡ï¼Œå¯èƒ½æ›´ç¨³å®šä½†å­¦ä¹ è¾ƒæ…¢
        """
        batch_size, chunk_size, action_dim = action_sequence.shape
        
        # è®¡ç®—åºåˆ—ä¸­æ¯ä¸ªåŠ¨ä½œçš„Qå€¼
        q_values_list = []
        for t in range(min(chunk_size, getattr(self.config, 'q_chunking_horizon', 3))):
            action_t = action_sequence[:, t, :]  # (batch, action_dim)
            
            q_preds_t = self.critic_forward(
                observations=current_obs,
                actions=action_t,
                use_target=False,
                observation_features=observation_features,
            )
            min_q_t = q_preds_t.min(dim=0)[0]
            q_values_list.append(min_q_t)
        
        # å¯¹å¤šä¸ªæ—¶é—´æ­¥çš„Qå€¼å–æœ€å°å€¼ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
        stacked_q_values = torch.stack(q_values_list, dim=1)  # (batch, horizon)
        conservative_q = stacked_q_values.min(dim=1)[0]  # (batch,)
        
        # ä½¿ç”¨è”åˆæ¦‚ç‡å’Œä¿å®ˆQå€¼
        sac_actor_loss = ((self.temperature * log_probs_joint) - conservative_q).mean()
        
        return sac_actor_loss
    
    def _compute_temporal_weighted_q_chunking_loss(
        self,
        action_sequence: Tensor,
        log_probs_joint: Tensor,
        current_obs,
        observation_features: Tensor | None = None,
    ) -> Tensor:
        """
        æ—¶é—´åŠ æƒQ-chunkingç­–ç•¥ï¼šå¯¹ä¸åŒæ—¶é—´æ­¥çš„åŠ¨ä½œç»™äºˆä¸åŒæƒé‡
        
        è¿‘æœŸåŠ¨ä½œæƒé‡æ›´é«˜ï¼Œè¿œæœŸåŠ¨ä½œæƒé‡è¾ƒä½
        """
        batch_size, chunk_size, action_dim = action_sequence.shape
        horizon = min(chunk_size, getattr(self.config, 'q_chunking_horizon', 3))
        
        # åˆ›å»ºæ—¶é—´è¡°å‡æƒé‡
        decay_factor = getattr(self.config, 'q_chunking_decay', 0.9)
        weights = torch.tensor([decay_factor ** t for t in range(horizon)], 
                              device=action_sequence.device)
        weights = weights / weights.sum()  # å½’ä¸€åŒ–æƒé‡
        
        # è®¡ç®—åºåˆ—ä¸­æ¯ä¸ªåŠ¨ä½œçš„åŠ æƒQå€¼
        weighted_q_sum = 0.0
        for t in range(horizon):
            action_t = action_sequence[:, t, :]  # (batch, action_dim)
            
            q_preds_t = self.critic_forward(
                observations=current_obs,
                actions=action_t,
                use_target=False,
                observation_features=observation_features,
            )
            min_q_t = q_preds_t.min(dim=0)[0]
            weighted_q_sum += weights[t] * min_q_t
        
        # ä½¿ç”¨è”åˆæ¦‚ç‡å’ŒåŠ æƒQå€¼
        sac_actor_loss = ((self.temperature * log_probs_joint) - weighted_q_sum).mean()
        
        return sac_actor_loss

    def _init_normalization(self, dataset_stats):
        """Initialize input/output normalization modules."""
        self.normalize_inputs = nn.Identity()  # ç½‘ç»œå±‚å ä½åˆå§‹åŒ–
        self.normalize_targets = nn.Identity() # ç½‘ç»œå±‚å ä½åˆå§‹åŒ–
        """
            pre-train é˜¶æ®µä½¿ç”¨configå½“ä¸­çš„dataset_statså¯¹è¾“å…¥æ•°æ®è¿›è¡Œå½’ä¸€åŒ–
            eval é˜¶æ®µä½¿ç”¨ ./pretrained_model/config.json å½“ä¸­çš„dataset_statså¯¹è¾“å‡ºæ•°æ®è¿›è¡Œå½’ä¸€åŒ–
        """
        if self.config.dataset_stats is not None: # å¦‚æœconfigå½“ä¸­å®šä¹‰äº†dataset_stats
            params = _convert_normalization_params_to_tensor(self.config.dataset_stats) # å°†å½’ä¸€åŒ–å‚æ•°è½¬æ¢ä¸ºå¼ é‡
            self.normalize_inputs = NormalizeBuffer(
                self.config.input_features, self.config.normalization_mapping, params
            )
            """
                ä¼˜å…ˆä½¿ç”¨dataset_statsä¸­çš„minå’Œmaxå¯¹è¾“å‡ºæ•°æ®è¿›è¡Œå½’ä¸€åŒ–, 
                å¦‚æœdataset_statsä¸ºNone
                åˆ™ä½¿ç”¨paramså¯¹è¾“å‡ºæ•°æ®è¿›è¡Œå½’ä¸€åŒ–
            """
            stats = dataset_stats or params # å¦‚æœdataset_stats ä¸ºNoneï¼Œåˆ™ä½¿ç”¨params
            self.normalize_targets = NormalizeBuffer(
                self.config.output_features, self.config.normalization_mapping, stats
            )

    def _init_encoders(self):
        """Initialize shared or separate encoders for actor and critic."""
        self.shared_encoder = self.config.shared_encoder
        self.encoder_critic = SACObservationEncoder(self.config, self.normalize_inputs)
        self.encoder_actor = (
            self.encoder_critic
            if self.shared_encoder
            else SACObservationEncoder(self.config, self.normalize_inputs)
        )

    def _init_critics(self, continuous_action_dim):
        """
                è§‚æµ‹è¾“å…¥ â†’ SACObservationEncoder â†’ è§‚æµ‹ç¼–ç  (256ç»´)
                åŠ¨ä½œè¾“å…¥ â†’ åŠ¨ä½œå½’ä¸€åŒ– â†’ å½’ä¸€åŒ–åŠ¨ä½œ (2ç»´)
                                        â†“
                                    æ‹¼æ¥ (258ç»´)
                                        â†“
                                CriticHead 1: MLP(258â†’256â†’256â†’1) â†’ Q1
                                        â†“
                                CriticHead 2: MLP(258â†’256â†’256â†’1) â†’ Q2
                                        â†“
                                    è¾“å‡º: [2, batch_size] çš„Qå€¼å¼ é‡    
        """
        """Build critic ensemble, targets, and optional discrete critic."""
        # æ­¥éª¤1: åˆå§‹åŒ– å½“å‰Qç½‘ç»œ(æ®æ•°é‡åˆå§‹åŒ–å¤šä¸ªå½“å‰Qç½‘ç»œ)
        """
        ä¸¾ä¾‹:
            å¯ä»¥æ··åˆä¸åŒç±»å‹çš„è¯„è®ºå®¶å¤´
            mixed_heads = [
                CriticHead(input_dim, [256, 256]),  # æ ‡å‡†è¯„è®ºå®¶
                CustomCriticHead(input_dim, [512, 256]),  # è‡ªå®šä¹‰è¯„è®ºå®¶
                LightweightCriticHead(input_dim, [128, 128])  # è½»é‡çº§è¯„è®ºå®¶
            ]
            ensemble = CriticEnsemble(encoder, mixed_heads, normalization)
        """
        heads = [
            CriticHead(
                input_dim=self.encoder_critic.output_dim + continuous_action_dim,
                **asdict(self.config.critic_network_kwargs),
            )
            for _ in range(self.config.num_critics)
        ]
        self.critic_ensemble = CriticEnsemble(
            encoder=self.encoder_critic, ensemble=heads, output_normalization=self.normalize_targets
        )
        # æ­¥éª¤2: åˆå§‹åŒ– ç›®æ ‡Qç½‘ç»œ(æ ¹æ®æ•°é‡åˆå§‹åŒ–å¤šä¸ªç›®æ ‡Qç½‘ç»œ)
        target_heads = [
            CriticHead(
                input_dim=self.encoder_critic.output_dim + continuous_action_dim,
                **asdict(self.config.critic_network_kwargs),
            )
            for _ in range(self.config.num_critics)
        ]
        self.critic_target = CriticEnsemble(
            encoder=self.encoder_critic, ensemble=target_heads, output_normalization=self.normalize_targets
        )
        # æ­¥éª¤3: å°†å½“å‰Qç½‘ç»œçš„å‚æ•°åŠ è½½åˆ°ç›®æ ‡Qç½‘ç»œ
        self.critic_target.load_state_dict(self.critic_ensemble.state_dict())

        # æ­¥éª¤4: ä½¿ç”¨torch.compile ç¼–è¯‘å½“å‰Qç½‘ç»œå’Œç›®æ ‡Qç½‘ç»œ - ç¼–è¯‘ä¼˜åŒ–
        if self.config.use_torch_compile:
            self.critic_ensemble = torch.compile(self.critic_ensemble)
            self.critic_target = torch.compile(self.critic_target)

        # æ­¥éª¤5: åˆå§‹åŒ– ç¦»æ•£Qç½‘ç»œ(æ ¹æ®æ•°é‡åˆå§‹åŒ–å¤šä¸ªç¦»æ•£Qç½‘ç»œ) - ç¦»æ•£Qçš„ç½‘ç»œç”¨äºæœ«ç«¯æ‰§è¡Œå™¨çš„æŠ“/æ”¾
        if self.config.num_discrete_actions is not None:
            self._init_discrete_critics()

    def _init_discrete_critics(self):
        """Build discrete discrete critic ensemble and target networks."""
        """
            æŒ‰éœ€æ„å»ºç¦»æ•£ç½‘ç»œQçš„ç½‘ç»œé›†åˆ, ç”¨äºæœ«ç«¯æ‰§è¡Œå™¨çš„æŠ“/æ”¾
            ç¦»æ•£ç½‘ç»œQçš„å½“å‰ç½‘ç»œ:
                è¾“å…¥: è§‚æµ‹ç¼–ç 
                è¾“å‡º: Qå€¼å‘é‡
            ç¦»æ•£ç½‘ç»œQçš„ç›®æ ‡ç½‘ç»œ:
                è¾“å…¥: è§‚æµ‹ç¼–ç 
                è¾“å‡º: Qå€¼å‘é‡
        """
        self.discrete_critic = DiscreteCritic(
            encoder=self.encoder_critic,
            input_dim=self.encoder_critic.output_dim,
            output_dim=self.config.num_discrete_actions,
            **asdict(self.config.discrete_critic_network_kwargs), # å°†configå½“ä¸­çš„ç¦»æ•£Qç½‘ç»œçš„ç½‘ç»œå‚æ•°è½¬æ¢ä¸ºå­—å…¸
        )
        self.discrete_critic_target = DiscreteCritic(
            encoder=self.encoder_critic,
            input_dim=self.encoder_critic.output_dim,
            output_dim=self.config.num_discrete_actions,
            **asdict(self.config.discrete_critic_network_kwargs), # å°†configå½“ä¸­çš„ç¦»æ•£Qç½‘ç»œçš„ç½‘ç»œå‚æ•°è½¬æ¢ä¸ºå­—å…¸
        )

        # TODO: (maractingi, azouitine) Compile the discrete critic
        self.discrete_critic_target.load_state_dict(self.discrete_critic.state_dict())

    def _init_actor(self, continuous_action_dim):
        """åˆå§‹åŒ–ç­–ç•¥Actorç½‘ç»œå’Œé»˜è®¤ç›®æ ‡ç†µå€¼ã€‚
        
        Actorç½‘ç»œæ¶æ„è¯´æ˜ï¼š
        æ”¯æŒä¸¤ç§æ¶æ„ï¼š
        1. ä¼ ç»ŸMLP Actor (use_act_actor=False)
        2. ACT Transformer Actor (use_act_actor=True)
        
        ACT Actoræ¶æ„ï¼š
        è§‚æµ‹è¾“å…¥ â†’ SACObservationEncoder â†’ è§‚æµ‹ç¼–ç  
                                        â†“
                                    ACT Transformer Encoder-Decoder
                                        â†“
                                    å‡å€¼å±‚ (dim_modelâ†’action_dim) â†’ åŠ¨ä½œå‡å€¼
                                    æ ‡å‡†å·®å±‚ (dim_modelâ†’action_dim) â†’ åŠ¨ä½œæ ‡å‡†å·®
                                        â†“
                                    TanhMultivariateNormalDiag â†’ é‡‡æ ·åŠ¨ä½œ
        """
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ACT Actor
        use_act_actor = getattr(self.config, 'use_act_actor', False)
        
        if use_act_actor:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨åºåˆ—ç‰ˆæœ¬
            use_sequence_actor = getattr(self.config, 'use_sequence_act_actor', False)
            
            if use_sequence_actor:
                # ä½¿ç”¨çœŸæ­£çš„åºåˆ—ACT Actor (å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–)
                from lerobot.common.policies.sac.modeling_sac_sequence_act_actor import SequenceACTSACActorV2
                
                logging.info("ğŸš€ Initializing Sequence ACT-SAC Actor V2")
                
                self.actor = SequenceACTSACActorV2(
                    encoder=self.encoder_actor,
                    action_dim=continuous_action_dim,
                    # åºåˆ—å‚æ•°
                    chunk_size=getattr(self.config, 'act_chunk_size', 8),
                    obs_history_length=getattr(self.config, 'obs_history_length', 5),
                    # ACT Transformerå‚æ•°
                    dim_model=getattr(self.config, 'act_dim_model', 512),
                    n_heads=getattr(self.config, 'act_n_heads', 8),
                    dim_feedforward=getattr(self.config, 'act_dim_feedforward', 3200),
                    n_encoder_layers=getattr(self.config, 'act_n_encoder_layers', 4),
                    n_decoder_layers=getattr(self.config, 'act_n_decoder_layers', 4),
                    dropout=getattr(self.config, 'act_dropout', 0.1),
                    feedforward_activation=getattr(self.config, 'act_feedforward_activation', 'relu'),
                    pre_norm=getattr(self.config, 'act_pre_norm', False),
                    # SACç­–ç•¥å‚æ•°
                    encoder_is_shared=self.shared_encoder,
                    use_tanh_squash=getattr(self.config.policy_kwargs, 'use_tanh_squash', True),
                )
                logging.info("âœ… Using Sequence ACT-SAC Actor with history length: {}".format(
                    getattr(self.config, 'obs_history_length', 5)))
            else:
                # ä½¿ç”¨åŸºç¡€ACT Actor (å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–)
                from lerobot.common.policies.sac.modeling_sac_act_actor import ACTSACActor
                
                logging.info("ğŸ¤– Initializing ACT-SAC Hybrid Actor")
                
                self.actor = ACTSACActor(
                    encoder=self.encoder_actor,
                    action_dim=continuous_action_dim,
                    # ACT Transformerå‚æ•°
                    dim_model=getattr(self.config, 'act_dim_model', 512),
                    n_heads=getattr(self.config, 'act_n_heads', 8),
                    dim_feedforward=getattr(self.config, 'act_dim_feedforward', 3200),
                    n_encoder_layers=getattr(self.config, 'act_n_encoder_layers', 4),
                    n_decoder_layers=getattr(self.config, 'act_n_decoder_layers', 1),
                    dropout=getattr(self.config, 'act_dropout', 0.1),
                    feedforward_activation=getattr(self.config, 'act_feedforward_activation', 'relu'),
                    pre_norm=getattr(self.config, 'act_pre_norm', False),
                    # SACç­–ç•¥å‚æ•°
                    encoder_is_shared=self.shared_encoder,
                    **asdict(self.config.policy_kwargs),
                    # åºåˆ—å‚æ•°
                    max_seq_length=getattr(self.config, 'act_max_seq_length', 10),
                )
                logging.info("âœ… Using ACT-SAC Actor")
        else:
            # ä½¿ç”¨ä¼ ç»ŸMLP Actor
            logging.info("ğŸ”§ Using traditional MLP Actor")
            self.actor = Policy(
                encoder=self.encoder_actor,  # è§‚æµ‹ç¼–ç å™¨ï¼Œå°†åŸå§‹è§‚æµ‹è½¬æ¢ä¸ºç‰¹å¾å‘é‡
                network=MLP(  # ä¸»å¹²ç½‘ç»œï¼šå¤šå±‚æ„ŸçŸ¥æœº
                    input_dim=self.encoder_actor.output_dim,  # è¾“å…¥ç»´åº¦ï¼šè§‚æµ‹ç¼–ç çš„ç»´åº¦
                    **asdict(self.config.actor_network_kwargs),  # ç½‘ç»œé…ç½®å‚æ•°ï¼ˆéšè—å±‚ç»´åº¦ã€æ¿€æ´»å‡½æ•°ç­‰ï¼‰
                ),
                action_dim=continuous_action_dim,  # åŠ¨ä½œç»´åº¦ï¼šè¿ç»­åŠ¨ä½œçš„ç»´åº¦
                encoder_is_shared=self.shared_encoder,  # ç¼–ç å™¨æ˜¯å¦åœ¨Actorå’ŒCriticä¹‹é—´å…±äº«
                **asdict(self.config.policy_kwargs),  # ç­–ç•¥é…ç½®å‚æ•°ï¼ˆæ ‡å‡†å·®èŒƒå›´ã€æ˜¯å¦ä½¿ç”¨tanhç­‰ï¼‰
            )

        # è®¾ç½®ç›®æ ‡ç†µå€¼ï¼Œç”¨äºæ¸©åº¦å‚æ•°çš„è‡ªåŠ¨è°ƒèŠ‚
        self.target_entropy = self.config.target_entropy
        if self.target_entropy is None:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ç†µå€¼ï¼Œåˆ™æ ¹æ®åŠ¨ä½œç»´åº¦è‡ªåŠ¨è®¡ç®—
            # ç›®æ ‡ç†µå€¼ = -åŠ¨ä½œç»´åº¦ / 2ï¼Œè¿™æ˜¯ä¸€ä¸ªç»éªŒå…¬å¼
            dim = continuous_action_dim + (1 if self.config.num_discrete_actions is not None else 0)
            self.target_entropy = -np.prod(dim) / 2

    def _init_temperature(self):
        """Set up temperature parameter and initial log_alpha."""
        # åˆå§‹åŒ–ç†µå€¼
        temp_init = self.config.temperature_init
        # åˆå§‹åŒ– log_alpha - ä¼˜åŒ–å™¨ä¼šä¸»è¦å°†æ¢¯åº¦é™„åŠ åˆ°log_alphaä¸Š
        self.log_alpha = nn.Parameter(torch.tensor([math.log(temp_init)])) # è¾“å…¥x>0çš„å˜é‡ï¼Œè¾“å‡ºlog_alphaå¯ä»¥ä¸ºæ­£æ•°æˆ–è€…è´Ÿæ•°ï¼Œ
        # ä» log_alpha å½“ä¸­é€šè¿‡expè¿˜åŸä¸ºtemperature
        self.temperature = self.log_alpha.exp().item() # temperature å¿…é¡»ä¸ºä¸€ä¸ª x > 0 çš„æ•°å­—

    def _init_warmup_parameters(self):
        """Initialize warm-up parameters if enabled."""
        if not self.config.enable_warmup or not self.config.warmup_model_path:
            return
        
        import logging
        from .warmup_utils import WarmupParameterLoader, validate_warmup_model_path
        
        # Validate warmup model path
        validated_path = validate_warmup_model_path(self.config.warmup_model_path)
        if not validated_path:
            logging.warning(f"Invalid warm-up model path: {self.config.warmup_model_path}")
            return
        
        # Create loader and load parameters
        loader = WarmupParameterLoader()
        if not loader.load_warmup_parameters(validated_path):
            logging.error("Failed to load warm-up parameters")
            return
        
        # Apply parameters to actor
        success = loader.apply_warmup_parameters(
            target_model=self.actor,
            strict=self.config.warmup_strict_loading,
            freeze_loaded_params=self.config.warmup_freeze_loaded_params
        )
        
        if success:
            logging.info("âœ… Successfully applied warm-up parameters to SAC Actor")
            
            # Log warm-up information
            warmup_info = loader.get_warmup_info()
            logging.info(f"Warm-up info: {warmup_info}")
            
            # Store warmup info in config for later reference
            if not hasattr(self.config, '_warmup_info'):
                self.config._warmup_info = warmup_info
        else:
            logging.error("âŒ Failed to apply warm-up parameters to SAC Actor")


class SACObservationEncoder(nn.Module):
    """Encode image and/or state vector observations."""

    def __init__(self, config: SACConfig, input_normalizer: nn.Module) -> None:
        super().__init__()
        self.config = config
        self.input_normalization = input_normalizer
        self._init_image_layers()
        self._init_state_layers()
        self._compute_output_dim()
        
        # Initialize feature visualization components
        self._init_feature_visualization()

    def _init_feature_visualization(self):
        """Initialize ROS publisher and utilities for feature visualization."""
        # Check if feature visualization is enabled in config
        if not getattr(self.config, 'enable_feature_visualization', False):
            self.enable_feature_viz = False
            self.feature_viz_enabled = False
            rospy.loginfo("-- Feature visualization disabled in config -- enable_feature_visualization: False")
            return
            
        # Determine which processes should enable feature visualization
        import os
        import sys
        
        # Determine if this should enable feature visualization
        should_enable_viz = False
        
        # Method 1: Check command line arguments for inference scripts
        cmd_line = ' '.join(sys.argv)
        if 'gym_manipulator.py' in cmd_line or 'actor.py' in cmd_line:
            should_enable_viz = True
        
        # Method 2: Check if environment variable is set (can be set by actor.py or gym_manipulator.py)
        process_type = os.environ.get('LEROBOT_PROCESS_TYPE')
        if process_type == 'actor':
            should_enable_viz = True
            
        # Method 3: Check process name/title
        try:
            import setproctitle
            proc_title = setproctitle.getproctitle().lower()
            if 'actor' in proc_title or 'gym_manipulator' in proc_title:
                should_enable_viz = True
        except ImportError:
            pass
        
        # Method 4: Check if this is an inference/evaluation context
        # If we're loading a pretrained model and not in a learner context, enable visualization
        if not should_enable_viz:
            # Check if we're in a learner process (learners typically don't need visualization)
            is_learner = ('learner.py' in cmd_line or 
                         process_type == 'learner' or
                         os.environ.get('LEROBOT_IS_LEARNER') == 'true')
            
            if not is_learner:
                # This is likely an inference/evaluation context
                should_enable_viz = True
        
        self.enable_feature_viz = should_enable_viz

        if self.enable_feature_viz and ROS_AVAILABLE and rospy is not None:
            try:
                # Initialize ROS publisher for feature visualization (using CompressedImage for better performance)
                self.feature_viz_pub = rospy.Publisher('/vision_features/resnet10_features/compressed', CompressedImage, queue_size=1, tcp_nodelay=True)
                self.feature_viz_enabled = True
                rospy.loginfo("Feature visualization enabled - publishing to /vision_features/resnet10_features/compressed")
            except Exception as e:
                rospy.logwarn(f"Failed to initialize feature visualization: {e}")
                self.feature_viz_enabled = False
        else:
            self.feature_viz_enabled = False
            rospy.loginfo("-- Feature visualization disabled -- [if self.enable_feature_viz and ROS_AVAILABLE and rospy is not None] ")

    def _visualize_features(self, features: Tensor, image_key: str = "observation.image.front"):
        """
        Convert CNN features to visualization image and publish to ROS topic.
        
        Args:
            features: Feature tensor from ResNet10, shape (B, C, H, W) or (B, feature_dim)
            image_key: Key identifying which image these features come from
        """
        if not self.feature_viz_enabled:
            return
            
        try:
            with torch.no_grad():
                # Handle different feature tensor shapes
                if len(features.shape) == 4:  # (B, C, H, W) - spatial features
                    # Take first batch item and convert to numpy
                    feat = features[0].cpu().numpy()  # (C, H, W)
                    
                    # Create feature map visualization
                    # Method 1: Average across channels to create a single heatmap
                    feat_avg = np.mean(feat, axis=0)  # (H, W)
                    
                    # Normalize to 0-255
                    feat_norm = ((feat_avg - feat_avg.min()) / 
                               (feat_avg.max() - feat_avg.min() + 1e-8) * 255).astype(np.uint8)
                    
                    # Convert grayscale to RGB for visualization
                    feat_rgb = np.stack([feat_norm, feat_norm, feat_norm], axis=-1)  # (H, W, 3)
                    
                    # Resize to reasonable size for visualization (224x224)
                    import cv2
                    feat_rgb = cv2.resize(feat_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)
                    
                elif len(features.shape) == 2:  # (B, feature_dim) - flattened features
                    # For flattened features, create a simple visualization
                    feat = features[0].cpu().numpy()  # (feature_dim,)
                    
                    # Reshape to a square-ish grid for visualization
                    grid_size = int(np.sqrt(len(feat)))
                    if grid_size * grid_size < len(feat):
                        grid_size += 1
                    
                    # Pad and reshape
                    padded_feat = np.pad(feat, (0, grid_size * grid_size - len(feat)), 'constant')
                    feat_grid = padded_feat.reshape(grid_size, grid_size)
                    
                    # Normalize to 0-255
                    feat_norm = ((feat_grid - feat_grid.min()) / 
                               (feat_grid.max() - feat_grid.min() + 1e-8) * 255).astype(np.uint8)
                    
                    # Convert to RGB and resize
                    feat_rgb = np.stack([feat_norm, feat_norm, feat_norm], axis=-1)
                    import cv2
                    feat_rgb = cv2.resize(feat_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)
                
                else:
                    rospy.logwarn(f"Unsupported feature shape for visualization: {features.shape}")
                    return
                
                # Publish as ROS CompressedImage message
                ros_image = CompressedImage()
                ros_image.header.stamp = rospy.Time.now()
                ros_image.header.frame_id = f"features_{image_key.replace('.', '_')}"
                ros_image.format = "jpeg"
                
                # Encode image to JPEG format
                _, encoded_img = cv2.imencode('.jpg', feat_rgb)
                ros_image.data = encoded_img.tobytes()
                
                self.feature_viz_pub.publish(ros_image)
                
                # Log occasionally for debugging
                if hasattr(self, '_viz_counter'):
                    self._viz_counter += 1
                else:
                    self._viz_counter = 1
                    
                if self._viz_counter % 30 == 1:  # Log every 30 frames
                    rospy.loginfo(f"Published feature visualization - shape: {features.shape}, "
                                f"feature range: [{feat.min():.3f}, {feat.max():.3f}], "
                                f"viz range: [0, 255]")
                
        except Exception as e:
            rospy.logwarn(f"Error in feature visualization: {e}")

    def _init_image_layers(self) -> None:
        # If the config explicitly disables vision features, skip image processing entirely
        if getattr(self.config, 'disable_vision_features', False):
            self.has_images = False
            self.image_keys = []
            self.image_encoder = None
            self.spatial_embeddings = nn.ModuleDict()
            self.post_encoders = nn.ModuleDict()
            return

        # If the config clearly indicates no vision, just exit.
        # This is a stronger check that relies on explicit config values.
        if self.config.vision_encoder_name is None and self.config.image_encoder_hidden_dim == 0:
            self.has_images = False
            self.image_keys = []
            self.image_encoder = None
            self.spatial_embeddings = nn.ModuleDict()
            self.post_encoders = nn.ModuleDict()
            return

        self.image_keys = [k for k in self.config.input_features if is_image_feature(k)]
        self.has_images = bool(self.image_keys)
        if not self.has_images:
            # Set image-related attributes to None or empty to avoid errors
            self.image_encoder = None
            self.spatial_embeddings = nn.ModuleDict()
            self.post_encoders = nn.ModuleDict()
            return

        if self.config.vision_encoder_name is not None:
            self.image_encoder = PretrainedImageEncoder(self.config)
        else:
            self.image_encoder = DefaultImageEncoder(self.config)

        if self.config.freeze_vision_encoder:
            freeze_image_encoder(self.image_encoder)

        dummy = torch.zeros(1, *self.config.input_features[self.image_keys[0]].shape)
        with torch.no_grad():
            _, channels, height, width = self.image_encoder(dummy).shape

        self.spatial_embeddings = nn.ModuleDict()
        self.post_encoders = nn.ModuleDict()

        for key in self.image_keys:
            name = key.replace(".", "_")
            self.spatial_embeddings[name] = SpatialLearnedEmbeddings(
                height=height,
                width=width,
                channel=channels,
                num_features=self.config.image_embedding_pooling_dim,
            )
            self.post_encoders[name] = nn.Sequential(
                nn.Dropout(0.1),
                nn.Linear(
                    in_features=channels * self.config.image_embedding_pooling_dim,
                    out_features=self.config.latent_dim,
                ),
                nn.LayerNorm(normalized_shape=self.config.latent_dim),
                nn.Tanh(),
            )

    def _init_state_layers(self) -> None:
        self.has_env = "observation.environment_state" in self.config.input_features
        self.has_state = "observation.state" in self.config.input_features
        if self.has_env:
            dim = self.config.input_features["observation.environment_state"].shape[0]
            self.env_encoder = nn.Sequential(
                nn.Linear(dim, self.config.latent_dim),
                nn.LayerNorm(self.config.latent_dim),
                nn.Tanh(),
            )
        if self.has_state:
            dim = self.config.input_features["observation.state"].shape[0]
            self.state_encoder = nn.Sequential(
                nn.Linear(dim, self.config.latent_dim),
                nn.LayerNorm(self.config.latent_dim),
                nn.Tanh(),
            )

    def _compute_output_dim(self) -> None:
        out = 0
        if self.has_images:
            out += len(self.image_keys) * self.config.latent_dim
        if self.has_env:
            out += self.config.latent_dim
        if self.has_state:
            out += self.config.latent_dim
        self._out_dim = out

    def forward(
        self, obs: dict[str, Tensor], cache: dict[str, Tensor] | None = None, detach: bool = False
    ) -> Tensor:
        obs = self.input_normalization(obs)
        parts = []
        if self.has_images:
            if cache is None:
                cache = self.get_cached_image_features(obs, normalize=False) # è¾“å…¥obsï¼Œè¿”å›ç¼“å­˜ResNet10çš„ç‰¹å¾
            parts.append(self._encode_images(cache, detach)) # ç¼–ç å›¾åƒ
        if self.has_env: # å¦‚æœconfigå½“ä¸­å®šä¹‰äº†observation.environment_state
            parts.append(self.env_encoder(obs["observation.environment_state"])) # ç¼–ç ç¯å¢ƒçŠ¶æ€
        if self.has_state: # å¦‚æœconfigå½“ä¸­å®šä¹‰äº†observation.state
            parts.append(self.state_encoder(obs["observation.state"])) # ç¼–ç çŠ¶æ€
        if parts:
            return torch.cat(parts, dim=-1) # å°†æ‰€æœ‰éƒ¨åˆ†æ‹¼æ¥åœ¨ä¸€èµ·

        raise ValueError(
            "No parts to concatenate, you should have at least one image or environment state or state"
        )

    def get_cached_image_features(self, obs: dict[str, Tensor], normalize: bool = False) -> dict[str, Tensor]:
        """Extract and optionally cache image features from observations.

        This function processes image observations through the vision encoder once and returns
        the resulting features.
        When the image encoder is shared between actor and critics AND frozen, these features can be safely cached and
        reused across policy components (actor, critic, discrete_critic), avoiding redundant forward passes.

        Performance impact:
        - The vision encoder forward pass is typically the main computational bottleneck during training and inference
        - Caching these features can provide 2-4x speedup in training and inference

        Normalization behavior:
        - When called from inside forward(): set normalize=False since inputs are already normalized
        - When called from outside forward(): set normalize=True to ensure proper input normalization

        Usage patterns:
        - Called in select_action() with normalize=True
        - Called in learner.py's get_observation_features() to pre-compute features for all policy components
        - Called internally by forward() with normalize=False

        Args:
            obs: Dictionary of observation tensors containing image keys
            normalize: Whether to normalize observations before encoding
                  Set to True when calling directly from outside the encoder's forward method
                  Set to False when calling from within forward() where inputs are already normalized

        Returns:
            Dictionary mapping image keys to their corresponding encoded features
        """
        # Early return if vision features are disabled
        if not self.has_images or len(self.image_keys) == 0 or self.image_encoder is None:
            return {}
        
        if normalize:
            obs = self.input_normalization(obs) # å½’ä¸€åŒ–å›¾åƒ
        batched = torch.cat([obs[k] for k in self.image_keys], dim=0) # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šåªæå–å›¾åƒé”®å¯¹åº”çš„æ•°æ®, åŒæ—¶æ‹¼æ¥åœ¨ä¸€èµ·
        out = self.image_encoder(batched)
        
        # Add feature visualization here
        if self.feature_viz_enabled and len(self.image_keys) > 0:
            # Visualize features for the first image key
            first_key = self.image_keys[0]
            first_image_features = out[:1]  # Take features for first image only
            self._visualize_features(first_image_features, first_key)
        
        chunks = torch.chunk(out, len(self.image_keys), dim=0) # å°†è¾“å‡ºåˆ†å‰²ä¸ºå¤šä¸ªå°å—
        return dict(zip(self.image_keys, chunks, strict=False)) # è¿”å›å­—å…¸ï¼Œé”®ä¸ºå›¾åƒé”®ï¼Œå€¼ä¸ºå°å—

    def _encode_images(self, cache: dict[str, Tensor], detach: bool) -> Tensor:
        """Encode image features from cached observations.

        This function takes pre-encoded image features from the cache and applies spatial embeddings and post-encoders.
        It also supports detaching the encoded features if specified.

        Args:
            cache (dict[str, Tensor]): The cached image features.
            detach (bool): Usually when the encoder is shared between actor and critics,
            we want to detach the encoded features on the policy side to avoid backprop through the encoder.
            More detail here `https://cdn.aaai.org/ojs/17276/17276-13-20770-1-2-20210518.pdf`

        Returns:
            Tensor: The encoded image features.
        
        cache:ç¼“å­˜ResNet10çš„ç‰¹å¾ 
        æ›´å¤šç»†èŠ‚è§ `https://cdn.aaai.org/ojs/17276/17276-13-20770-1-2-20210518.pdf`

        detach: å½“ç¼–ç å™¨åœ¨actorå’Œcriticä¹‹é—´å…±äº«æ—¶, æˆ‘ä»¬å¸Œæœ›åœ¨policy actorä¾§åˆ†ç¦»ç¼–ç å™¨ä»¥é¿å…é€šè¿‡ç¼–ç å™¨è¿›è¡Œåå‘ä¼ æ’­,ä½†æ˜¯å¯ä»¥ç¼–ç å™¨åªé€šè¿‡ critic çš„æ¢¯åº¦æ›´æ–°
        æ›´å¤šç»†èŠ‚è§ `https://cdn.aaai.org/ojs/17276/17276-13-20770-1-2-20210518.pdf`
        """
        feats = [] # å­˜å‚¨ç¼–ç åçš„ç‰¹å¾
        for k, feat in cache.items(): # éå†ç¼“å­˜ä¸­çš„ç‰¹å¾
            safe_key = k.replace(".", "_") # å°†ç‰¹å¾é”®ä¸­çš„ç‚¹æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
            x = self.spatial_embeddings[safe_key](feat) # å°†ç‰¹å¾é€šè¿‡ç©ºé—´å­¦ä¹ åµŒå…¥
            x = self.post_encoders[safe_key](x) # ä¿æŒæ¢¯åº¦ä¼ æ’­
            if detach:
                x = x.detach() # å¦‚æœdetachä¸ºTrueï¼Œåˆ™å°†ç‰¹å¾åˆ†ç¦»
            feats.append(x) # å°†ç¼–ç åçš„ç‰¹å¾æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        return torch.cat(feats, dim=-1) # å°†æ‰€æœ‰ç‰¹å¾æ‹¼æ¥åœ¨ä¸€èµ·

    @property
    def output_dim(self) -> int:
        """
            è¾“å‡ºè§‚æµ‹ç¼–ç çš„ç»´åº¦
        """
        return self._out_dim


class MLP(nn.Module):
    """Multi-layer perceptron builder.

    Dynamically constructs a sequence of layers based on `hidden_dims`:
      1) Linear (in_dim -> out_dim)
      2) Optional Dropout if `dropout_rate` > 0 and (not final layer or `activate_final`)
      3) LayerNorm on the output features
      4) Activation (standard for intermediate layers, `final_activation` for last layer if `activate_final`)

    Arguments:
        input_dim (int): Size of input feature dimension.
        hidden_dims (list[int]): Sizes for each hidden layer.
        activations (Callable or str): Activation to apply between layers.
        activate_final (bool): Whether to apply activation at the final layer.
        dropout_rate (Optional[float]): Dropout probability applied before normalization and activation.
        final_activation (Optional[Callable or str]): Activation for the final layer when `activate_final` is True.

    For each layer, `in_dim` is updated to the previous `out_dim`. All constructed modules are
    stored in `self.net` as an `nn.Sequential` container.
    """

    def __init__(
        self,
        input_dim: int,
        hidden_dims: list[int],
        activations: Callable[[torch.Tensor], torch.Tensor] | str = nn.SiLU(),
        activate_final: bool = False,
        dropout_rate: float | None = None,
        final_activation: Callable[[torch.Tensor], torch.Tensor] | str | None = None,
    ):
        super().__init__()
        layers: list[nn.Module] = []
        in_dim = input_dim
        total = len(hidden_dims)

        for idx, out_dim in enumerate(hidden_dims):
            # 1) linear transform
            layers.append(nn.Linear(in_dim, out_dim))

            is_last = idx == total - 1
            # 2-4) optionally add dropout, normalization, and activation
            if not is_last or activate_final:
                if dropout_rate and dropout_rate > 0:
                    layers.append(nn.Dropout(p=dropout_rate))
                layers.append(nn.LayerNorm(out_dim))
                act_cls = final_activation if is_last and final_activation else activations
                act = act_cls if isinstance(act_cls, nn.Module) else getattr(nn, act_cls)()
                layers.append(act)

            in_dim = out_dim

        self.net = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class CriticHead(nn.Module):
    """
    èŒè´£: å®ç°å•ä¸ªQå€¼ç½‘ç»œçš„é€»è¾‘
    è¾“å…¥: è§‚æµ‹ç¼–ç  + å½’ä¸€åŒ–åŠ¨ä½œ
    è¾“å‡º: Qå€¼
    """
    def __init__(
        self,
        input_dim: int,
        hidden_dims: list[int],
        activations: Callable[[torch.Tensor], torch.Tensor] | str = nn.SiLU(),
        activate_final: bool = False,
        dropout_rate: float | None = None,
        init_final: float | None = None,
        final_activation: Callable[[torch.Tensor], torch.Tensor] | str | None = None,
    ):
        super().__init__()
        self.net = MLP(
            input_dim=input_dim,
            hidden_dims=hidden_dims,
            activations=activations,
            activate_final=activate_final,
            dropout_rate=dropout_rate,
            final_activation=final_activation,
        )
        self.output_layer = nn.Linear(in_features=hidden_dims[-1], out_features=1)
        if init_final is not None:
            nn.init.uniform_(self.output_layer.weight, -init_final, init_final)
            nn.init.uniform_(self.output_layer.bias, -init_final, init_final)
        else:
            orthogonal_init()(self.output_layer.weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.output_layer(self.net(x))


class CriticEnsemble(nn.Module):
    """
    CriticEnsemble wraps multiple CriticHead modules into an ensemble.

    Args:
        encoder (SACObservationEncoder): encoder for observations.
        ensemble (List[CriticHead]): list of critic heads.
        output_normalization (nn.Module): normalization layer for actions.
        init_final (float | None): optional initializer scale for final layers.

    Forward returns a tensor of shape (num_critics, batch_size) containing Q-values.

    ç®¡ç†å¤šä¸ªCriticHeadæ¨¡å—çš„é›†åˆ
    èŒè´£: ç®¡ç†å¤šä¸ªCriticHeadæ¨¡å—çš„é›†åˆ
    è¾“å…¥: è§‚æµ‹ç¼–ç  + å½’ä¸€åŒ–åŠ¨ä½œ
    è¾“å‡º: Qå€¼å‘é‡
    """

    def __init__(
        self,
        encoder: SACObservationEncoder,
        ensemble: list[CriticHead],
        output_normalization: nn.Module,
        init_final: float | None = None,
    ):
        super().__init__()
        self.encoder = encoder
        self.init_final = init_final
        self.output_normalization = output_normalization
        self.critics = nn.ModuleList(ensemble)

    def forward(
        self,
        observations: dict[str, torch.Tensor],
        actions: torch.Tensor,
        observation_features: torch.Tensor | None = None,
    ) -> torch.Tensor:
        device = get_device_from_parameters(self)
        # Move each tensor in observations to device
        observations = {k: v.to(device) for k, v in observations.items()}
        # NOTE: We normalize actions it helps for sample efficiency
        actions: dict[str, torch.tensor] = {"action": actions}
        # NOTE: Normalization layer took dict in input and outputs a dict that why
        actions = self.output_normalization(actions)["action"]
        actions = actions.to(device)

        obs_enc = self.encoder(observations, cache=observation_features)

        inputs = torch.cat([obs_enc, actions], dim=-1)

        # Loop through critics and collect outputs
        q_values = []
        for critic in self.critics:
            q_values.append(critic(inputs))

        # Stack outputs to match expected shape [num_critics, batch_size]
        q_values = torch.stack([q.squeeze(-1) for q in q_values], dim=0)
        return q_values


class DiscreteCritic(nn.Module):
    """
        ç¦»æ•£Qç½‘ç»œçš„è¯„è®ºå®¶å¤´
        èŒè´£: å®ç°å•ä¸ªç¦»æ•£Qå€¼ç½‘ç»œçš„é€»è¾‘
        è¾“å…¥: è§‚æµ‹ç¼–ç 
        è¾“å‡º: Qå€¼
    """
    def __init__(
        self,
        encoder: nn.Module,
        input_dim: int,
        hidden_dims: list[int],
        output_dim: int = 3,
        activations: Callable[[torch.Tensor], torch.Tensor] | str = nn.SiLU(),
        activate_final: bool = False,
        dropout_rate: float | None = None,
        init_final: float | None = None,
        final_activation: Callable[[torch.Tensor], torch.Tensor] | str | None = None,
    ):
        super().__init__()
        self.encoder = encoder # è§‚æµ‹ç¼–ç å™¨
        self.output_dim = output_dim # è¾“å‡ºå±‚çš„å¤§å°ï¼Œé’ˆå¯¹ç›®æ ‡çš„åŠ¨ä½œç»´åº¦ï¼ˆæ¯”å¦‚Franka æŠ“/æ”¾/ä¿æŒ ä¸‰ä¸ªåŠ¨ä½œï¼‰

        self.net = MLP(
            input_dim=input_dim,
            hidden_dims=hidden_dims,
            activations=activations,
            activate_final=activate_final,
            dropout_rate=dropout_rate,
            final_activation=final_activation,
        )

        self.output_layer = nn.Linear(in_features=hidden_dims[-1], out_features=self.output_dim) # ç½‘ç»œçš„æœ€åä¸€å±‚ï¼Œå°†å‰é¢MLPç½‘ç»œæå–çš„ç‰¹å¾æ˜ å°„ä¸ºæœ€ç»ˆçš„è¾“å‡º
        if init_final is not None:
            nn.init.uniform_(self.output_layer.weight, -init_final, init_final)
            nn.init.uniform_(self.output_layer.bias, -init_final, init_final)
        else:
            orthogonal_init()(self.output_layer.weight)

    def forward(
        self, observations: torch.Tensor, observation_features: torch.Tensor | None = None
    ) -> torch.Tensor:
        device = get_device_from_parameters(self)
        observations = {k: v.to(device) for k, v in observations.items()}
        obs_enc = self.encoder(observations, cache=observation_features)
        return self.output_layer(self.net(obs_enc))


class Policy(nn.Module):
    """
    SACç­–ç•¥ç½‘ç»œï¼ˆActorç½‘ç»œï¼‰
    
    ç½‘ç»œæ¶æ„ï¼š
    1. è§‚æµ‹ç¼–ç å™¨ (encoder): å°†åŸå§‹è§‚æµ‹è½¬æ¢ä¸ºç‰¹å¾å‘é‡
    2. ä¸»å¹²ç½‘ç»œ (network): å¤šå±‚æ„ŸçŸ¥æœºï¼Œå¤„ç†ç¼–ç åçš„è§‚æµ‹ç‰¹å¾
    3. å‡å€¼å±‚ (mean_layer): è¾“å‡ºåŠ¨ä½œçš„å‡å€¼
    4. æ ‡å‡†å·®å±‚ (std_layer): è¾“å‡ºåŠ¨ä½œçš„æ ‡å‡†å·®ï¼ˆç”¨äºæ¢ç´¢ï¼‰
    
    å‰å‘ä¼ æ’­æµç¨‹ï¼š
    è§‚æµ‹è¾“å…¥ â†’ ç¼–ç å™¨ â†’ è§‚æµ‹ç‰¹å¾ â†’ ä¸»å¹²ç½‘ç»œ â†’ ç‰¹å¾å‘é‡
                                        â†“
                                    å‡å€¼å±‚ â†’ åŠ¨ä½œå‡å€¼
                                    æ ‡å‡†å·®å±‚ â†’ åŠ¨ä½œæ ‡å‡†å·®
                                        â†“
                                    TanhMultivariateNormalDiag â†’ é‡‡æ ·åŠ¨ä½œ
    
    è¾“å‡ºï¼š
    - actions: é‡‡æ ·çš„åŠ¨ä½œ
    - log_probs: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
    - means: åŠ¨ä½œçš„å‡å€¼ï¼ˆç”¨äºç¡®å®šæ€§åŠ¨ä½œé€‰æ‹©ï¼‰
    """
    def __init__(
        self,
        encoder: SACObservationEncoder,  # è§‚æµ‹ç¼–ç å™¨
        network: nn.Module,  # ä¸»å¹²ç½‘ç»œï¼ˆé€šå¸¸æ˜¯MLPï¼‰
        action_dim: int,  # åŠ¨ä½œç»´åº¦
        std_min: float = -5,  # æ ‡å‡†å·®çš„æœ€å°å€¼ï¼ˆlogç©ºé—´ï¼‰
        std_max: float = 2,  # æ ‡å‡†å·®çš„æœ€å¤§å€¼ï¼ˆlogç©ºé—´ï¼‰
        fixed_std: torch.Tensor | None = None,  # å›ºå®šçš„æ ‡å‡†å·®ï¼ˆå¦‚æœä¸ºNoneåˆ™å­¦ä¹ ï¼‰
        init_final: float | None = None,  # æœ€ç»ˆå±‚çš„åˆå§‹åŒ–å‚æ•°
        use_tanh_squash: bool = False,  # æ˜¯å¦ä½¿ç”¨tanhå‹ç¼©åŠ¨ä½œ
        encoder_is_shared: bool = False,  # ç¼–ç å™¨æ˜¯å¦ä¸Criticå…±äº«
    ):
        super().__init__()
        self.encoder: SACObservationEncoder = encoder  # è§‚æµ‹ç¼–ç å™¨
        self.network = network  # ä¸»å¹²ç½‘ç»œ
        self.action_dim = action_dim  # åŠ¨ä½œç»´åº¦
        self.std_min = std_min  # æ ‡å‡†å·®æœ€å°å€¼
        self.std_max = std_max  # æ ‡å‡†å·®æœ€å¤§å€¼
        self.fixed_std = fixed_std  # å›ºå®šæ ‡å‡†å·®
        self.use_tanh_squash = use_tanh_squash  # æ˜¯å¦ä½¿ç”¨tanhå‹ç¼©
        self.encoder_is_shared = encoder_is_shared  # ç¼–ç å™¨æ˜¯å¦å…±äº«

        # æ‰¾åˆ°ä¸»å¹²ç½‘ç»œæœ€åä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦
        for layer in reversed(network.net):
            if isinstance(layer, nn.Linear):
                out_features = layer.out_features
                break
        
        # å‡å€¼å±‚ï¼šå°†ä¸»å¹²ç½‘ç»œçš„è¾“å‡ºæ˜ å°„ä¸ºåŠ¨ä½œå‡å€¼
        self.mean_layer = nn.Linear(out_features, action_dim)
        if init_final is not None:
            # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
            nn.init.uniform_(self.mean_layer.weight, -init_final, init_final)
            nn.init.uniform_(self.mean_layer.bias, -init_final, init_final)
        else:
            # ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–
            orthogonal_init()(self.mean_layer.weight)

        # æ ‡å‡†å·®å±‚ï¼šå°†ä¸»å¹²ç½‘ç»œçš„è¾“å‡ºæ˜ å°„ä¸ºåŠ¨ä½œæ ‡å‡†å·®
        if fixed_std is None:
            # å¦‚æœä½¿ç”¨å­¦ä¹ å‹æ ‡å‡†å·®
            self.std_layer = nn.Linear(out_features, action_dim)
            if init_final is not None:
                # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
                nn.init.uniform_(self.std_layer.weight, -init_final, init_final)
                nn.init.uniform_(self.std_layer.bias, -init_final, init_final)
            else:
                # ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–
                orthogonal_init()(self.std_layer.weight)

    def forward(
        self,
        observations: torch.Tensor,  # è§‚æµ‹è¾“å…¥
        observation_features: torch.Tensor | None = None,  # é¢„è®¡ç®—çš„è§‚æµ‹ç‰¹å¾
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            observations: è§‚æµ‹å­—å…¸
            observation_features: é¢„è®¡ç®—çš„è§‚æµ‹ç‰¹å¾ï¼ˆç”¨äºç¼“å­˜ï¼‰
            
        Returns:
            actions: é‡‡æ ·çš„åŠ¨ä½œ
            log_probs: åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            means: åŠ¨ä½œçš„å‡å€¼
        """
        # å¦‚æœç¼–ç å™¨æ˜¯å…±äº«çš„ï¼Œåˆ™åˆ†ç¦»æ¢¯åº¦ä»¥é¿å…é€šè¿‡ç¼–ç å™¨è¿›è¡Œåå‘ä¼ æ’­
        # è¿™å¾ˆé‡è¦ï¼Œå¯ä»¥é¿å…ç¼–ç å™¨é€šè¿‡ç­–ç•¥ç½‘ç»œæ›´æ–°
        obs_enc = self.encoder(observations, cache=observation_features, detach=self.encoder_is_shared)

        # è·å–ä¸»å¹²ç½‘ç»œçš„è¾“å‡º
        outputs = self.network(obs_enc)
        
        # è®¡ç®—åŠ¨ä½œå‡å€¼
        means = self.mean_layer(outputs)

        # è®¡ç®—åŠ¨ä½œæ ‡å‡†å·®
        if self.fixed_std is None:
            # ä½¿ç”¨å­¦ä¹ å‹æ ‡å‡†å·®
            log_std = self.std_layer(outputs)  # è¾“å‡ºlogæ ‡å‡†å·®
            std = torch.exp(log_std)  # è½¬æ¢ä¸ºæ ‡å‡†å·®
            std = torch.clamp(std, self.std_min, self.std_max)  # è£å‰ªåˆ°æŒ‡å®šèŒƒå›´
        else:
            # ä½¿ç”¨å›ºå®šæ ‡å‡†å·®
            std = self.fixed_std.expand_as(means)

        # æ„å»ºå˜æ¢åˆ†å¸ƒï¼šä½¿ç”¨tanhå˜æ¢çš„å¤šå…ƒæ­£æ€åˆ†å¸ƒ
        dist = TanhMultivariateNormalDiag(loc=means, scale_diag=std)

        # é‡‡æ ·åŠ¨ä½œï¼ˆä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§ï¼‰
        actions = dist.rsample()

        # è®¡ç®—åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
        log_probs = dist.log_prob(actions)

        return actions, log_probs, means

    def get_features(self, observations: torch.Tensor) -> torch.Tensor:
        """è·å–è§‚æµ‹çš„ç¼–ç ç‰¹å¾"""
        device = get_device_from_parameters(self)
        observations = observations.to(device)
        if self.encoder is not None:
            with torch.inference_mode():
                return self.encoder(observations)
        return observations


class DefaultImageEncoder(nn.Module):
    def __init__(self, config: SACConfig):
        super().__init__()
        image_key = next(key for key in config.input_features if is_image_feature(key))
        self.image_enc_layers = nn.Sequential(
            nn.Conv2d(
                in_channels=config.input_features[image_key].shape[0],
                out_channels=config.image_encoder_hidden_dim,
                kernel_size=7,
                stride=2,
            ),
            nn.ReLU(),
            nn.Conv2d(
                in_channels=config.image_encoder_hidden_dim,
                out_channels=config.image_encoder_hidden_dim,
                kernel_size=5,
                stride=2,
            ),
            nn.ReLU(),
            nn.Conv2d(
                in_channels=config.image_encoder_hidden_dim,
                out_channels=config.image_encoder_hidden_dim,
                kernel_size=3,
                stride=2,
            ),
            nn.ReLU(),
            nn.Conv2d(
                in_channels=config.image_encoder_hidden_dim,
                out_channels=config.image_encoder_hidden_dim,
                kernel_size=3,
                stride=2,
            ),
            nn.ReLU(),
        )

    def forward(self, x):
        x = self.image_enc_layers(x)
        return x


def freeze_image_encoder(image_encoder: nn.Module):
    """Freeze all parameters in the encoder"""
    for param in image_encoder.parameters():
        param.requires_grad = False


class PretrainedImageEncoder(nn.Module):
    def __init__(self, config: SACConfig):
        super().__init__()

        self.image_enc_layers, self.image_enc_out_shape = self._load_pretrained_vision_encoder(config)

    def _load_pretrained_vision_encoder(self, config: SACConfig):
        """Set up CNN encoder"""
        from transformers import AutoModel

        self.image_enc_layers = AutoModel.from_pretrained(config.vision_encoder_name, trust_remote_code=True)

        if hasattr(self.image_enc_layers.config, "hidden_sizes"):
            self.image_enc_out_shape = self.image_enc_layers.config.hidden_sizes[-1]  # Last channel dimension
        elif hasattr(self.image_enc_layers, "fc"):
            self.image_enc_out_shape = self.image_enc_layers.fc.in_features
        else:
            raise ValueError("Unsupported vision encoder architecture, make sure you are using a CNN")
        return self.image_enc_layers, self.image_enc_out_shape

    def forward(self, x):
        enc_feat = self.image_enc_layers(x).last_hidden_state # ğŸ”¥ è¿™é‡Œè°ƒç”¨ ResNet10
        return enc_feat


def orthogonal_init():
    return lambda x: torch.nn.init.orthogonal_(x, gain=1.0)


class SpatialLearnedEmbeddings(nn.Module):
    def __init__(self, height, width, channel, num_features=8):
        """
        PyTorch implementation of learned spatial embeddings

        Args:
            height: Spatial height of input features
            width: Spatial width of input features
            channel: Number of input channels
            num_features: Number of output embedding dimensions
        """
        super().__init__()
        self.height = height
        self.width = width
        self.channel = channel
        self.num_features = num_features

        self.kernel = nn.Parameter(torch.empty(channel, height, width, num_features))

        nn.init.kaiming_normal_(self.kernel, mode="fan_in", nonlinearity="linear")

    def forward(self, features):
        """
        Forward pass for spatial embedding

        Args:
            features: Input tensor of shape [B, C, H, W] where B is batch size,
                     C is number of channels, H is height, and W is width
        Returns:
            Output tensor of shape [B, C*F] where F is the number of features
        """

        features_expanded = features.unsqueeze(-1)  # [B, C, H, W, 1]
        kernel_expanded = self.kernel.unsqueeze(0)  # [1, C, H, W, F]

        # Element-wise multiplication and spatial reduction
        output = (features_expanded * kernel_expanded).sum(dim=(2, 3))  # Sum over H,W dimensions

        # Reshape to combine channel and feature dimensions
        output = output.view(output.size(0), -1)  # [B, C*F]

        return output


class RescaleFromTanh(Transform):
    """
    ä»TanhèŒƒå›´é‡æ–°ç¼©æ”¾åˆ°æŒ‡å®šèŒƒå›´çš„å˜æ¢
    
    è¿™ä¸ªå˜æ¢ç”¨äºå°†åŠ¨ä½œä»Tanhçš„[-1, 1]èŒƒå›´é‡æ–°ç¼©æ”¾åˆ°åŠ¨ä½œç©ºé—´çš„å®é™…èŒƒå›´[low, high]ã€‚
    
    å˜æ¢å…¬å¼ï¼š
    - å‰å‘å˜æ¢ï¼šy = 0.5 * (x + 1.0) * (high - low) + low
    - åå‘å˜æ¢ï¼šx = 2.0 * (y - low) / (high - low) - 1.0
    
    ä½œç”¨ï¼š
    - å°†æ ‡å‡†åŒ–çš„åŠ¨ä½œèŒƒå›´æ˜ å°„åˆ°å®é™…çš„åŠ¨ä½œç©ºé—´
    - ä¿æŒå˜æ¢çš„å¯é€†æ€§
    - æä¾›æ­£ç¡®çš„é›…å¯æ¯”è¡Œåˆ—å¼ç”¨äºæ¦‚ç‡è®¡ç®—
    """
    def __init__(self, low: float = -1, high: float = 1):
        super().__init__()

        self.low = low  # åŠ¨ä½œç©ºé—´çš„ä¸‹ç•Œ
        self.high = high  # åŠ¨ä½œç©ºé—´çš„ä¸Šç•Œ

    def _call(self, x):
        """å‰å‘å˜æ¢ï¼šä»[-1, 1]é‡æ–°ç¼©æ”¾åˆ°[low, high]"""
        # é‡æ–°ç¼©æ”¾å…¬å¼ï¼šy = 0.5 * (x + 1.0) * (high - low) + low
        return 0.5 * (x + 1.0) * (self.high - self.low) + self.low

    def _inverse(self, y):
        """åå‘å˜æ¢ï¼šä»[low, high]é‡æ–°ç¼©æ”¾åˆ°[-1, 1]"""
        # åå‘ç¼©æ”¾å…¬å¼ï¼šx = 2.0 * (y - low) / (high - low) - 1.0
        return 2.0 * (y - self.low) / (self.high - self.low) - 1.0

    def log_abs_det_jacobian(self, x, y):
        """è®¡ç®—é›…å¯æ¯”è¡Œåˆ—å¼çš„å¯¹æ•°ç»å¯¹å€¼"""
        # log|d(rescale)/dx| = sum(log(0.5 * (high - low)))
        scale = 0.5 * (self.high - self.low)

        return torch.sum(torch.log(scale), dim=-1)


class TanhMultivariateNormalDiag(TransformedDistribution):
    """
    Tanhå˜æ¢çš„å¤šå…ƒæ­£æ€åˆ†å¸ƒ
    
    è¿™ä¸ªåˆ†å¸ƒç”¨äºSACä¸­çš„åŠ¨ä½œé‡‡æ ·ï¼Œå®ƒç»“åˆäº†ï¼š
    1. åŸºç¡€åˆ†å¸ƒï¼šå¤šå…ƒæ­£æ€åˆ†å¸ƒï¼ˆå¯¹è§’åæ–¹å·®çŸ©é˜µï¼‰
    2. å˜æ¢ï¼šTanhå˜æ¢ï¼Œå°†åŠ¨ä½œå‹ç¼©åˆ°[-1, 1]èŒƒå›´
    3. å¯é€‰å˜æ¢ï¼šä»[-1, 1]é‡æ–°ç¼©æ”¾åˆ°åŠ¨ä½œç©ºé—´çš„å®é™…èŒƒå›´
    
    ä½œç”¨ï¼š
    - ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
    - æä¾›å¹³æ»‘çš„åŠ¨ä½œåˆ†å¸ƒ
    - æ”¯æŒé‡å‚æ•°åŒ–é‡‡æ ·ï¼ˆç”¨äºæ¢¯åº¦è®¡ç®—ï¼‰
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - SACç®—æ³•ä¸­çš„åŠ¨ä½œé‡‡æ ·
    - è¿ç»­åŠ¨ä½œç©ºé—´çš„ç­–ç•¥ç½‘ç»œ
    """
    def __init__(self, loc, scale_diag, low=None, high=None):
        # åˆ›å»ºåŸºç¡€åˆ†å¸ƒï¼šå¤šå…ƒæ­£æ€åˆ†å¸ƒï¼ˆå¯¹è§’åæ–¹å·®çŸ©é˜µï¼‰
        base_dist = MultivariateNormal(loc, torch.diag_embed(scale_diag))

        # å®šä¹‰å˜æ¢åºåˆ—
        transforms = [TanhTransform(cache_size=1)]  # Tanhå˜æ¢

        # å¦‚æœæŒ‡å®šäº†åŠ¨ä½œèŒƒå›´ï¼Œæ·»åŠ é‡æ–°ç¼©æ”¾å˜æ¢
        if low is not None and high is not None:
            low = torch.as_tensor(low)
            high = torch.as_tensor(high)
            # åœ¨Tanhå˜æ¢ä¹‹å‰æ’å…¥é‡æ–°ç¼©æ”¾å˜æ¢
            transforms.insert(0, RescaleFromTanh(low, high))

        # åˆ›å»ºå˜æ¢åˆ†å¸ƒ
        super().__init__(base_dist, transforms)

    def mode(self):
        """è·å–åˆ†å¸ƒçš„ä¼—æ•°ï¼ˆæœ€å¯èƒ½çš„åŠ¨ä½œï¼‰"""
        # ä¼—æ•°æ˜¯åŸºç¡€åˆ†å¸ƒçš„å‡å€¼ç»è¿‡å˜æ¢åçš„ç»“æœ
        x = self.base_dist.mean

        # ä¾æ¬¡åº”ç”¨æ‰€æœ‰å˜æ¢
        for transform in self.transforms:
            x = transform(x)

        return x

    def stddev(self):
        """è·å–å˜æ¢ååˆ†å¸ƒçš„æ ‡å‡†å·®"""
        std = self.base_dist.stddev

        x = std

        # ä¾æ¬¡åº”ç”¨æ‰€æœ‰å˜æ¢
        for transform in self.transforms:
            x = transform(x)

        return x


def _convert_normalization_params_to_tensor(normalization_params: dict) -> dict:
    converted_params = {}
    for outer_key, inner_dict in normalization_params.items():
        converted_params[outer_key] = {}
        for key, value in inner_dict.items():
            converted_params[outer_key][key] = torch.tensor(value)
            if "image" in outer_key:
                # å›¾åƒæ•°æ®éœ€è¦ç‰¹æ®Šå¤„ç†ï¼šå°†å½¢çŠ¶è°ƒæ•´ä¸º (3, 1, 1) ä»¥ä¾¿å¹¿æ’­
                converted_params[outer_key][key] = converted_params[outer_key][key].view(3, 1, 1)

    return converted_params
