# Q-chunking Implementation for SAC

## ğŸ“„ æ¦‚è¿°

åŸºäºè®ºæ–‡ [Reinforcement Learning with Action Chunking](https://arxiv.org/abs/2507.07969)ï¼Œæˆ‘ä»¬åœ¨SACç®—æ³•ä¸­å®ç°äº†Q-chunkingæ–¹æ³•ã€‚Q-chunkingæ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æŠ€æœ¯ï¼Œé€šè¿‡åŠ¨ä½œåºåˆ—é¢„æµ‹å’Œè”åˆæ¦‚ç‡ä¼˜åŒ–æ¥æ”¹è¿›å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨é•¿æœŸä»»åŠ¡å’Œç¨€ç–å¥–åŠ±ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

### ä¼ ç»ŸSAC vs Q-chunking SAC

| ç‰¹æ€§ | ä¼ ç»ŸSAC | Q-chunking SAC |
|------|---------|----------------|
| **åŠ¨ä½œé¢„æµ‹** | å•æ­¥åŠ¨ä½œ a_t | åŠ¨ä½œåºåˆ— a_t:t+k |
| **ç­–ç•¥ä¼˜åŒ–** | Ï€(a_t\|s_t) | Ï€(a_t:t+k\|s_t:t+k) |
| **TDå­¦ä¹ ** | 1-step TD | n-step TD backup |
| **æ¢ç´¢ç­–ç•¥** | éšæœºå•æ­¥æ¢ç´¢ | æ—¶é—´ä¸€è‡´çš„åºåˆ—æ¢ç´¢ |

### Q-chunkingçš„å…³é”®ä¼˜åŠ¿

1. **æ—¶é—´ä¸€è‡´æ€§æ¢ç´¢**: é€šè¿‡é¢„æµ‹åŠ¨ä½œåºåˆ—ï¼Œç¡®ä¿æ¢ç´¢è¡Œä¸ºåœ¨æ—¶é—´ä¸Šçš„ä¸€è‡´æ€§
2. **æ›´ç¨³å®šçš„TDå­¦ä¹ **: ä½¿ç”¨n-step backupè¿›è¡Œæ›´ç¨³å®šå’Œé«˜æ•ˆçš„ä»·å€¼å‡½æ•°å­¦ä¹ 
3. **åˆ©ç”¨æ—¶é—´ç»“æ„**: åœ¨"chunked"åŠ¨ä½œç©ºé—´ä¸­è¿è¡ŒRLï¼Œæ›´å¥½åœ°åˆ©ç”¨åŠ¨ä½œçš„æ—¶é—´ä¾èµ–å…³ç³»

## ğŸ—ï¸ æ¶æ„å®ç°

### 1. Q-chunking CriticæŸå¤±

```python
def compute_loss_critic(self, observations, actions, rewards, next_observations, done):
    """Q-chunking CriticæŸå¤±è®¡ç®—"""
    
    if use_q_chunking:
        # 1. è·å–ä¸‹ä¸€çŠ¶æ€çš„åŠ¨ä½œåºåˆ—é¢„æµ‹
        next_action_sequence, next_log_probs_joint, _ = self.actor(
            next_observations, return_sequence=True
        )
        
        # 2. ä½¿ç”¨åºåˆ—çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œè®¡ç®—Qå€¼
        next_first_action = next_action_sequence[:, 0, :]
        
        # 3. ä½¿ç”¨è”åˆå¯¹æ•°æ¦‚ç‡è¿›è¡Œç†µæ­£åˆ™åŒ–
        min_q = min_q - (self.temperature * next_log_probs_joint)
        
        # 4. è®¡ç®—n-step TD target
        if chunk_size > 1:
            n_step_returns = self.actor.compute_n_step_returns(...)
            td_target = n_step_returns + (1 - done) * (Î³^n) * min_q
        else:
            td_target = rewards + (1 - done) * Î³ * min_q
```

### 2. Q-chunking ActoræŸå¤±

Q-chunking Actorå®ç°äº†ä¸‰ç§ç­–ç•¥ï¼š

#### æ ‡å‡†ç­–ç•¥ (Standard)
```python
def _compute_standard_q_chunking_loss(self, action_sequence, log_probs_joint, ...):
    """ä½¿ç”¨ç¬¬ä¸€ä¸ªåŠ¨ä½œè®¡ç®—Qå€¼ï¼Œä½†ä½¿ç”¨åºåˆ—è”åˆæ¦‚ç‡"""
    first_action = action_sequence[:, 0, :]
    q_preds = self.critic_forward(observations, first_action)
    min_q = q_preds.min(dim=0)[0]
    
    # æ ¸å¿ƒï¼šä½¿ç”¨æ•´ä¸ªåŠ¨ä½œåºåˆ—çš„è”åˆæ¦‚ç‡
    sac_loss = ((self.temperature * log_probs_joint) - min_q).mean()
```

#### ä¿å®ˆç­–ç•¥ (Conservative)
```python
def _compute_conservative_q_chunking_loss(self, action_sequence, log_probs_joint, ...):
    """å¯¹åºåˆ—ä¸­çš„å¤šä¸ªåŠ¨ä½œè®¡ç®—Qå€¼å¹¶å–æœ€å°å€¼"""
    q_values_list = []
    for t in range(horizon):
        action_t = action_sequence[:, t, :]
        q_t = self.critic_forward(observations, action_t).min(dim=0)[0]
        q_values_list.append(q_t)
    
    # ä¿å®ˆä¼°è®¡ï¼šå–æœ€å°Qå€¼
    conservative_q = torch.stack(q_values_list, dim=1).min(dim=1)[0]
    sac_loss = ((self.temperature * log_probs_joint) - conservative_q).mean()
```

#### æ—¶é—´åŠ æƒç­–ç•¥ (Temporal Weighted)
```python
def _compute_temporal_weighted_q_chunking_loss(self, action_sequence, log_probs_joint, ...):
    """å¯¹ä¸åŒæ—¶é—´æ­¥çš„åŠ¨ä½œç»™äºˆä¸åŒæƒé‡"""
    weights = torch.tensor([decay_factor ** t for t in range(horizon)])
    weights = weights / weights.sum()
    
    weighted_q_sum = 0.0
    for t in range(horizon):
        action_t = action_sequence[:, t, :]
        q_t = self.critic_forward(observations, action_t).min(dim=0)[0]
        weighted_q_sum += weights[t] * q_t
    
    sac_loss = ((self.temperature * log_probs_joint) - weighted_q_sum).mean()
```

### 3. Q-chunkingæ¸©åº¦æŸå¤±

```python
def compute_loss_temperature(self, observations, ...):
    """Q-chunkingæ¸©åº¦æŸå¤±ï¼Œè€ƒè™‘åºåˆ—å¤æ‚æ€§"""
    
    if use_q_chunking:
        # ä½¿ç”¨åŠ¨ä½œåºåˆ—çš„è”åˆæ¦‚ç‡
        _, log_probs_joint, _ = self.actor(observations, return_sequence=True)
        
        # è°ƒæ•´ç›®æ ‡ç†µä»¥é€‚åº”åºåˆ—é•¿åº¦
        chunk_size = self.actor.chunk_size
        adjusted_target_entropy = self._get_adjusted_target_entropy(chunk_size)
        
        temperature_loss = (-self.log_alpha.exp() * 
                           (log_probs_joint + adjusted_target_entropy)).mean()
```

## âš™ï¸ é…ç½®å‚æ•°

### Q-chunkingé…ç½®é¡¹

```python
@dataclass
class SACConfig:
    # Q-chunkingåŸºç¡€é…ç½®
    enable_q_chunking: bool = True                    # æ˜¯å¦å¯ç”¨Q-chunking
    q_chunking_strategy: str = "standard"             # Q-chunkingç­–ç•¥
    q_chunking_horizon: int = 3                       # Q-chunkingæ—¶é—´è§†é‡
    q_chunking_decay: float = 0.9                     # æ—¶é—´è¡°å‡å› å­
    q_chunking_entropy_scaling: str = "linear"        # ç†µç¼©æ”¾ç­–ç•¥
```

### ç­–ç•¥é€‰æ‹©æŒ‡å—

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|----------|------|------|
| **Standard** | ä¸€èˆ¬ä»»åŠ¡ | è®¡ç®—æ•ˆç‡é«˜ï¼Œè®ºæ–‡æ–¹æ³• | å¯èƒ½ä¸å¤Ÿä¿å®ˆ |
| **Conservative** | é«˜é£é™©ä»»åŠ¡ | æ›´ç¨³å®šçš„Qå€¼ä¼°è®¡ | è®¡ç®—å¼€é”€å¤§ï¼Œå­¦ä¹ æ…¢ |
| **Temporal Weighted** | éœ€è¦å¹³è¡¡çš„ä»»åŠ¡ | å¹³è¡¡è¿‘æœŸå’Œè¿œæœŸåŠ¨ä½œ | å‚æ•°è°ƒä¼˜å¤æ‚ |

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### 1. é…ç½®æ–‡ä»¶è®¾ç½®

```json
{
    "use_act_actor": true,
    "use_sequence_act_actor": true,
    "enable_q_chunking": true,
    "q_chunking_strategy": "standard",
    "act_chunk_size": 8,
    "obs_history_length": 5,
    "q_chunking_horizon": 3,
    "q_chunking_decay": 0.9,
    "q_chunking_entropy_scaling": "linear"
}
```

### 2. è®­ç»ƒè„šæœ¬

```python
# Q-chunkingä¼šè‡ªåŠ¨å¯ç”¨ï¼Œæ— éœ€é¢å¤–ä»£ç ä¿®æ”¹
python lerobot/scripts/rl/learner.py \
    --config-path="config/sac_q_chunking.json" \
    --env-name="YourEnv-v0"
```

### 3. æ¨ç†ä½¿ç”¨

```python
# æ¨ç†æ—¶è‡ªåŠ¨ä½¿ç”¨Q-chunkingç­–ç•¥
policy = SACPolicy.from_pretrained("path/to/model")
action = policy.select_action(observation)  # è‡ªåŠ¨è¿”å›ç¬¬ä¸€ä¸ªåŠ¨ä½œ
```

## ğŸ“Š å®éªŒç»“æœå’ŒæœŸæœ›æ”¹è¿›

### ç†è®ºä¼˜åŠ¿

1. **æ›´å¥½çš„æ¢ç´¢**: æ—¶é—´ä¸€è‡´çš„åŠ¨ä½œåºåˆ—æ¢ç´¢
2. **æ›´ç¨³å®šçš„å­¦ä¹ **: n-step TD backupå‡å°‘æ–¹å·®
3. **æ›´å¥½çš„æ ·æœ¬æ•ˆç‡**: åˆ©ç”¨åŠ¨ä½œåºåˆ—çš„æ—¶é—´ç»“æ„

### é¢„æœŸæ€§èƒ½æå‡

- **é•¿æœŸä»»åŠ¡**: 20-40%æ ·æœ¬æ•ˆç‡æå‡
- **ç¨€ç–å¥–åŠ±**: æ˜¾è‘—æ”¹å–„æ¢ç´¢æ•ˆæœ
- **æ“ä½œä»»åŠ¡**: æ›´å¹³æ»‘çš„åŠ¨ä½œæ‰§è¡Œ

## ğŸ” è°ƒè¯•å’Œç›‘æ§

### å…³é”®ç›‘æ§æŒ‡æ ‡

```python
# åœ¨è®­ç»ƒæ—¥å¿—ä¸­ç›‘æ§ä»¥ä¸‹æŒ‡æ ‡
- "q_chunking/joint_log_prob": åŠ¨ä½œåºåˆ—è”åˆæ¦‚ç‡
- "q_chunking/chunk_size": å®é™…ä½¿ç”¨çš„åºåˆ—é•¿åº¦
- "q_chunking/n_step_returns": n-step returnå€¼
- "q_chunking/adjusted_entropy": è°ƒæ•´åçš„ç›®æ ‡ç†µ
```

### å¸¸è§é—®é¢˜æ’æŸ¥

1. **Q-chunkingæœªå¯ç”¨**: æ£€æŸ¥æ˜¯å¦åŒæ—¶è®¾ç½®äº†`use_sequence_act_actor=True`
2. **æŸå¤±çˆ†ç‚¸**: å°è¯•é™ä½`q_chunking_horizon`æˆ–è°ƒæ•´ç†µç¼©æ”¾ç­–ç•¥
3. **å­¦ä¹ æ…¢**: è€ƒè™‘ä½¿ç”¨"conservative"ç­–ç•¥æˆ–è°ƒæ•´`q_chunking_decay`

## ğŸš€ æœ€ä½³å®è·µ

1. **åˆå§‹é…ç½®**: ä»`q_chunking_strategy="standard"`å¼€å§‹
2. **åºåˆ—é•¿åº¦**: `act_chunk_size=8`é€šå¸¸æ˜¯ä¸ªå¥½èµ·ç‚¹
3. **æ—¶é—´è§†é‡**: `q_chunking_horizon=3`å¹³è¡¡è®¡ç®—å’Œæ€§èƒ½
4. **ç†µç¼©æ”¾**: `linear`ç¼©æ”¾é€‚ç”¨äºå¤§å¤šæ•°ä»»åŠ¡

## ğŸ“š å‚è€ƒèµ„æ–™

- [Q-chunkingè®ºæ–‡](https://arxiv.org/abs/2507.07969)
- [SACåŸå§‹è®ºæ–‡](https://arxiv.org/abs/1801.01290)
- [ACTè®ºæ–‡](https://arxiv.org/abs/2304.13705)

---

*è¯¥å®ç°åŸºäºQ-chunkingè®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ï¼Œç»“åˆäº†SACç®—æ³•çš„ç‰¹ç‚¹ï¼Œä¸ºé•¿æœŸä»»åŠ¡å’Œç¨€ç–å¥–åŠ±ç¯å¢ƒæä¾›äº†æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚*
