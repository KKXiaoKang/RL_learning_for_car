from typing import Any, Dict, Tuple

import numpy as np
from gymnasium import spaces
import rospy
import message_filters
import threading
import os
import xml.etree.ElementTree as ET
import cv2
from std_srvs.srv import Trigger
from geometry_msgs.msg import PoseStamped, Twist
from sensor_msgs.msg import Image, JointState, CompressedImage
from std_msgs.msg import Float64MultiArray
from kuavo_msgs.msg import sensorsData
from ocs2_msgs.msg import mpc_observation
from kuavo_msgs.srv import resetIsaaclab
# Add the following imports for the new message types
from kuavo_msgs.msg import twoArmHandPoseCmd, twoArmHandPose, armHandPose, ikSolveParam
from kuavo_msgs.srv import changeTorsoCtrlMode, changeTorsoCtrlModeRequest, changeArmCtrlMode, changeArmCtrlModeRequest
from enum import Enum
from gym_hil.isaacLab_gym_env import IsaacLabGymEnv
from collections import deque
import time

"""
# èº¯å¹² 2 + 14joint = 16 action dim 16æ§åˆ¶ | True
# èº¯å¹² 2 + 6pose = 8 action dim 8æ§åˆ¶ | False
"""
TEST_DEMO_USE_ACTION_16_DIM = False 
USE_CMD_VEL = False

IF_USE_ZERO_OBS_FLAG = False # æ˜¯å¦ä½¿ç”¨0è§‚æµ‹
IF_USE_ARM_MPC_CONTROL = False # æ˜¯å¦ä½¿ç”¨è¿åŠ¨å­¦mpc | ikä½œä¸ºæœ«ç«¯æ§åˆ¶æ‰‹æ®µ
LEARN_TARGET_EEF_POSE_TARGET = True # æ˜¯å¦ä½¿ç”¨ç›®æ ‡æœ«ç«¯ä½ç½®ä½œä¸ºå­¦ä¹ ç›®æ ‡

# å¢åŠ DEMOæ¨¡å¼çš„åŠ¨ä½œå°ºåº¦å¸¸é‡
DEMO_MAX_INCREMENT_PER_STEP = 0.02  # DEMOæ¨¡å¼ä¸‹æ¯æ­¥æœ€å¤§2cmå¢é‡ï¼ˆç²¾ç»†æ§åˆ¶ï¼‰
DEMO_MAX_INCREMENT_RANGE = 0.4     # DEMOæ¨¡å¼ä¸‹æœ€å¤§ç´¯ç§¯å¢é‡èŒƒå›´40cm

# æ‰‹è‚˜å…³èŠ‚é…ç½®
"""
    "left_elbow": [-0.0178026345146559, 0.4004180715613648, 0.17417275957965042],
    "right_elbow": [-0.0178026345146559, -0.4004180715613648, 0.17417275957965042]
"""
DEMO_LEFT_ELBOW_POS = np.array([-0.0178026345146559, 0.4004180715613648, 0.17417275957965042])
DEMO_RIGHT_ELBOW_POS = np.array([-0.0178026345146559, -0.4004180715613648, 0.17417275957965042])

# Target Key-Points
DEMO_TARGET_LEFT_POS_WORLD = np.array([0.49856212735176086, 0.22971099615097046, 0.9128270149230957])
DEMO_TARGET_LEFT_QUAT = np.array([0.0, -0.70711, 0.0, 0.70711])
DEMO_TARGET_RIGHT_POS_WORLD = np.array([0.49856212735176086, -0.22971099615097046, 0.9128270149230957])
DEMO_TARGET_RIGHT_QUAT = np.array([0.0, -0.70711, 0.0, 0.70711])
# normal box pose: 0.4083724915981293 -0.009208906441926956 0.9235677719116211
DEMO_TARGET_BOX_POS_WORLD = np.array([0.4000142514705658, 0.0020876736380159855, 1.1273181653022766]) 

# Joint control mode target joint angles (in radians)
DEMO_TARGET_LEFT_JOINT_ANGLES = np.array([
    -0.0974561870098114, -0.3386945128440857, 0.14182303845882416, 
    -1.5295206308364868, -0.35505950450897217, -0.06419740617275238, 0.057071615010499954, 
])

DEMO_TARGET_RIGHT_JOINT_ANGLES = np.array([
    -0.10812032222747803, 0.33889538049697876, -0.16906462609767914, 
    -1.522423505783081, 0.37139785289764404, 0.12298937886953354, 0.04463687911629677,
])

# Combined target joint angles for convenience (left + right)
DEMO_TARGET_ALL_JOINT_ANGLES = np.concatenate([DEMO_TARGET_LEFT_JOINT_ANGLES, DEMO_TARGET_RIGHT_JOINT_ANGLES])

# æ˜¯å¦ä½¿ç”¨åˆå§‹ä½ç½®éšæœºåŒ–
IF_USE_RANDOM_INITIAL_POSITION = True

class IncrementalMpcCtrlMode(Enum):
    """è¡¨ç¤ºKuavoæœºå™¨äºº Manipulation MPC æ§åˆ¶æ¨¡å¼çš„æšä¸¾ç±»"""
    NoControl = 0
    """æ— æ§åˆ¶"""
    ArmOnly = 1
    """ä»…æ§åˆ¶æ‰‹è‡‚"""
    BaseOnly = 2
    """ä»…æ§åˆ¶åº•åº§"""
    BaseArm = 3
    """åŒæ—¶æ§åˆ¶åº•åº§å’Œæ‰‹è‡‚"""
    ERROR = -1
    """é”™è¯¯çŠ¶æ€"""

def ros_image_to_cv2(ros_image):
    """
    Convert ROS Image message to OpenCV image format without using cv_bridge.
    Compatible with numpy 2.x.
    
    Args:
        ros_image: sensor_msgs/Image message
        
    Returns:
        OpenCV image (numpy array)
    """
    # Get image data as numpy array
    if ros_image.encoding == 'rgb8':
        channels = 3
        dtype = np.uint8
    elif ros_image.encoding == 'bgr8':
        channels = 3  
        dtype = np.uint8
    elif ros_image.encoding == 'mono8':
        channels = 1
        dtype = np.uint8
    elif ros_image.encoding == '16UC1':
        channels = 1
        dtype = np.uint16
    else:
        raise ValueError(f"Unsupported encoding: {ros_image.encoding}")
    
    # Convert image data to numpy array
    img_array = np.frombuffer(ros_image.data, dtype=dtype)
    
    # Reshape to image dimensions
    if channels == 1:
        cv_image = img_array.reshape(ros_image.height, ros_image.width)
    else:
        cv_image = img_array.reshape(ros_image.height, ros_image.width, channels)
    
    return cv_image


def ros_compressed_image_to_cv2(compressed_image):
    """
    Convert ROS CompressedImage message to OpenCV image format.
    Compatible with numpy 2.x.
    
    Args:
        compressed_image: sensor_msgs/CompressedImage message
        
    Returns:
        OpenCV image (numpy array)
    """
    # Convert compressed image data to numpy array
    np_arr = np.frombuffer(compressed_image.data, np.uint8)
    
    # Decode the image
    cv_image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
    
    return cv_image


class RLKuavoGymEnv(IsaacLabGymEnv):
    """
    A gymnasium environment for the RL Kuavo robot task in Isaac Lab.
    This class will define the task-specific logic, including reward calculation,
    termination conditions, and observation/action spaces.
    """

    metadata = {"render_modes": ["human"], "render_fps": 30}

    # å›ºå®šåŸºå‡†ä½ç½®å¸¸é‡ - åŸºäºè¿™äº›ä½ç½®è¿›è¡Œå¢é‡æ§åˆ¶
    FIXED_LEFT_POS = np.array([0.3178026345146559, 0.4004180715613648, -0.019417275957965042])
    FIXED_LEFT_QUAT = np.array([0.0, -0.70711, 0.0, 0.70711])
    FIXED_RIGHT_POS = np.array([0.3178026345146559, -0.4004180715613648, -0.019417275957965042])
    FIXED_RIGHT_QUAT = np.array([0.0, -0.70711, 0.0, 0.70711])
    
    # å¢é‡æ§åˆ¶çš„æœ€å¤§èŒƒå›´ (ç±³)
    MAX_INCREMENT_RANGE = 0.2  # Â±20cmçš„å¢é‡èŒƒå›´
    MAX_INCREMENT_PER_STEP = 0.02  # æ¯æ­¥æœ€å¤§2cmçš„å¢é‡å˜åŒ–
    
    # End-effector position constraints (absolute world coordinates)
    # åŒæ‰‹xèŒƒå›´: [0.2, 0.5]
    # å·¦æ‰‹yèŒƒå›´: [0.0, 0.5], å³æ‰‹yèŒƒå›´: [-0.5, 0.0]  
    # åŒæ‰‹zèŒƒå›´: [0.0, 0.2]
    EEF_POS_LIMITS = {
        'x_min': 0.3, 'x_max': 0.6,
        'left_y_min': 0.0, 'left_y_max': 0.5,
        'right_y_min': -0.5, 'right_y_max': 0.0,
        'z_min': 0.0, 'z_max': 0.3
    }

    def __init__(self, debug: bool = False, image_size=(224, 224), enable_roll_pitch_control: bool = False, 
                 vel_smoothing_factor: float = 0.3, arm_smoothing_factor: float = 0.4, 
                 wbc_observation_enabled: bool = False, action_dim: int = None, image_obs: bool = True,
                 render_mode: str = None, use_gripper: bool = True, gripper_penalty: float = 0.0,
                 box_reward_weight: float = 3.0, hand_reward_weight: float = 1.0,
                 auto_record_tool_enable: bool = False):
        # Store initialization parameters
        self.image_obs = image_obs
        self.render_mode = render_mode  
        self.use_gripper = use_gripper
        self.gripper_penalty = gripper_penalty
        
        # æ˜¯å¦å¯ç”¨è‡ªåŠ¨å½•åˆ¶åŠŸèƒ½
        self.auto_record_tool_enable = auto_record_tool_enable
        print(f"auto_record_tool_enable è‡ªåŠ¨å½•åˆ¶åŠŸèƒ½: {auto_record_tool_enable}")
        
        # Reward weight parameters
        """
            æœ€ç»ˆkey-pointåŸºäºbaseæŠ¬å‡é«˜åº¦çš„episode reward
            0.00 -> -109
            0.10 -> -89
            0.20 -> -77
            0.30 -> -28
        """
        self.box_reward_weight = box_reward_weight  # ç®±å­ç§»åŠ¨å¥–åŠ±æƒé‡
        self.hand_reward_weight = hand_reward_weight  # æ‰‹ç§»åŠ¨å¥–åŠ±æƒé‡
        print(f"box_reward_weight: {box_reward_weight}, hand_reward_weight: {hand_reward_weight}")

        # Separate storage for headerless topics that will be initialized in callbacks.
        # This needs to be done BEFORE super().__init__() which sets up subscribers.
        self.latest_ang_vel = None
        self.latest_lin_accel = None
        self.latest_wbc = None
        self.latest_robot_pose = None  # New: for robot pose when WBC is disabled
        # Add new variables for base_link end-effector poses
        self.latest_base_link_eef_left = None
        self.latest_base_link_eef_right = None
        self.ang_vel_lock = threading.Lock()
        self.lin_accel_lock = threading.Lock()
        self.wbc_lock = threading.Lock()
        self.robot_pose_lock = threading.Lock()  # New: lock for robot pose
        # Add new locks for base_link end-effector poses
        self.base_link_eef_left_lock = threading.Lock()
        self.base_link_eef_right_lock = threading.Lock()
        
        self.enable_roll_pitch_control = enable_roll_pitch_control
        self.wbc_observation_enabled = wbc_observation_enabled  # New: WBC observation flag
        self.debug = debug  # Set debug before super().__init__() to avoid AttributeError
        
        # VR intervention state and data - MUST be initialized before super().__init__()
        # because ROS subscribers will start receiving messages immediately
        self._is_vr_intervention_active = False
        self._latest_vr_cmd_vel = None
        self._latest_vr_arm_traj = None
        self._vr_action_lock = threading.Lock()
        self._should_publish_action = True
        self._is_first_step = True
        self._vr_intervention_mode = False
        self.latest_vr_cmd_vel = None
        self.latest_vr_arm_traj = None
        self.vr_cmd_vel_lock = threading.Lock()
        self.vr_arm_traj_lock = threading.Lock()

        # æ·»åŠ å½“å‰æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®çŠ¶æ€è·Ÿè¸ªï¼ˆç”¨äºå¢é‡æ§åˆ¶ï¼‰
        if not IF_USE_RANDOM_INITIAL_POSITION:
            self.current_left_pos = np.array([0.3178026345146559, 0.4004180715613648, -0.019417275957965042], dtype=np.float32)
            self.current_right_pos = np.array([0.3178026345146559, -0.4004180715613648, -0.019417275957965042], dtype=np.float32)
        else:
            # éšæœºåˆå§‹åŒ–ä½ç½®ï¼ˆå°†åœ¨resetä¸­ç”Ÿæˆï¼‰
            self.random_initial_left_pos = None
            self.random_initial_right_pos = None
            self.current_left_pos = None
            self.current_right_pos = None

        # å¢é‡æ§åˆ¶å‚æ•°
        self.INCREMENT_SCALE = 0.01  # å°†action[-1,1]ç¼©æ”¾åˆ°Â±0.01mçš„å¢é‡èŒƒå›´

        # Call the base class constructor to set up the node and observation buffer
        super().__init__()
        self.image_size = image_size
        
        # State observation dimension - depends on WBC observation mode
        if self.wbc_observation_enabled:
            # agent_pos: 7 (left_eef) + 7 (right_eef) + 14 (arm_joints) + 3 (imu_ang_vel) + 3 (imu_lin_accel) + 12 (wbc) = 46
            # environment_state: 3 (box_pos) + 4 (box_orn) = 7
            agent_dim = 46
            env_state_dim = 7
        else:
            # agent_pos: 3 (left_eef_pos) + 3 (right_eef_pos) + 14 (arm_joints) + 3 (robot_pos) + 3 (base_link_left_eef) + 3 (base_link_right_eef) = 29
            # environment_state: 3 (box_pos) = 3
            agent_dim = 29
            env_state_dim = 3

        """
            vel_dim 
        """
        if self.enable_roll_pitch_control:
            self.vel_dim = 6
            self.vel_action_scale = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])  # m/s and rad/s
        else:
            self.vel_dim = 4
            self.vel_action_scale = np.array([1.0, 1.0, 1.0, 1.0])  # m/s and rad/s x y z yaw
        
        if TEST_DEMO_USE_ACTION_16_DIM:
            self.vel_dim = 2
            self.vel_action_scale = np.array([1.0, 1.0])  # m/s and rad/s æ§åˆ¶xå’Œyaw
        else:
            if USE_CMD_VEL:
                self.vel_dim = 2
                self.vel_action_scale = np.array([1.0, 1.0])  # m/s and rad/s æ§åˆ¶xå’Œyaw
            else:
                self.vel_dim = 0
                self.vel_action_scale = np.array([1.0, 1.0])  # m/s and rad/s æ§åˆ¶xå’Œyaw
        
        """
            action_dim = vel_dim + arm_dim
        """
        # Use provided action_dim if specified, otherwise use default calculation
        if TEST_DEMO_USE_ACTION_16_DIM:
            # vel_dim 2 + arm angle 7 + 7 
            self.arm_dim = 14
            self.action_dim = self.vel_dim + self.arm_dim  # 2 + 7 + 7 = 16
        else:
            # vel_dim 2 + eef pose 3 + 3
            if USE_CMD_VEL:
                self.arm_dim = 6
                self.action_dim = self.vel_dim + self.arm_dim # vel_dim 2 + eef pose 3 + 3 = 8
            else:
                self.arm_dim = 6
                self.action_dim = self.arm_dim # ef pose 3 + 3 = 6
        
        # elif self.wbc_observation_enabled:
        #     self.arm_dim = 14 # å…³èŠ‚ joint space
        #     self.action_dim = self.vel_dim + self.arm_dim # 4 + 14 = 18
        # else:
        #     # Default behavior: 14 for arm joints
        #     self.arm_dim = 6 # æœ«ç«¯ eef position
        #     self.action_dim = self.vel_dim + self.arm_dim # 4 + 6 = 10

        agent_box = spaces.Box(-np.inf, np.inf, (agent_dim,), dtype=np.float32)
        env_box = spaces.Box(-np.inf, np.inf, (env_state_dim,), dtype=np.float32)

        # Define observation and action spaces for the Kuavo robot
        self.observation_space = spaces.Dict(
            {
                "pixels": spaces.Dict(
                    {
                        "front": spaces.Box(
                            0,
                            255,
                            (self.image_size[0], self.image_size[1], 3),
                            dtype=np.uint8,
                        )
                    }
                ),
                "agent_pos": agent_box,
                "environment_state": env_box,
            }
        )

        # Action space
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.action_dim,), dtype=np.float32)

        # Define arm joint names in order (only use the number of joints specified by arm_dim)
        self.arm_joint_names = [f'zarm_l{i}_joint' for i in range(1, 8)] + [f'zarm_r{i}_joint' for i in range(1, 8)]
        # Truncate to match arm_dim if necessary
        if len(self.arm_joint_names) > self.arm_dim:
            self.arm_joint_names = self.arm_joint_names[:self.arm_dim]

        # Parse URDF for joint limits to define action scaling for arms
        urdf_path = os.path.join(os.path.dirname(__file__), '../assets/biped_s45.urdf')
        try:
            tree = ET.parse(urdf_path)
            root = tree.getroot()

            joint_limits = {}
            for joint_name in self.arm_joint_names:
                joint_element = root.find(f".//joint[@name='{joint_name}']")
                if joint_element is not None:
                    limit_element = joint_element.find('limit')
                    if limit_element is not None:
                        lower = float(limit_element.get('lower'))
                        upper = float(limit_element.get('upper'))
                        joint_limits[joint_name] = {'lower': lower, 'upper': upper}
            
            lower_limits = np.array([joint_limits[name]['lower'] for name in self.arm_joint_names])
            upper_limits = np.array([joint_limits[name]['upper'] for name in self.arm_joint_names])

            self.arm_joint_centers = (upper_limits + lower_limits) / 2.0
            self.arm_joint_scales = (upper_limits - lower_limits) / 2.0
            
            # FIXME
            self.arm_joint_centers = np.zeros(self.arm_dim)
            self.arm_joint_scales = np.full(self.arm_dim, np.deg2rad(10.0))

            print(f" arm_joint_centers: {self.arm_joint_centers}")
            print(f" arm_joint_scales: {self.arm_joint_scales}")
            print(f" arm_joint_center rad2deg: {np.rad2deg(self.arm_joint_centers)}") 

        except (ET.ParseError, FileNotFoundError, KeyError) as e:
            rospy.logerr(f"Failed to parse URDF or find all joint limits: {e}")
            # Fallback to a default scaling if URDF parsing fails
            self.arm_joint_centers = np.zeros(self.arm_dim)
            self.arm_joint_scales = np.full(self.arm_dim, np.deg2rad(10.0))

        # Task-specific state
        self.initial_box_pose = None
        self.last_action = np.zeros(self.action_space.shape, dtype=np.float32)
        
        # Last converted VR action for recording
        self._last_vr_action = np.zeros(self.action_space.shape, dtype=np.float32)
        
        # For smooth end-effector motion penalty
        self.last_left_eef_pos = None
        self.last_right_eef_pos = None
        
        # For trajectory tracking rewards - track distance changes over time
        self.last_dist_left_hand_to_box = None
        self.last_dist_right_hand_to_box = None
        self.last_dist_torso_to_box = None
        
        # Step counting for efficiency reward
        self.episode_step_count = 0
        
        # For continuous approach tracking
        self.consecutive_approach_steps_left = 0
        self.consecutive_approach_steps_right = 0
        self.consecutive_approach_steps_torso = 0
        
        # Distance change history for smooth trajectory reward
        self.distance_change_history_left = deque(maxlen=5)
        self.distance_change_history_right = deque(maxlen=5)
        
        # Action smoothing parameters
        self.vel_smoothing_factor = vel_smoothing_factor  # 0.0 = no smoothing, 1.0 = full smoothing
        self.arm_smoothing_factor = arm_smoothing_factor  # Slightly more smoothing for arm joints
        self.last_smoothed_vel_action = np.zeros(self.vel_dim, dtype=np.float32)
        self.last_smoothed_arm_action = np.zeros(self.arm_dim, dtype=np.float32)
        self.is_first_action = True
        
    def change_mobile_ctrl_mode(self, mode: int):
        # print(f"change_mobile_ctrl_mode: {mode}")
        mobile_manipulator_service_name = "/mobile_manipulator_mpc_control"
        try:
            rospy.wait_for_service(mobile_manipulator_service_name)
            changeHandTrackingMode_srv = rospy.ServiceProxy(mobile_manipulator_service_name, changeArmCtrlMode)
            changeHandTrackingMode_srv(mode)
        except rospy.ROSException:
            rospy.logerr(f"Service {mobile_manipulator_service_name} not available")

    def _compute_symmetry_constraint(self, left_pos: np.ndarray, right_pos: np.ndarray, box_pos: np.ndarray) -> float:
        """
        è®¡ç®—åŒè‡‚å¯¹ç§°æ€§çº¦æŸå¥–åŠ±ã€‚
        
        å¯¹ç§°æ€§çº¦æŸåŒ…æ‹¬ï¼š
        1. åŒæ‰‹ç›¸å¯¹äºç®±å­çš„å¯¹ç§°æ€§ï¼ˆé•œåƒå¯¹ç§°ï¼‰
        2. åŒæ‰‹é«˜åº¦çš„ä¸€è‡´æ€§  
        3. åŒæ‰‹åˆ°ç®±å­è·ç¦»çš„å‡è¡¡æ€§
        4. åŒæ‰‹Yåæ ‡ç›¸å¯¹äºç®±å­ä¸­å¿ƒçš„å¯¹ç§°æ€§
        
        Args:
            left_pos: å·¦æ‰‹ä¸–ç•Œåæ ‡ä½ç½® [x, y, z]
            right_pos: å³æ‰‹ä¸–ç•Œåæ ‡ä½ç½® [x, y, z]
            box_pos: ç®±å­ä¸–ç•Œåæ ‡ä½ç½® [x, y, z]
            
        Returns:
            symmetry_reward: å¯¹ç§°æ€§å¥–åŠ±ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºè¶Šå¯¹ç§°
        """
        # è®¡ç®—ç®±å­ä¸­å¿ƒä½œä¸ºå¯¹ç§°è½´
        box_center_x = box_pos[0]
        box_center_y = box_pos[1]
        box_center_z = box_pos[2]
        
        # 1. Yåæ ‡å¯¹ç§°æ€§ï¼ˆç›¸å¯¹äºç®±å­Yåæ ‡çš„é•œåƒå¯¹ç§°ï¼‰
        left_y_offset = left_pos[1] - box_center_y   # å·¦æ‰‹ç›¸å¯¹ç®±å­çš„Yåç§»
        right_y_offset = right_pos[1] - box_center_y  # å³æ‰‹ç›¸å¯¹ç®±å­çš„Yåç§»
        
        # ç†æƒ³æƒ…å†µä¸‹ï¼šleft_y_offset = -right_y_offset (é•œåƒå¯¹ç§°)
        y_symmetry_error = np.abs(left_y_offset + right_y_offset)
        y_symmetry_reward = np.exp(-5.0 * y_symmetry_error)  # æŒ‡æ•°è¡°å‡ï¼Œè¶Šå¯¹ç§°å¥–åŠ±è¶Šå¤§
        
        # 2. Xåæ ‡ä¸€è‡´æ€§ï¼ˆåŒæ‰‹åº”è¯¥åœ¨ç›¸ä¼¼çš„Xä½ç½®ï¼‰
        x_consistency_error = np.abs(left_pos[0] - right_pos[0])
        x_consistency_reward = np.exp(-3.0 * x_consistency_error)
        
        # 3. Zåæ ‡ä¸€è‡´æ€§ï¼ˆåŒæ‰‹é«˜åº¦åº”è¯¥ç›¸ä¼¼ï¼‰
        z_consistency_error = np.abs(left_pos[2] - right_pos[2])
        z_consistency_reward = np.exp(-5.0 * z_consistency_error)
        
        # 4. åŒæ‰‹åˆ°ç®±å­è·ç¦»çš„å‡è¡¡æ€§
        left_to_box_dist = np.linalg.norm(left_pos - box_pos)
        right_to_box_dist = np.linalg.norm(right_pos - box_pos)
        distance_balance_error = np.abs(left_to_box_dist - right_to_box_dist)
        distance_balance_reward = np.exp(-3.0 * distance_balance_error)
        
        # 5. åŒæ‰‹é—´è·çº¦æŸï¼ˆé˜²æ­¢åŒæ‰‹è¿‡è¿‘æˆ–è¿‡è¿œï¼‰
        hand_distance = np.linalg.norm(left_pos - right_pos)
        ideal_hand_distance = 0.4  # ç†æƒ³åŒæ‰‹é—´è·40cm
        hand_distance_error = np.abs(hand_distance - ideal_hand_distance)
        hand_distance_reward = np.exp(-2.0 * hand_distance_error)
        
        # ç»„åˆå¯¹ç§°æ€§å¥–åŠ±ï¼ˆåŠ æƒå¹³å‡ï¼‰
        symmetry_reward = (
            0.3 * y_symmetry_reward +      # Yè½´å¯¹ç§°æ€§æœ€é‡è¦
            0.2 * x_consistency_reward +    # Xåæ ‡ä¸€è‡´æ€§
            0.2 * z_consistency_reward +    # Zåæ ‡ä¸€è‡´æ€§  
            0.2 * distance_balance_reward +  # è·ç¦»å‡è¡¡æ€§
            0.1 * hand_distance_reward      # åŒæ‰‹é—´è·
        )
        
        # è½¬æ¢ä¸ºè´Ÿå¥–åŠ±å½¢å¼ï¼ˆä¸å…¶ä»–MSEå¥–åŠ±ä¿æŒä¸€è‡´ï¼‰
        symmetry_penalty = -(1.0 - symmetry_reward)  # å°†[0,1]è½¬æ¢ä¸º[-1,0]
        
        # Debugä¿¡æ¯
        if self.debug and self.episode_step_count % 20 == 0:
            print(f"[SYMMETRY DEBUG] Y-symmetry error: {y_symmetry_error:.4f}, reward: {y_symmetry_reward:.4f}")
            print(f"[SYMMETRY DEBUG] X-consistency error: {x_consistency_error:.4f}, reward: {x_consistency_reward:.4f}")
            print(f"[SYMMETRY DEBUG] Z-consistency error: {z_consistency_error:.4f}, reward: {z_consistency_reward:.4f}")
            print(f"[SYMMETRY DEBUG] Distance balance error: {distance_balance_error:.4f}, reward: {distance_balance_reward:.4f}")
            print(f"[SYMMETRY DEBUG] Hand distance: {hand_distance:.4f}m (ideal: {ideal_hand_distance:.4f}m)")
            print(f"[SYMMETRY DEBUG] Total symmetry reward: {symmetry_penalty:.4f}")
        
        return symmetry_penalty

    def _generate_random_initial_positions(self):
        """
        ç”Ÿæˆéšæœºçš„åˆå§‹æ‰‹è‡‚ä½ç½®
        x: (0.2, 0.5)
        å·¦æ‰‹y: (0.1, 0.4) - ç¡®ä¿å·¦æ‰‹åœ¨å³ä¾§ï¼ˆæ­£yæ–¹å‘ï¼‰
        å³æ‰‹y: (-0.4, -0.1) - ç¡®ä¿å³æ‰‹åœ¨å·¦ä¾§ï¼ˆè´Ÿyæ–¹å‘ï¼‰ 
        z: (-0.05, 0.3)
        å·¦å³æ‰‹ä½ç½®ç‹¬ç«‹éšæœºï¼Œä½†é¿å…äº¤å‰
        """
        print(" =========================== éšæœºä½ç½® ================================ ï¼ï¼")
        # å·¦æ‰‹éšæœºä½ç½® - yå€¼é™åˆ¶åœ¨æ­£æ•°èŒƒå›´ï¼Œé¿å…ä¸å³æ‰‹äº¤å‰
        left_x = np.random.uniform(0.2, 0.5)
        left_y = np.random.uniform(0.3, 0.6)  # å·¦æ‰‹ä¿æŒåœ¨æ­£yæ–¹å‘
        left_z = np.random.uniform(-0.05, 0.3)
        self.random_initial_left_pos = np.array([left_x, left_y, left_z], dtype=np.float32)
        
        # å³æ‰‹éšæœºä½ç½® - yå€¼é™åˆ¶åœ¨è´Ÿæ•°èŒƒå›´ï¼Œé¿å…ä¸å·¦æ‰‹äº¤å‰
        right_x = np.random.uniform(0.2, 0.5)
        right_y = np.random.uniform(-0.6, -0.3)  # å³æ‰‹ä¿æŒåœ¨è´Ÿyæ–¹å‘
        right_z = np.random.uniform(-0.05, 0.3)
        self.random_initial_right_pos = np.array([right_x, right_y, right_z], dtype=np.float32)
        
        # æ›´æ–°å½“å‰ä½ç½®ä¸ºéšæœºåˆå§‹ä½ç½®
        self.current_left_pos = self.random_initial_left_pos.copy()
        self.current_right_pos = self.random_initial_right_pos.copy()
        
        if self.debug:
            print(f"[RANDOM INITIAL] Generated random initial positions:")
            print(f"  Left hand: {self.random_initial_left_pos}")
            print(f"  Right hand: {self.random_initial_right_pos}")

    def _scale_action_to_eef_positions(self, ee_action: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        å°†å½’ä¸€åŒ–çš„action [-1, 1] scaleåˆ°æŒ‡å®šçš„end-effectorä½ç½®èŒƒå›´
        
        Args:
            ee_action: å½’ä¸€åŒ–çš„æœ«ç«¯æ‰§è¡Œå™¨åŠ¨ä½œ [left_x, left_y, left_z, right_x, right_y, right_z]
        
        Returns:
            left_pos, right_pos: ç¼©æ”¾åçš„ç»å¯¹ä¸–ç•Œåæ ‡ä½ç½®
        """
        if len(ee_action) < 6:
            rospy.logwarn(f"ee_action length {len(ee_action)} < 6, padding with zeros")
            padded_action = np.zeros(6)
            padded_action[:len(ee_action)] = ee_action
            ee_action = padded_action
        
        # æå–å·¦å³æ‰‹çš„åŠ¨ä½œ
        left_action = ee_action[0:3]  # [x, y, z]
        right_action = ee_action[3:6]  # [x, y, z]
        
        # Scaleå·¦æ‰‹ä½ç½®: action [-1,1] -> world coordinates
        left_x = (left_action[0] + 1) / 2 * (self.EEF_POS_LIMITS['x_max'] - self.EEF_POS_LIMITS['x_min']) + self.EEF_POS_LIMITS['x_min']
        left_y = (left_action[1] + 1) / 2 * (self.EEF_POS_LIMITS['left_y_max'] - self.EEF_POS_LIMITS['left_y_min']) + self.EEF_POS_LIMITS['left_y_min']
        left_z = (left_action[2] + 1) / 2 * (self.EEF_POS_LIMITS['z_max'] - self.EEF_POS_LIMITS['z_min']) + self.EEF_POS_LIMITS['z_min']
        
        # Scaleå³æ‰‹ä½ç½®: action [-1,1] -> world coordinates  
        right_x = (right_action[0] + 1) / 2 * (self.EEF_POS_LIMITS['x_max'] - self.EEF_POS_LIMITS['x_min']) + self.EEF_POS_LIMITS['x_min']
        right_y = (right_action[1] + 1) / 2 * (self.EEF_POS_LIMITS['right_y_max'] - self.EEF_POS_LIMITS['right_y_min']) + self.EEF_POS_LIMITS['right_y_min']
        right_z = (right_action[2] + 1) / 2 * (self.EEF_POS_LIMITS['z_max'] - self.EEF_POS_LIMITS['z_min']) + self.EEF_POS_LIMITS['z_min']
        
        left_pos = np.array([left_x, left_y, left_z], dtype=np.float32)
        right_pos = np.array([right_x, right_y, right_z], dtype=np.float32)
        
        if self.debug:
            print(f"[EEF SCALING] Action: {ee_action[:6]}")
            print(f"[EEF SCALING] Left pos: {left_pos}, Right pos: {right_pos}")
        
        return left_pos, right_pos

    def _ang_vel_callback(self, msg):
        with self.ang_vel_lock:
            self.latest_ang_vel = msg

    def _lin_accel_callback(self, msg):
        with self.lin_accel_lock:
            self.latest_lin_accel = msg

    def _wbc_callback(self, msg):
        with self.wbc_lock:
            self.latest_wbc = msg

    def _robot_pose_callback(self, msg):
        """Callback for robot pose messages when WBC observation is disabled."""
        with self.robot_pose_lock:
            self.latest_robot_pose = msg

    def _base_link_eef_left_callback(self, msg):
        """Callback for base_link left end-effector pose messages."""
        with self.base_link_eef_left_lock:
            self.latest_base_link_eef_left = msg

    def _base_link_eef_right_callback(self, msg):
        """Callback for base_link right end-effector pose messages."""
        with self.base_link_eef_right_lock:
            self.latest_base_link_eef_right = msg

    def _vr_cmd_vel_callback(self, msg):
        """Callback for VR-generated cmd_vel messages."""
        # Only process VR messages when in VR intervention mode
        if not self._is_vr_intervention_active:
            return
            
        with self._vr_action_lock:
            self._latest_vr_cmd_vel = msg

    def _vr_arm_traj_callback(self, msg):
        """Callback for VR-generated arm trajectory messages."""
        # Only process VR messages when in VR intervention mode
        if not self._is_vr_intervention_active:
            return
            
        with self._vr_action_lock:
            self._latest_vr_arm_traj = msg

    def set_vr_intervention_mode(self, active: bool):
        """
        Set whether VR intervention mode is active.
        
        Args:
            active: True to enable VR intervention mode, False to disable
        """
        self._is_vr_intervention_active = active
        self._should_publish_action = not active  # Don't publish when VR is controlling
        
        # Clear VR data when disabling intervention mode
        if not active:
            with self._vr_action_lock:
                self._latest_vr_cmd_vel = None
                self._latest_vr_arm_traj = None
                print("[VR DEBUG] VR intervention mode disabled, cleared VR data")

    def get_vr_action(self) -> np.ndarray:
        """
        Convert VR-generated ROS messages to environment action format.
        Now supports incremental position control.
        
        Returns:
            Action array matching the environment's action space (with increments)

            è·å–vrçš„å¦‚ä¸‹ä¿¡æ¯ã€‚
            è·å–/cmd_vel
            è·å–/mm_kuavo_arm_traj - ç°åœ¨è½¬æ¢ä¸ºå¢é‡æ§åˆ¶
            åœ¨RLKuavoMetaVRWrapperçš„stepå½“ä¸­, å°†è·å–åˆ°çš„å€¼æ˜ å°„åˆ°actionæ•°ç»„ä¸­,è¯¥æ•°æ®ç”¨äºæœ€ç»ˆçš„action_intervention recordå’Œbufferéƒ½ä¼šä½¿ç”¨è¿™ä¸ªkeyé‡Œé¢çš„action
        """
        
        with self._vr_action_lock:
            if not self._is_vr_intervention_active:
                return np.zeros(self.action_space.shape[0], dtype=np.float32)
            
            if self._latest_vr_cmd_vel is None and self._latest_vr_arm_traj is None:
                return np.zeros(self.action_space.shape[0], dtype=np.float32)
            
            # Create action array with correct dimensions
            action = np.zeros(self.action_space.shape[0], dtype=np.float32)
            
            # Process cmd_vel data if available
            if self._latest_vr_cmd_vel is not None:
                vel_cmd = self._latest_vr_cmd_vel
                vel_action = np.array([
                    vel_cmd.linear.x,
                    vel_cmd.linear.y, 
                    vel_cmd.linear.z,
                    vel_cmd.angular.z
                ], dtype=np.float32)
                
                # Scale the velocity commands
                if hasattr(self, 'vel_action_scale'):
                    vel_action = vel_action * self.vel_action_scale
                
                # Set velocity portion of action
                action[:4] = vel_action
            
            # Process arm trajectory data if available - Convert to increments
            if self._latest_vr_arm_traj is not None and len(self._latest_vr_arm_traj.position) >= self.arm_dim:
                if TEST_DEMO_USE_ACTION_16_DIM:
                    # Joint control mode: convert joint angles directly
                    arm_positions_deg = np.array(self._latest_vr_arm_traj.position[:self.arm_dim], dtype=np.float32)
                    arm_positions_rad = np.deg2rad(arm_positions_deg)
                    arm_action = (arm_positions_rad - self.arm_joint_centers) / self.arm_joint_scales
                    arm_action = np.clip(arm_action, -1.0, 1.0)
                    action[4:4+self.arm_dim] = arm_action
                    
                    if self.debug:
                        print(f"[VR JOINT CONTROL] VR joint input converted to action (mean: {np.mean(np.abs(arm_action)):.3f})")
                elif self.wbc_observation_enabled:
                    # WBC mode: convert joint angles to increments (legacy logic)
                    arm_positions_deg = np.array(self._latest_vr_arm_traj.position[:self.arm_dim], dtype=np.float32)
                    arm_positions_rad = np.deg2rad(arm_positions_deg)
                    arm_action = (arm_positions_rad - self.arm_joint_centers) / self.arm_joint_scales
                    arm_action = np.clip(arm_action, -1.0, 1.0)
                    action[4:4+self.arm_dim] = arm_action
                else:
                    # Position mode: convert to increments based on appropriate reference positions
                    left_increment = np.zeros(3, dtype=np.float32)
                    right_increment = np.zeros(3, dtype=np.float32)
                    
                    if len(self._latest_vr_arm_traj.position) >= 6:
                        # Assume VR sends [left_x, left_y, left_z, right_x, right_y, right_z]
                        vr_left_pos = np.array(self._latest_vr_arm_traj.position[0:3], dtype=np.float32)
                        vr_right_pos = np.array(self._latest_vr_arm_traj.position[3:6], dtype=np.float32)
                        
                        # Choose appropriate reference positions based on IF_USE_RANDOM_INITIAL_POSITION
                        if not IF_USE_RANDOM_INITIAL_POSITION:
                            # Normal mode: use fixed positions
                            left_increment = vr_left_pos - self.FIXED_LEFT_POS
                            right_increment = vr_right_pos - self.FIXED_RIGHT_POS
                        else:
                            # Random mode: use random initial positions as reference
                            left_increment = vr_left_pos - self.random_initial_left_pos
                            right_increment = vr_right_pos - self.random_initial_right_pos
                        
                        # Limit increments to normal ranges
                        left_increment = np.clip(left_increment, -self.MAX_INCREMENT_RANGE, self.MAX_INCREMENT_RANGE)
                        right_increment = np.clip(right_increment, -self.MAX_INCREMENT_RANGE, self.MAX_INCREMENT_RANGE)
                        
                        # Only debug non-zero increments to reduce spam
                        if self.debug and (np.linalg.norm(left_increment) > 0.001 or np.linalg.norm(right_increment) > 0.001):
                            print(f"[VR POSITION] Non-zero increments - Left: {np.linalg.norm(left_increment):.3f}m, Right: {np.linalg.norm(right_increment):.3f}m")
                    
                    action[4:7] = left_increment
                    action[7:10] = right_increment
            
            return action



    def _setup_ros_communication(self):
        """
        Implement this method to set up ROS publishers, subscribers,
        and service clients specific to the Kuavo robot.
        """
        global IF_USE_ARM_MPC_CONTROL
        # Publishers
        self.cmd_vel_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=1)
        
        # Replace the arm_traj_pub with the new publisher
        if IF_USE_ARM_MPC_CONTROL:
            self.ee_pose_pub = rospy.Publisher('/mm/two_arm_hand_pose_cmd', twoArmHandPoseCmd, queue_size=10)
        else:
            self.ee_pose_pub = rospy.Publisher('/ik/two_arm_hand_pose_cmd', twoArmHandPoseCmd, queue_size=10)
        
        # kuavo_arm_traj pub
        self.robot_arm_traj_pub = rospy.Publisher('/kuavo_arm_traj', JointState, queue_size=10)

        # Service Client
        self.reset_client = rospy.ServiceProxy('/isaac_lab_reset_scene', resetIsaaclab)

        # # Subscribers for headerless topics that are not synchronized
        # if self.wbc_observation_enabled:
        #     rospy.Subscriber('/state_estimate/imu_data_filtered/angularVel', Float64MultiArray, self._ang_vel_callback)
        #     rospy.Subscriber('/state_estimate/imu_data_filtered/linearAccel', Float64MultiArray, self._lin_accel_callback)
        
        # # Conditionally subscribe to WBC or robot pose based on flag
        # if self.wbc_observation_enabled:
        #     rospy.Subscriber('/humanoid_wbc_observation', mpc_observation, self._wbc_callback)
        #     if self.debug:
        #         rospy.loginfo("WBC observation enabled - subscribing to /humanoid_wbc_observation")
        
        if not self.wbc_observation_enabled:
            rospy.Subscriber('/robot_pose', PoseStamped, self._robot_pose_callback)
            # Subscribe to base_link end-effector poses when WBC is disabled
            rospy.Subscriber('/fk/base_link_eef_left', PoseStamped, self._base_link_eef_left_callback)
            rospy.Subscriber('/fk/base_link_eef_right', PoseStamped, self._base_link_eef_right_callback)
            if self.debug:
                rospy.loginfo("WBC observation disabled - subscribing to /robot_pose, /fk/base_link_eef_left, /fk/base_link_eef_right")

        # Subscribers for VR intervention commands - listen to VR-generated control commands
        rospy.Subscriber('/cmd_vel', Twist, self._vr_cmd_vel_callback)
        rospy.Subscriber('/mm_kuavo_arm_traj', JointState, self._vr_arm_traj_callback)

        # Synchronized subscribers
        eef_left_sub = message_filters.Subscriber('/fk/eef_pose_left', PoseStamped)
        eef_right_sub = message_filters.Subscriber('/fk/eef_pose_right', PoseStamped)
        # image_sub = message_filters.Subscriber('/camera/eval/image_raw', Image)
        image_sub = message_filters.Subscriber('/camera/rgb/image_raw', Image)
        sensors_sub = message_filters.Subscriber('/sensors_data_raw', sensorsData)
        box_real_sub = message_filters.Subscriber('/box_real_pose', PoseStamped)

        self.ts = message_filters.ApproximateTimeSynchronizer(
            [eef_left_sub, eef_right_sub, image_sub, sensors_sub, box_real_sub],
            queue_size=10,
            slop=0.1,  # Increased slop for the large number of topics
            allow_headerless=True,  # Allow synchronizing messages without a header
        )
        self.ts.registerCallback(self._obs_callback)

    def pub_control_robot_arm_traj(self, joint_q: list)->bool:
        try:
            msg = JointState()
            msg.name = ['zarm_l1_joint', 'zarm_l2_joint', 'zarm_l3_joint', 'zarm_l4_joint', 'zarm_l5_joint', 'zarm_l6_joint', 'zarm_l7_joint',
                        'zarm_r1_joint', 'zarm_r2_joint', 'zarm_r3_joint', 'zarm_r4_joint', 'zarm_r5_joint', 'zarm_r6_joint', 'zarm_r7_joint']
            msg.header.stamp = rospy.Time.now()
            msg.position = (180.0 / np.pi * np.array(joint_q)).tolist()
            print(f"publish robot arm traj: {msg.position}")
            self.robot_arm_traj_pub.publish(msg)
            return True
        except Exception as e:
            print(f"publish robot arm traj: {e}")
        return False

    def pub_control_robot_arm_traj_deg(self,joint_q: list)->bool:
        try:
            msg = JointState()
            msg.name = ['zarm_l1_joint', 'zarm_l2_joint', 'zarm_l3_joint', 'zarm_l4_joint', 'zarm_l5_joint', 'zarm_l6_joint', 'zarm_l7_joint',
                        'zarm_r1_joint', 'zarm_r2_joint', 'zarm_r3_joint', 'zarm_r4_joint', 'zarm_r5_joint', 'zarm_r6_joint', 'zarm_r7_joint']
            msg.header.stamp = rospy.Time.now()
            msg.position = np.array(joint_q).tolist()
            self.robot_arm_traj_pub.publish(msg)
            return True
        except Exception as e:
            print(f"publish robot arm traj: {e}")
        return False

    def _obs_callback(self, left_eef, right_eef, image, sensors, box_real):
        """Synchronously handles incoming observation messages and populates the observation buffer."""
        # Retrieve the latest data from headerless topics
        if self.wbc_observation_enabled:
            with self.ang_vel_lock:
                ang_vel = self.latest_ang_vel
            with self.lin_accel_lock:
                lin_accel = self.latest_lin_accel
        else:
            # When WBC is disabled, use dummy IMU data
            ang_vel = type('dummy', (), {'data': [0.0, 0.0, 0.0]})()
            lin_accel = type('dummy', (), {'data': [0.0, 0.0, 0.0]})()
            
        # Get WBC or robot pose data based on mode
        if self.wbc_observation_enabled:
            with self.wbc_lock:
                wbc = self.latest_wbc
            robot_pose = None
            base_link_eef_left = None
            base_link_eef_right = None
        else:
            with self.robot_pose_lock:
                robot_pose = self.latest_robot_pose
            # Get base_link end-effector poses when WBC is disabled
            with self.base_link_eef_left_lock:
                base_link_eef_left = self.latest_base_link_eef_left
            with self.base_link_eef_right_lock:
                base_link_eef_right = self.latest_base_link_eef_right
            wbc = None

        # Wait until all data sources are available (only check IMU data if WBC is enabled)
        if self.wbc_observation_enabled and (ang_vel is None or lin_accel is None):
            if self.debug:
                rospy.logwarn_throttle(1.0, "IMU data not yet available for observation callback.")
            return
            
        if self.wbc_observation_enabled and wbc is None:
            if self.debug:
                rospy.logwarn_throttle(1.0, "WBC data not yet available for observation callback.")
            return
        elif not self.wbc_observation_enabled and (robot_pose is None or base_link_eef_left is None or base_link_eef_right is None):
            if self.debug:
                rospy.logwarn_throttle(1.0, "Robot pose or base_link end-effector data not yet available for observation callback.")
            return

        with self.obs_lock:
            try:
                # Process image data using custom function instead of cv_bridge
                cv_image = ros_image_to_cv2(image)
                
                # Resize image to target size
                cv_image = cv2.resize(cv_image, self.image_size)
                
                # Convert BGR to RGB if necessary
                if image.encoding == 'bgr8':
                    rgb_image = cv_image[:, :, ::-1].copy()  # BGR to RGB
                elif image.encoding == 'rgb8':
                    rgb_image = cv_image.copy()
                else:
                    # For other encodings, assume RGB or convert to RGB
                    if len(cv_image.shape) == 2:  # Grayscale
                        rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_GRAY2RGB)
                    else:
                        rgb_image = cv_image.copy()

                # Process state data
                left_eef_position = np.array([left_eef.pose.position.x, left_eef.pose.position.y, left_eef.pose.position.z])
                left_eef_orientation = np.array([left_eef.pose.orientation.x, left_eef.pose.orientation.y, left_eef.pose.orientation.z, left_eef.pose.orientation.w])
                left_eef_data = np.concatenate([left_eef_position, left_eef_orientation])
                
                right_eef_position = np.array([right_eef.pose.position.x, right_eef.pose.position.y, right_eef.pose.position.z])
                right_eef_orientation = np.array([right_eef.pose.orientation.x, right_eef.pose.orientation.y, right_eef.pose.orientation.z, right_eef.pose.orientation.w])
                right_eef_data = np.concatenate([right_eef_position, right_eef_orientation])

                # arm_data
                arm_data = np.array(sensors.joint_data.joint_q[12:26])
                
                # imu data 
                ang_vel_data = np.array(ang_vel.data[:3])
                lin_accel_data = np.array(lin_accel.data[:3])
                
                # Process WBC or robot pose data based on mode
                if self.wbc_observation_enabled:
                    wbc_data = np.array(wbc.state.value[:12])
                else:
                    # Convert robot pose to 12-dimensional data similar to WBC
                    # Extract position and orientation components
                    robot_pos = np.array([robot_pose.pose.position.x, robot_pose.pose.position.y, robot_pose.pose.position.z])
                    robot_orn = np.array([robot_pose.pose.orientation.x, robot_pose.pose.orientation.y, 
                                        robot_pose.pose.orientation.z, robot_pose.pose.orientation.w])
                    # Pad with zeros to match WBC data dimension (7)
                    wbc_data = np.concatenate([robot_pos, robot_orn])  # 3 + 4 = 7
                    
                    # Extract base_link end-effector positions
                    base_link_left_eef_pos = np.array([
                        base_link_eef_left.pose.position.x,
                        base_link_eef_left.pose.position.y,
                        base_link_eef_left.pose.position.z
                    ])
                    base_link_right_eef_pos = np.array([
                        base_link_eef_right.pose.position.x,
                        base_link_eef_right.pose.position.y,
                        base_link_eef_right.pose.position.z
                    ])
                
                box_pos_data = np.array([
                    box_real.pose.position.x, box_real.pose.position.y, box_real.pose.position.z
                ])
                box_orn_data = np.array([
                    box_real.pose.orientation.x, box_real.pose.orientation.y,
                    box_real.pose.orientation.z, box_real.pose.orientation.w
                ])


                if self.wbc_observation_enabled:
                    """
                        46 ç»´åº¦ - agent_pos
                        7 + 7
                        14
                        3 + 3
                        12
                    """
                    agent_pos_obs = np.concatenate([
                        left_eef_data, right_eef_data, 
                        arm_data, 
                        ang_vel_data, lin_accel_data, 
                        wbc_data
                    ]).astype(np.float32)
                else:
                    """
                        29 ç»´åº¦ - agent_pos (increased from 23)
                        3 + 3 (world frame eef positions)
                        14 (arm joints)
                        3 (robot position)
                        3 + 3 (base_link frame eef positions)
                    """
                    agent_pos_obs = np.concatenate([
                        left_eef_position, right_eef_position, 
                        arm_data, 
                        robot_pos,
                        base_link_left_eef_pos, base_link_right_eef_pos,
                    ]).astype(np.float32)

                if self.wbc_observation_enabled:
                    """
                        7 ç»´åº¦ - environment_state
                    """
                    env_state_obs = np.concatenate([
                        box_pos_data, box_orn_data
                    ]).astype(np.float32)
                else:
                    """
                        3 ç»´åº¦ - environment_state
                    """
                    env_state_obs = box_pos_data.astype(np.float32)

                self.latest_obs = {
                    "pixels": {"front": rgb_image},
                    "agent_pos": agent_pos_obs,
                    "environment_state": env_state_obs,
                }
                self.new_obs_event.set()

            except Exception as e:
                rospy.logerr(f"Error in observation callback: {e}")

    def _send_action(self, action: np.ndarray, episode_step_count: int):
        """
        Implement this method to publish an action to the Kuavo robot.
        
        Args:
            action: The action array to send
        """
        global TEST_DEMO_USE_ACTION_16_DIM
        if not self._should_publish_action:
            # During VR intervention, don't publish actions - VR system handles control
            return
        
        # ğŸ”¥ å¤„ç†åºåˆ—ACT Actorè¾“å‡ºçš„å¤šç»´åŠ¨ä½œ
        if action.ndim > 1:
            rospy.logwarn(f"Received multi-dimensional action with shape {action.shape}, taking first action")
            action = action[0]  # å–ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼ˆå¦‚æœæ˜¯åºåˆ—ï¼‰
            
        # ç¡®ä¿æ˜¯1Dæ•°ç»„
        action = np.asarray(action).flatten()
        
        # De-normalize and publish cmd_vel
        if TEST_DEMO_USE_ACTION_16_DIM:
            """
                2 + 7 + 7 = 16
            """
            twist_cmd = Twist()
            vel_action = action[:self.vel_dim] * self.vel_action_scale
            twist_cmd.linear.x = vel_action[0]
            twist_cmd.linear.y = 0.0 
            twist_cmd.linear.z = 0.0 
            twist_cmd.angular.x = 0.0
            twist_cmd.angular.y = 0.0
            twist_cmd.angular.z = vel_action[1]
            ee_action = action[self.vel_dim:]
            # print(" ============================================================ ")
            # print(" ==================== step begin ============================")
            # print( " ==============  send_action | action: ", action)
            # print(" ================ send_action | len action: ", len(action))
            # print(" ================ send_action | vel_action: ", vel_action)
            # print(" ================ send_action | ee_action: ", ee_action)
            # vel pub
            self.cmd_vel_pub.publish(twist_cmd) 
            # joint pub
            self._publish_joint_control_arm_poses(ee_action)
        else:
            """
                2 + 3 + 3 = 8 (å¢é‡æ§åˆ¶æ¨¡å¼)
                action[0:2]: åº•ç›˜é€Ÿåº¦æ§åˆ¶ (x, yaw)
                action[2:5]: å·¦æ‰‹ä½ç½®å¢é‡ (x, y, z) - æ¯æ­¥Â±0.01m
                action[5:8]: å³æ‰‹ä½ç½®å¢é‡ (x, y, z) - æ¯æ­¥Â±0.01m
            """
            if USE_CMD_VEL:
                twist_cmd = Twist()
                vel_action = action[:self.vel_dim] * self.vel_action_scale
                twist_cmd.linear.x = vel_action[0]
                twist_cmd.linear.y = 0.0 
                twist_cmd.linear.z = 0.0 
                twist_cmd.angular.x = 0.0
                twist_cmd.angular.y = 0.0
                twist_cmd.angular.z = vel_action[1]
                ee_action = action[self.vel_dim:]
                # print(" ============================================================ ")
                # print(" ==================== vel eef step begin ============================")
                # print( " ==============  send_action | action: ", action)
                # print(" ================ send_action | len action: ", len(action))
                # print(" ================ send_action | vel_action: ", vel_action)
                # print(" ================ send_action | ee_action (increments): ", ee_action)
                # vel pub
                self.cmd_vel_pub.publish(twist_cmd) 
                # eef pub (å¢é‡æ§åˆ¶)
                self._publish_action_based_arm_poses(ee_action)
            else:
                ee_action = action
                # print(" ============================================================ ")
                # print(" ==================== only eef step begin ============================")
                # print( " ==============  send_action | action: ", action)
                # print(" ================ send_action | len action: ", len(action))
                # print(" ================ send_action | ee_action (increments): ", ee_action)
                # eef pub (å¢é‡æ§åˆ¶)
                self._publish_action_based_arm_poses(ee_action)

    def _publish_fixed_arm_poses(self):
        """
        Publish fixed arm poses for the approach stage.
        Also updates the current position state for incremental control.
        """
        global DEMO_LEFT_ELBOW_POS
        global DEMO_RIGHT_ELBOW_POS

        if IF_USE_ARM_MPC_CONTROL:
            self.change_mobile_ctrl_mode(IncrementalMpcCtrlMode.ArmOnly.value)
            print( "=============== change_mobile_ctrl_mode to ArmOnly ================")
        
        # ä½¿ç”¨åˆå§‹ä½ç½®ä½œä¸ºå›ºå®šå§¿æ€
        left_quat = np.array([0.0, -0.70711, 0.0, 0.70711])
        right_quat = np.array([0.0, -0.70711, 0.0, 0.70711])
        if not IF_USE_RANDOM_INITIAL_POSITION:
            left_pos = np.array([0.3178026345146559, 0.4004180715613648, -0.019417275957965042])
            right_pos = np.array([0.3178026345146559, -0.4004180715613648, -0.019417275957965042])
        else:
            left_pos = self.random_initial_left_pos.copy()
            right_pos = self.random_initial_right_pos.copy()

        # æ›´æ–°å½“å‰ä½ç½®çŠ¶æ€ä»¥åŒ¹é…å‘å¸ƒçš„ä½ç½®
        self.current_left_pos = left_pos.copy()
        self.current_right_pos = right_pos.copy()
        
        left_elbow_pos = DEMO_LEFT_ELBOW_POS
        right_elbow_pos = DEMO_RIGHT_ELBOW_POS

        msg = twoArmHandPoseCmd()
        msg.hand_poses.left_pose.pos_xyz = left_pos.tolist()
        msg.hand_poses.left_pose.quat_xyzw = left_quat.tolist()
        msg.hand_poses.left_pose.elbow_pos_xyz = left_elbow_pos # left_elbow_pos.tolist()

        msg.hand_poses.right_pose.pos_xyz = right_pos.tolist()
        msg.hand_poses.right_pose.quat_xyzw = right_quat.tolist()
        msg.hand_poses.right_pose.elbow_pos_xyz = right_elbow_pos # right_elbow_pos.tolist()
        
        # Set default IK params
        msg.use_custom_ik_param = False
        if not IF_USE_ARM_MPC_CONTROL:
            msg.joint_angles_as_q0 = True
        else:
            msg.joint_angles_as_q0 = False
        
        msg.ik_param = ikSolveParam()
        msg.frame = 3  # VR Frame
        self.ee_pose_pub.publish(msg)
        
        if self.debug:
            print(f"[FIXED POSES] Set initial positions - Left: {left_pos}, Right: {right_pos}")

    def _publish_action_based_arm_poses(self, ee_action: np.ndarray):
        """
        Publish arm poses based on the action input using incremental control.
        
        Action mapping:
        - action[0:3]: left hand x,y,z position increments (scaled from [-1,1] to Â±0.01m)
        - action[3:6]: right hand x,y,z position increments (scaled from [-1,1] to Â±0.01m)
        
        Args:
            ee_action: The arm portion of the action array (normalized [-1,1])
        """
        global DEMO_LEFT_ELBOW_POS
        global DEMO_RIGHT_ELBOW_POS
        
        if IF_USE_ARM_MPC_CONTROL:
            self.change_mobile_ctrl_mode(IncrementalMpcCtrlMode.ArmOnly.value)
            print( "=============== change_mobile_ctrl_mode to ArmOnly ================")
        
        # ğŸ”¥ å¤„ç†åºåˆ—ACT Actorè¾“å‡ºçš„å¤šç»´åŠ¨ä½œ
        if ee_action.ndim > 1:
            rospy.logwarn(f"Received multi-dimensional action with shape {ee_action.shape}, taking first action")
            ee_action = ee_action[0]  # å–ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼ˆå¦‚æœæ˜¯åºåˆ—ï¼‰
            
        # ç¡®ä¿æ˜¯1Dæ•°ç»„
        ee_action = np.asarray(ee_action).flatten()
        
        # ç¡®ä¿actioné•¿åº¦è¶³å¤Ÿ
        if len(ee_action) < 6:
            rospy.logwarn(f"ee_action length {len(ee_action)} < 6, padding with zeros")
            padded_action = np.zeros(6)
            padded_action[:len(ee_action)] = ee_action
            ee_action = padded_action
        
        # æå–å·¦å³æ‰‹çš„å¢é‡action
        left_increment_action = ee_action[0:3]  # [x, y, z] å¢é‡
        right_increment_action = ee_action[3:6]  # [x, y, z] å¢é‡
        
        # å°†action[-1,1]ç¼©æ”¾åˆ°Â±0.01mçš„å¢é‡èŒƒå›´
        left_increment = left_increment_action * self.INCREMENT_SCALE
        right_increment = right_increment_action * self.INCREMENT_SCALE
        
        # æ›´æ–°å½“å‰ä½ç½®ï¼ˆåŸºäºå¢é‡ï¼‰
        self.current_left_pos += left_increment
        self.current_right_pos += right_increment
        
        # ä¿æŒå›ºå®šçš„å§¿æ€
        left_quat = self.FIXED_LEFT_QUAT.copy()
        right_quat = self.FIXED_RIGHT_QUAT.copy()
        left_elbow_pos = DEMO_LEFT_ELBOW_POS
        right_elbow_pos = DEMO_RIGHT_ELBOW_POS

        msg = twoArmHandPoseCmd()
        msg.hand_poses.left_pose.pos_xyz = self.current_left_pos.tolist()
        msg.hand_poses.left_pose.quat_xyzw = left_quat.tolist()
        msg.hand_poses.left_pose.elbow_pos_xyz = left_elbow_pos.tolist()

        msg.hand_poses.right_pose.pos_xyz = self.current_right_pos.tolist()
        msg.hand_poses.right_pose.quat_xyzw = right_quat.tolist()
        msg.hand_poses.right_pose.elbow_pos_xyz = right_elbow_pos.tolist()
        
        # Set default IK params (can be customized as needed)
        msg.use_custom_ik_param = False
        msg.joint_angles_as_q0 = False
        # if not IF_USE_ARM_MPC_CONTROL:
        #     msg.joint_angles_as_q0 = True
        # else:
        #     msg.joint_angles_as_q0 = False
        
        msg.ik_param = ikSolveParam()
        msg.frame = 3  # keep current frame3 | 3 ä¸ºvrç³»
        self.ee_pose_pub.publish(msg)
        
        if self.debug:
            print(f"[INCREMENT CONTROL] Left increment: {left_increment}, New pos: {self.current_left_pos}")
            print(f"[INCREMENT CONTROL] Right increment: {right_increment}, New pos: {self.current_right_pos}")

    def _publish_joint_control_arm_poses(self, joint_action: np.ndarray):
        """
        Publish arm poses using joint control mode.
        
        Args:
            joint_action: Normalized joint action array [-1, 1] with 14 values 
                         (7 for left arm, 7 for right arm)
        """
        # Extract left and right arm actions (normalized [-1, 1])
        left_arm_action = joint_action[0:7]  # First 7 joints for left arm
        right_arm_action = joint_action[7:14]  # Next 7 joints for right arm
        
        # FIXME:æ‰“å°
        print(f"publish two joint control: {left_arm_action} {right_arm_action}")

        # # Convert normalized actions to actual joint angles using centers and scales
        # left_arm_centers = self.arm_joint_centers[0:7]
        # left_arm_scales = self.arm_joint_scales[0:7]
        # right_arm_centers = self.arm_joint_centers[7:14]
        # right_arm_scales = self.arm_joint_scales[7:14]
        
        # Calculate target joint angles: center + (action * scale)
        left_joint_angles_rad = left_arm_action
        right_joint_angles_rad = right_arm_action
        #left_joint_angles_rad = left_arm_centers + (left_arm_action * left_arm_scales)
        #right_joint_angles_rad = right_arm_centers + (right_arm_action * right_arm_scales)

        # Combine all joint angles in the correct order
        all_joint_angles_rad = np.concatenate([left_joint_angles_rad, right_joint_angles_rad])
        
        # Publish joint angles using existing method
        success = self.pub_control_robot_arm_traj(all_joint_angles_rad.tolist())
        
    def _reset_simulation(self):
        """
        Implement this method to call the reset service for the Kuavo simulation.
        """
        try:
            # call æœåŠ¡
            rospy.wait_for_service('/isaac_lab_reset_scene', timeout=5.0)
            resp = self.reset_client(0) # 0 for random seed in sim | åœ¨è¿™é‡Œç­‰å¾…æœåŠ¡ç«¯å¤„ç†å®Œæˆå¹¶ä¸”è¿”å›ç»“æœ
            if not resp.success:
                raise RuntimeError(f"Failed to reset simulation: {resp.message}")
            if self.debug:
                rospy.loginfo("Simulation reset successfully via ROS service.")

            # ç­‰å¾…3ç§’ è®©æ‰‹è‡‚è‡ªç„¶å½’ä½
            time.sleep(3)

            # ä½¿ç”¨eef poseæ§åˆ¶æ—¶æ¯æ¬¡å›åˆ°å›ºå®šä½ç½®
            if not TEST_DEMO_USE_ACTION_16_DIM:
                self._publish_fixed_arm_poses()

            time.sleep(1)
            
        except (rospy.ServiceException, rospy.ROSException) as e:
            raise RuntimeError(f"Service call to reset simulation failed: {str(e)}")

    def _compute_reward_and_done(self, obs: Dict[str, np.ndarray], action: np.ndarray) -> Tuple[float, bool, Dict[str, Any]]:
        """
        åˆ†é˜¶æ®µå¥–åŠ±å‡½æ•°ï¼ˆä¿®å¤ç´¯ç§¯å¥–åŠ±é—®é¢˜ç‰ˆï¼‰ï¼š
        - é˜¶æ®µ1 (dist_torso_to_box > 0.5): é è¿‘ç®±å­é˜¶æ®µ
        - é˜¶æ®µ2 (dist_torso_to_box <= 0.5): æŠ“å–ç®±å­é˜¶æ®µ
        - ä¿®å¤äº†å¥–åŠ±ç´¯ç§¯å’Œç»ˆç«¯æ¡ä»¶é—®é¢˜
        """
        # ========== DEMO MODE: CHOOSE CONTROL MODE ==========
        info = {}
        
        # Extract data from observation
        agent_state = obs['agent_pos']
        env_state = obs['environment_state']
        
        global TEST_DEMO_USE_ACTION_16_DIM
        global LEARN_TARGET_EEF_POSE_TARGET
        
        global DEMO_TARGET_LEFT_POS_WORLD
        global DEMO_TARGET_RIGHT_POS_WORLD
        global DEMO_TARGET_BOX_POS_WORLD

        if not LEARN_TARGET_EEF_POSE_TARGET:
            """
                å­¦ä¹ æ§åˆ¶eef pose or joint | ä½¿ç”¨å…³èŠ‚è§’åº¦ä½œä¸ºå­¦ä¹ ç›®æ ‡
            """
            # ========== JOINT CONTROL MODE ==========
            if self.episode_step_count >= 200:
                # Timeout - terminate but no success
                terminated = True
                info["success"] = False
            else:
                terminated = False
                info["success"] = False
            
            # FIXME:=========== é‡æ–°è®¾è®¡çš„reward - åŸºäºç›®æ ‡å…³èŠ‚è§’åº¦ ====================
            reward = 0.0
            
            current_joint_angles = agent_state[6:20]  # arm_dataä½ç½®
            
            # ç›®æ ‡å…³èŠ‚è§’åº¦ (å·¦æ‰‹7ä¸ª + å³æ‰‹7ä¸ª = 14ä¸ª)
            target_joint_angles = DEMO_TARGET_ALL_JOINT_ANGLES
            
            # è®¡ç®—å…³èŠ‚è§’åº¦å·®å¼‚çš„MSE
            joint_angle_diff = current_joint_angles - target_joint_angles
            mse_joint_angles = np.mean(joint_angle_diff ** 2) # reward = -np.mean((action - target_action) ** 2)
            
            # é€‰æ‹©å¥–åŠ±å‡½æ•°ç±»å‹ (å¯ä»¥åˆ‡æ¢ä¸åŒçš„å®ç°)
            reward_type = "MSE_joint_angles"  # æ–°çš„åŸºäºå…³èŠ‚è§’åº¦çš„MSEå¥–åŠ±
            
            if reward_type == "MSE_joint_angles":
                # åŸºäºå…³èŠ‚è§’åº¦MSEçš„å¥–åŠ±å‡½æ•°
                # ä½¿ç”¨è´Ÿçš„MSEï¼Œè®©agentæœ€å°åŒ–ä¸ç›®æ ‡çš„å·®å¼‚
                reward = -mse_joint_angles
                
                # å¯é€‰ï¼šæ·»åŠ ä¸€ä¸ªscale factorè®©å¥–åŠ±èŒƒå›´æ›´åˆç†
                reward_scale = 1.0  # å¯ä»¥è°ƒæ•´è¿™ä¸ªå€¼
                reward *= reward_scale
                
                if self.debug and self.episode_step_count % 10 == 0:  # æ¯10æ­¥æ‰“å°ä¸€æ¬¡
                    print(f"[REWARD DEBUG] MSE joint angles: {mse_joint_angles:.6f}, Reward: {reward:.6f}")
                    print(f"[REWARD DEBUG] Max joint diff: {np.max(np.abs(joint_angle_diff)):.4f} rad ({np.rad2deg(np.max(np.abs(joint_angle_diff))):.2f} deg)")
            elif reward_type == "original":
                # åŸå§‹çš„æŒ‡æ•°è¡°å‡å‡½æ•° (ä¿ç•™ä½œä¸ºå¤‡é€‰)
                target_action = np.ones_like(self.last_action) * 0.0
                action_distance = np.linalg.norm(action - target_action)
                reward = np.exp(-action_distance)
            elif reward_type == "shaped":
                # æ›´å¹³ç¼“çš„å¥–åŠ±å‡½æ•°ï¼Œæä¾›æ›´å¥½çš„æ¢¯åº¦ä¿¡å·
                target_action = np.ones_like(self.last_action) * 0.0
                action_distance = np.linalg.norm(action - target_action)
                reward = 1.0 / (1.0 + action_distance)
        else:
            """
                å­¦ä¹ æ§åˆ¶eef pose or joint | ä½¿ç”¨ç›®æ ‡eef poseä½œä¸ºå­¦ä¹ ç›®æ ‡
            """
            # print(" === use MSE reward function === ")
            # ========== EEF POSITION CONTROL MODE ==========
            if self.episode_step_count >= 200:
                # Timeout - terminate but no success
                terminated = True
                info["success"] = False
            else:
                terminated = False
                info["success"] = False
            
            # FIXME:=========== é‡æ–°è®¾è®¡çš„reward - åŸºäºç›®æ ‡æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® ====================
            """
                L_reward = w_hand * Mse_hand + w_box * Mse_box + alpha * box_down_fail
                å…¶ä¸­ w_box > w_hand è®©ç®±å­ç§»åŠ¨å¥–åŠ±æ›´å¤§
            """
            mse_eef = 0.0
            mse_box = 0.0
            box_down_fail = 0.0
                        
            # box lift target MSE - æ ¹æ®WBCæ¨¡å¼é€‰æ‹©æ­£ç¡®çš„æ•°æ®æº
            if self.wbc_observation_enabled:
                # WBCæ¨¡å¼ï¼šboxä½ç½®åœ¨agent_state[29:32]
                current_box_pos_world = agent_state[29:32]
            else:
                # éWBCæ¨¡å¼ï¼šboxä½ç½®åœ¨environment_state[0:3]
                current_box_pos_world = env_state[0:3]
            
            current_box_pos_z = current_box_pos_world[2]
            target_box_pos_world = DEMO_TARGET_BOX_POS_WORLD
            mse_box = np.mean((current_box_pos_world - target_box_pos_world) ** 2)
            mse_box = -mse_box  # è´ŸMSEï¼Œè¶Šå°è¶Šå¥½
            
            # è®¡ç®—å·¦å³æ‰‹ä½ç½®å·®å¼‚çš„MSE
            current_left_eef_pos_world = agent_state[0:3]   # worldåæ ‡ç³»å·¦æ‰‹ä½ç½®
            current_right_eef_pos_world = agent_state[3:6]  # worldåæ ‡ç³»å³æ‰‹ä½ç½®
            target_left_eef_pos_world = DEMO_TARGET_LEFT_POS_WORLD
            target_right_eef_pos_world = DEMO_TARGET_RIGHT_POS_WORLD
            mse_left_eef = np.mean((current_left_eef_pos_world - target_left_eef_pos_world) ** 2)
            mse_right_eef = np.mean((current_right_eef_pos_world - target_right_eef_pos_world) ** 2)
            mse_eef = -(mse_left_eef + mse_right_eef)  # è´ŸMSEï¼Œè¶Šå°è¶Šå¥½
            
            # ========== æ‰‹è‡‚å¯¹ç§°æ€§è½¯çº¦æŸ ==========
            current_left_eef_pos_world = agent_state[0:3]   # worldåæ ‡ç³»å·¦æ‰‹ä½ç½®
            current_right_eef_pos_world = agent_state[3:6]  # worldåæ ‡ç³»å³æ‰‹ä½ç½®
            
            # è®¡ç®—å¯¹ç§°æ€§å¥–åŠ±
            symmetry_reward = self._compute_symmetry_constraint(
                current_left_eef_pos_world, 
                current_right_eef_pos_world, 
                current_box_pos_world
            )
            
            # åº”ç”¨æƒé‡ï¼šè®©ç®±å­ç§»åŠ¨å¥–åŠ±æ›´å¤§
            weighted_hand_reward = self.hand_reward_weight * mse_eef
            weighted_box_reward = self.box_reward_weight * mse_box
            weighted_symmetry_reward = 0.5 * symmetry_reward  # å¯¹ç§°æ€§æƒé‡ï¼Œå¯è°ƒèŠ‚
            
            # ä½¿ç”¨åŠ æƒçš„MSEï¼Œè®©agentä¼˜å…ˆè€ƒè™‘ç®±å­ç§»åŠ¨
            reward = weighted_hand_reward + weighted_box_reward + weighted_symmetry_reward + box_down_fail
            
            # Debugä¿¡æ¯ - æ˜¾ç¤ºæƒé‡åº”ç”¨æ•ˆæœ
            if self.debug and self.episode_step_count % 20 == 0:  # æ¯20æ­¥æ‰“å°ä¸€æ¬¡
                print(f"[REWARD DEBUG] Hand MSE: {-mse_eef:.6f}, Box MSE: {-mse_box:.6f}")
                print(f"[REWARD DEBUG] Weighted Hand: {weighted_hand_reward:.6f}, Weighted Box: {weighted_box_reward:.6f}")
                print(f"[REWARD DEBUG] Weighted Symmetry: {weighted_symmetry_reward:.6f}")
                print(f"[REWARD DEBUG] Hand weight: {self.hand_reward_weight}, Box weight: {self.box_reward_weight}")
                print(f"[REWARD DEBUG] Total reward (before scale): {reward:.6f}")
            
            # success condition - æˆåŠŸåˆ¤æ–­
            if current_box_pos_z > target_box_pos_world[2]:
                terminated = True
                info["success"] = True
                reward = reward + 3.0
                print(f"[SUCCESS CONDITION] Box lifted successfully! Current z: {current_box_pos_z:.3f}, Target z: {target_box_pos_world[2]:.3f}")

            # box fail - ç®±å­æ‰äº†
            if current_box_pos_z < 0.20:
                terminated = True
                info["success"] = False
                reward = reward - 1.0

            # å¯é€‰ï¼šæ·»åŠ ä¸€ä¸ªscale factorè®©å¥–åŠ±èŒƒå›´æ›´åˆç†
            reward_scale = 10.0  # ä½ç½®è¯¯å·®é€šå¸¸æ¯”è¾ƒå°ï¼Œéœ€è¦æ”¾å¤§
            reward *= reward_scale

        return reward, terminated, info


    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:
        """
        Implement the step logic for the Kuavo robot.
        
        Args:
            action: The action to execute
        """
        global IF_USE_ZERO_OBS_FLAG

        # Increment step counter for efficiency reward calculation
        self.episode_step_count += 1
        
        self._send_action(action, self.episode_step_count)
        obs = self._get_observation()

        # Zero out all observations for testing
        if IF_USE_ZERO_OBS_FLAG:
            obs['agent_pos'] = np.zeros_like(obs['agent_pos'])
            obs['environment_state'] = np.zeros_like(obs['environment_state'])
            obs['pixels']['front'] = np.zeros_like(obs['pixels']['front'])
        
        # If this is the first step after reset, recalibrate initial position
        if self._is_first_step:
            if self.debug:
                old_box_pos = self.initial_box_pose['position']
                new_box_pos = obs['environment_state'][0:3]
                rospy.loginfo(f"First step - Recalibrating initial box position")
                rospy.loginfo(f"  Old initial position: {old_box_pos}")
                rospy.loginfo(f"  New initial position: {new_box_pos}")
                rospy.loginfo(f"  Position drift: {new_box_pos - old_box_pos}")
            
            # Update initial box pose with current observation
            self.initial_box_pose['position'] = obs['environment_state'][0:3]
            if self.wbc_observation_enabled:
                self.initial_box_pose['orientation'] = obs['environment_state'][3:7]
            self._is_first_step = False
        
        reward, done, info = self._compute_reward_and_done(obs, action)
        self.last_action = action
        return obs, reward, done, False, info

    def reset(self, *, seed: int | None = None, options: Dict[str, Any] | None = None) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Implement the reset logic for the Kuavo robot. This should include
        calling `super().reset(seed=seed)`, `self._reset_simulation()`, and returning
        the first observation.
        """
        super().reset(seed=seed)
        
        # åœ¨resetä»¿çœŸä¹‹å‰å…ˆç”Ÿæˆéšæœºä½ç½®ï¼Œé¿å…å‘å¸ƒä½ç½®æ—¶çš„è·³å˜
        if IF_USE_RANDOM_INITIAL_POSITION:
            self._generate_random_initial_positions()
        
        self._reset_simulation()
        
        # Wait for simulation to stabilize after reset
        import time
        time.sleep(0.5)  # ç­‰å¾…500msè®©ä»¿çœŸç¨³å®š
        
        # Get initial observation to establish baseline
        obs = self._get_observation()
        
        # Wait a bit more and get another observation to ensure stability
        time.sleep(0.2)  # å†ç­‰å¾…200ms
        obs_stable = self._get_observation()

        # Store initial box pose for reward calculation (use the stable observation)
        box_pos = obs_stable['environment_state'][0:3]
        if self.wbc_observation_enabled:
            box_orn = obs_stable['environment_state'][3:7]
        else:
            box_orn = np.array([0.0, 0.0, 0.0, 1.0])  # Default orientation when WBC is disabled
        self.initial_box_pose = {'position': box_pos, 'orientation': box_orn}
        
        if self.debug:
            rospy.loginfo(f"reset - Initial box position (first): {obs['environment_state'][0:3]}")
            rospy.loginfo(f"reset - Initial box position (stable): {box_pos}")
            rospy.loginfo(f"reset - Position difference: {box_pos - obs['environment_state'][0:3]}")
        
        self.last_action.fill(0.0)
        # Reset the first step flag
        self._is_first_step = True
        
        # Reset action smoothing state
        self.is_first_action = True
        self.last_smoothed_vel_action.fill(0.0)
        self.last_smoothed_arm_action.fill(0.0)
        
        # Reset end-effector position history for velocity penalty
        self.last_left_eef_pos = None
        self.last_right_eef_pos = None
        
        # Reset trajectory tracking variables
        self.last_dist_left_hand_to_box = None
        self.last_dist_right_hand_to_box = None
        self.last_dist_torso_to_box = None
        
        # Reset consecutive approach counters
        self.consecutive_approach_steps_left = 0
        self.consecutive_approach_steps_right = 0
        self.consecutive_approach_steps_torso = 0
        
        # Clear distance change history
        self.distance_change_history_left.clear()
        self.distance_change_history_right.clear()
        
        # Reset step counter for efficiency reward
        self.episode_step_count = 0
        
        # FIXED: Reset lift progress tracking to prevent cross-episode exploitation
        if hasattr(self, 'max_z_lift_achieved'):
            self.max_z_lift_achieved = 0.0
            
        # FIXED: Reset achievement flags to prevent cross-episode exploitation
        if hasattr(self, 'both_hands_close_achieved'):
            self.both_hands_close_achieved = False
        if hasattr(self, 'good_symmetry_achieved'):
            self.good_symmetry_achieved = False

        # Reset incremental control state - é‡ç½®åˆ°åˆå§‹ä½ç½®
        if not IF_USE_RANDOM_INITIAL_POSITION:
            self.current_left_pos = np.array([0.3178026345146559, 0.4004180715613648, -0.019417275957965042], dtype=np.float32)
            self.current_right_pos = np.array([0.3178026345146559, -0.4004180715613648, -0.019417275957965042], dtype=np.float32)

        # Reset reward tracking variables for improved reward function
        self.last_mean_distance = None
        self.last_ee_action = None

        # Reset reward tracking variables for improved reward function
        if hasattr(self, 'last_mse_total_eef'):
            self.last_mse_total_eef = None
        if hasattr(self, 'last_ee_action'):
            self.last_ee_action = None
        
        # Reset demo mode progress tracking
        global TEST_DEMO_USE_ACTION_16_DIM
        if TEST_DEMO_USE_ACTION_16_DIM:
            # Reset joint control tracking for DEMO mode
            if hasattr(self, 'best_joint_deviation'):
                self.best_joint_deviation = float('inf')
            if hasattr(self, 'left_arm_achieved'):
                self.left_arm_achieved = False
            if hasattr(self, 'right_arm_achieved'):
                self.right_arm_achieved = False
        else:
            # Reset end-effector position control tracking for DEMO mode
            if hasattr(self, 'best_mean_distance'):
                self.best_mean_distance = float('inf')
            if hasattr(self, 'left_hand_achieved'):
                self.left_hand_achieved = False
            if hasattr(self, 'right_hand_achieved'):
                self.right_hand_achieved = False

        # æ˜¯å¦å¯ç”¨äº†è‡ªåŠ¨å½•åˆ¶åŠŸèƒ½ - ä¸€èˆ¬ç”¨äºè‡ªåŠ¨å½•åˆ¶çš„æ—¶å€™æ¿€æ´»è‡ªåŠ¨ä¸“å®¶å·¥å…·
        if self.auto_record_tool_enable:
            rospy.wait_for_service('/robot_control/start_record_tool')
            try:
                start_record_tool = rospy.ServiceProxy('/robot_control/start_record_tool', Trigger)
                start_record_tool()
            except rospy.ServiceException as e:
                print(f"Service call failed: {e}")
                
        return obs_stable, {}


if __name__ == "__main__":
    import traceback
    
    print("Starting RLKuavoGymEnv test script...")

    # The environment itself handles ROS node initialization,
    # but it's good practice to have it here for a standalone script.
    if not rospy.core.is_initialized():
        rospy.init_node('rl_kuavo_env_test', anonymous=True)

    # Instantiate the environment with debugging enabled
    env = RLKuavoGymEnv(debug=True, enable_roll_pitch_control=False, wbc_observation_enabled=True,
                       box_reward_weight=3.0, hand_reward_weight=1.0)

    try:
        num_episodes = 3
        for i in range(num_episodes):
            print(f"\n--- Starting Episode {i + 1}/{num_episodes} ---")
            
            # Reset the environment
            obs, info = env.reset()
            print(f"Initial observation received.")
            print(f"  Agent_pos shape: {obs['agent_pos'].shape}")
            print(f"  Environment_state shape: {obs['environment_state'].shape}")
            print(f"  Pixels shape: {obs['pixels']['front'].shape}")

            episode_reward = 0
            terminated = False
            step_count = 0
            max_steps = 100

            # Run the episode
            while not terminated and step_count < max_steps:
                # Sample a random action from the normalized space [-1, 1]
                action = env.action_space.sample()
                print(f"\nStep {step_count + 1}/{max_steps}: Sampled normalized action shape: {action.shape}")
                
                # Step the environment
                obs, reward, terminated, truncated, info = env.step(action)
                
                print(f"Received observation:")
                print(f"  Agent_pos shape: {obs['agent_pos'].shape}")
                print(f"  Environment_state shape: {obs['environment_state'].shape}")
                print(f"  Pixels shape: {obs['pixels']['front'].shape}")
                print(f"Reward: {reward:.4f}, Terminated: {terminated}, Info: {info}")
                
                episode_reward += reward
                step_count += 1
                
                # A small delay to allow ROS messages to be processed
                rospy.sleep(0.1)

            print(f"--- Episode {i + 1} Finished ---")
            print(f"Total steps: {step_count}, Total reward: {episode_reward:.4f}")

    except Exception as e:
        print(f"\nAn error occurred during the test: {e}")
        traceback.print_exc()
    finally:
        # Cleanly close the environment
        env.close()
        print("\nTest script finished.")